timestamp,first_token_latency_ms,total_latency_ms,tokens_per_second,completion_tokens,prompt_tokens,error
2025-01-14 01:57:58,3720.714807510376,4969.884872436523,45.63029614656185,57,6,
2025-01-14 01:58:05,4618.0360317230225,6074.201345443726,34.33676075708953,50,9210,
2025-01-14 01:58:13,5847.190380096436,7852.517366409302,46.87514836312802,94,1233,
2025-01-14 01:58:23,2657.674789428711,3281.3072204589844,41.691225000993995,26,4,
2025-01-14 01:58:28,3278.07879447937,5142.486572265625,45.5908846834605,85,42,
2025-01-14 01:58:35,5414.355039596558,10140.03849029541,47.188941520622144,223,7195,
2025-01-14 01:58:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 162123 tokens (162123 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 01:58:51,5134.971618652344,7032.948732376099,44.25764641344667,84,1002,
2025-01-14 01:59:00,3179.3322563171387,4573.078155517578,43.049454017709145,60,39,
2025-01-14 01:59:06,3603.234052658081,4916.564702987671,47.2082182689033,62,111,
2025-01-14 01:59:23,4055.0427436828613,5210.348844528198,41.54743055963993,48,74,
2025-01-14 02:00:29,2588.6147022247314,3348.155736923218,39.49753684066464,30,13,
2025-01-14 02:01:32,6326.10821723938,8725.93641281128,38.336077628288564,92,18129,
2025-01-14 02:02:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 227951 tokens (227951 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:03:42,4280.74836730957,5617.6393032073975,36.65220451741088,49,257,
2025-01-14 02:04:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 205534 tokens (205534 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:05:49,18606.770038604736,20522.881984710693,26.616398955001248,51,37662,
2025-01-14 02:07:10,3171.1108684539795,4256.662607192993,37.7688124267812,41,8,
2025-01-14 02:08:14,3068.0387020111084,4462.4083042144775,42.31302798538406,59,216,
2025-01-14 02:09:18,3113.463878631592,3970.4740047454834,40.83965747138524,35,7,
2025-01-14 02:10:22,4798.973798751831,5809.386968612671,45.525930750027136,46,17,
2025-01-14 02:11:28,5688.522815704346,7877.243995666504,41.576789603495016,91,2438,
2025-01-14 02:12:36,9704.867601394653,12500.620603561401,33.26474117274441,93,27091,
2025-01-14 02:13:49,3091.4430618286133,4545.515537261963,46.76520678911436,68,1764,
2025-01-14 02:14:53,21281.731605529785,24289.203882217407,32.25299888943087,97,44330,
2025-01-14 02:16:17,3955.95645904541,6721.371173858643,48.4556617429621,134,6,
2025-01-14 02:17:24,5531.225919723511,6995.477199554443,34.83008736443695,51,9177,
2025-01-14 02:18:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 240257 tokens (240257 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:19:33,3493.450880050659,5041.694164276123,36.17002610026821,56,553,
2025-01-14 02:20:38,3470.046281814575,7589.632511138916,44.66467983853274,184,6485,
2025-01-14 02:21:45,3524.4672298431396,5554.2097091674805,41.877233622412405,85,1677,
2025-01-14 02:22:51,3396.122455596924,4587.818145751953,40.27873927592674,48,65,
2025-01-14 02:23:56,6886.41357421875,9267.646551132202,36.53569425733771,87,18414,
2025-01-14 02:25:05,2941.002607345581,5383.896112442017,41.753764454817464,102,553,
2025-01-14 02:26:10,5273.413896560669,7502.129077911377,39.93332155886413,89,9518,
2025-01-14 02:27:18,3159.782886505127,4301.021575927734,42.059562512978665,48,76,
2025-01-14 02:28:22,3217.4508571624756,4195.187330245972,35.79696673237544,35,12,
2025-01-14 02:29:26,3457.899570465088,5627.724170684814,42.39974050929446,92,5,
2025-01-14 02:30:32,2408.4272384643555,3080.7573795318604,28.259925949225465,19,4,
2025-01-14 02:31:35,2703.2110691070557,3848.9067554473877,41.8959419785591,48,112,
2025-01-14 02:32:39,4349.0355014801025,5665.3149127960205,35.706704515732646,47,1885,
2025-01-14 02:33:44,4101.600646972656,5794.457197189331,38.396637914583145,65,116,
2025-01-14 02:34:50,6227.705478668213,7666.836500167847,35.43805201756814,51,1723,
2025-01-14 02:35:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 463496 tokens (463496 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:37:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109039 tokens (109039 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:38:01,12807.441473007202,19701.38430595398,45.692284898938595,315,17698,
2025-01-14 02:39:21,5518.326759338379,7620.030641555786,46.62883331433203,98,328,
2025-01-14 02:40:29,3567.7077770233154,5130.107164382935,38.402472815479754,60,16,
2025-01-14 02:41:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 386723 tokens (386723 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:42:36,3983.384609222412,5409.823656082153,39.95964645351207,57,1843,
2025-01-14 02:43:41,32092.554569244385,35070.28579711914,32.57513609421026,97,48473,
2025-01-14 02:45:16,3701.533555984497,5882.525205612183,41.724139574552936,91,2406,
2025-01-14 02:46:22,3570.4405307769775,4807.398080825806,43.65549973632352,54,1080,
2025-01-14 02:47:27,3612.762212753296,5052.092790603638,41.68604552931179,60,6,
2025-01-14 02:48:32,69641.9768333435,72577.19254493713,31.343522602653994,92,41034,
2025-01-14 02:50:45,49955.03830909729,53089.02597427368,28.71740721893827,90,51898,
2025-01-14 02:52:38,5136.578798294067,6015.654802322388,36.40185814805734,32,5,
2025-01-14 02:53:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 407029 tokens (407029 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:54:51,2754.098415374756,3623.6207485198975,40.25198510244332,35,14,
2025-01-14 02:55:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 338916 tokens (338916 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:56:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 660953 tokens (660953 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:58:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 401965 tokens (401965 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 02:59:04,5390.62762260437,8393.563508987427,45.622019644585976,137,5,
2025-01-14 03:00:12,3546.0500717163086,4751.107454299927,37.34262006969405,45,22,
2025-01-14 03:01:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-14 03:32:18,4688.433408737183,6928.715229034424,39.727144680481075,89,2304,
2025-01-14 03:33:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95508 tokens (95508 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 03:34:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-14 04:05:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-14 04:36:29,3242.2900199890137,4068.472385406494,37.521982188926664,31,10,
2025-01-14 04:37:33,4116.76025390625,5754.625558853149,39.075252285214574,64,722,
2025-01-14 04:38:39,3251.5182495117188,4333.088636398315,44.37991330196509,48,451,
2025-01-14 04:39:43,4009.777784347534,5411.062479019165,44.245113241980235,62,673,
2025-01-14 04:40:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 233627 tokens (233627 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 04:41:50,3480.041742324829,5260.761976242065,43.8025010972191,78,74,
2025-01-14 04:42:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 316621 tokens (316621 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 04:43:58,3624.2353916168213,4766.552925109863,38.51818667744192,44,4,
2025-01-14 04:45:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 154985 tokens (154985 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 04:46:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 610966 tokens (610966 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 04:47:06,3583.98175239563,5271.799802780151,34.956374584664815,59,9,
2025-01-14 04:48:12,3932.647228240967,5581.053733825684,43.67854637558617,72,24,
2025-01-14 04:49:17,3061.3322257995605,4183.397531509399,43.669472490285855,49,129,
2025-01-14 04:50:21,5548.763990402222,7682.79767036438,47.328212740199575,101,153,
2025-01-14 04:51:29,3974.5683670043945,5298.823356628418,39.26736195629786,52,4,
2025-01-14 04:52:34,3843.3279991149902,5827.145338058472,43.350765371271976,86,322,
2025-01-14 04:53:40,11147.711753845215,15259.79733467102,39.152881630364156,161,15158,
2025-01-14 04:54:56,3088.9339447021484,4087.4550342559814,44.06516843791494,44,21,
2025-01-14 04:56:00,5088.79017829895,6407.051801681519,40.2044624981236,53,341,
2025-01-14 04:57:06,17008.96430015564,21177.229642868042,41.98389152599455,175,35164,
2025-01-14 04:58:27,19551.978826522827,22076.47204399109,36.44295788295444,92,39537,
2025-01-14 04:59:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118046 tokens (118046 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:00:51,3963.395357131958,4887.563228607178,44.36423431876398,41,7,
2025-01-14 05:01:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 594399 tokens (594399 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:02:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71186 tokens (71186 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:03:59,3121.461868286133,4395.391464233398,45.52841866969304,58,662,
2025-01-14 05:05:04,3853.091239929199,4966.012001037598,43.129755214733386,48,224,
2025-01-14 05:06:09,3146.7883586883545,4165.381908416748,38.288088522059944,39,37,
2025-01-14 05:07:13,2986.273765563965,3737.2114658355713,41.281720159725126,31,4,
2025-01-14 05:08:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 493489 tokens (493489 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:09:19,4403.98097038269,5953.792572021484,43.87630078913836,68,395,
2025-01-14 05:10:25,3652.583360671997,4499.433994293213,37.787065073287934,32,4,
2025-01-14 05:11:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 211290 tokens (211290 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:12:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 357298 tokens (357298 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:13:33,3409.303903579712,5500.805854797363,43.03127709137584,90,136,
2025-01-14 05:14:38,3300.1461029052734,4547.396898269653,38.48464172434299,48,268,
2025-01-14 05:15:43,5871.224641799927,8983.385562896729,44.0208599341058,137,6513,
2025-01-14 05:16:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70612 tokens (70612 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:17:54,5094.053745269775,6555.173397064209,38.326772164911446,56,69,
2025-01-14 05:19:00,3474.440336227417,4735.616207122803,38.85263041483017,49,116,
2025-01-14 05:20:05,3181.7593574523926,4447.640419006348,25.27883619707354,32,4,
2025-01-14 05:21:10,34162.44649887085,36542.95015335083,21.424037683798858,51,59292,
2025-01-14 05:22:46,3592.254877090454,5088.288068771362,38.1007589383469,57,926,
2025-01-14 05:23:51,36690.03939628601,43249.39298629761,40.55277648167306,266,58781,
2025-01-14 05:25:35,4331.334114074707,5625.689506530762,40.94702298062966,53,35,
2025-01-14 05:26:40,3943.0055618286133,5774.60503578186,35.488108030358156,65,790,
2025-01-14 05:27:46,5469.899654388428,7006.149530410767,42.96176099352231,66,257,
2025-01-14 05:28:53,5138.0205154418945,6077.535629272461,37.25325913842824,35,6,
2025-01-14 05:29:59,5365.347862243652,13805.426120758057,49.288642505220444,416,8,
2025-01-14 05:31:13,3565.371513366699,4716.511487960815,52.990949273139584,61,307,
2025-01-14 05:32:18,3156.3286781311035,4236.208438873291,47.227480182560654,51,143,
2025-01-14 05:33:22,3101.156711578369,4276.206016540527,57.01888398815546,67,17,
2025-01-14 05:34:26,4830.315113067627,5764.498949050903,54.59310901726306,51,1581,
2025-01-14 05:35:32,2881.4995288848877,3889.7554874420166,56.53326371764786,57,2628,
2025-01-14 05:36:36,3817.566394805908,4852.37979888916,57.01510993884786,59,1295,
2025-01-14 05:37:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 206435 tokens (206435 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:38:42,2300.8720874786377,2919.0800189971924,48.527361864006735,30,4,
2025-01-14 05:39:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 99763 tokens (99763 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:40:47,3805.325746536255,5015.118360519409,59.514332595357175,72,10,
2025-01-14 05:41:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 137525 tokens (137525 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:42:53,2501.9776821136475,3454.456329345703,57.74407663608249,55,11,
2025-01-14 05:43:57,2494.9538707733154,5660.941362380981,56.53844194725642,179,1334,
2025-01-14 05:45:03,2815.523862838745,3813.755750656128,35.061993537921246,35,9,
2025-01-14 05:46:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 746420 tokens (746420 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:47:10,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 509867 tokens (509867 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 05:48:12,4802.252292633057,6503.048658370972,52.32842790170596,89,2494,
2025-01-14 05:49:18,6069.475173950195,8450.268507003784,45.36303025575091,108,195,
2025-01-14 05:50:27,3192.0957565307617,4058.3341121673584,55.41200027413345,48,55,
2025-01-14 05:51:31,4004.246711730957,5809.638738632202,52.066254087396494,94,3495,
2025-01-14 05:52:37,12621.683359146118,14777.143239974976,43.14624495086307,93,34313,
2025-01-14 05:53:51,3041.4063930511475,4034.5282554626465,50.34628870075418,50,2345,
2025-01-14 05:54:55,9858.433246612549,11552.294254302979,41.325704814142114,70,20506,
2025-01-14 05:56:07,3071.558713912964,3924.097776412964,41.05383734249714,35,15,
2025-01-14 05:57:11,2783.9622497558594,3791.6951179504395,48.623997039797594,49,123,
2025-01-14 05:58:15,12256.09302520752,14933.195352554321,36.980282370497214,99,32056,
2025-01-14 05:59:30,4404.966354370117,7689.3486976623535,46.58410136459147,153,4282,
2025-01-14 06:00:37,4104.744911193848,5488.589286804199,41.91222728669819,58,4672,
2025-01-14 06:01:43,5293.715953826904,6720.244646072388,37.85412820193555,54,9558,
2025-01-14 06:02:50,6253.833532333374,10514.46795463562,45.533125063372964,194,182,
2025-01-14 06:04:00,3319.154977798462,5165.807008743286,48.195327819537226,89,1187,
2025-01-14 06:05:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 189921 tokens (189921 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:06:07,3049.366235733032,4007.603883743286,53.222705354876915,51,127,
2025-01-14 06:07:11,2833.906412124634,3548.4468936920166,43.38452585919253,31,7,
2025-01-14 06:08:15,4041.0513877868652,5245.738506317139,42.334643755650305,51,146,
2025-01-14 06:09:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 720462 tokens (720462 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:10:22,8662.495613098145,11082.440376281738,40.08355954058647,97,19817,
2025-01-14 06:11:33,3721.5118408203125,5647.72629737854,44.12800439255271,85,72,
2025-01-14 06:12:39,3849.658727645874,4995.365142822266,48.87814125696268,56,1085,
2025-01-14 06:13:44,3943.8047409057617,4623.487234115601,45.609531376335376,31,14,
2025-01-14 06:14:49,3604.6173572540283,5048.745155334473,42.24002895109566,61,9,
2025-01-14 06:15:54,4177.0501136779785,5208.188533782959,40.73167984151339,42,8,
2025-01-14 06:16:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 598103 tokens (598103 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:18:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 599705 tokens (599705 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:19:04,5840.554475784302,8303.94196510315,45.87179259859182,113,439,
2025-01-14 06:20:12,4117.32029914856,6136.318206787109,32.68948410015669,66,15,
2025-01-14 06:21:18,7205.942392349243,9347.57375717163,42.95790653384919,92,15951,
2025-01-14 06:22:28,2647.60422706604,4060.5080127716064,43.881261149738414,62,20,
2025-01-14 06:23:32,2997.864007949829,4037.930727005005,44.22793187901217,46,47,
2025-01-14 06:24:36,4671.332836151123,6803.203105926514,45.030882676606,96,9393,
2025-01-14 06:25:42,2741.732358932495,3487.0662689208984,40.25041608594834,30,4,
2025-01-14 06:26:46,4000.5431175231934,5662.571668624878,42.7188810643098,71,634,
2025-01-14 06:27:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 77498 tokens (77498 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:28:52,4424.362659454346,6589.446067810059,43.87821717785385,95,1479,
2025-01-14 06:29:59,5833.077907562256,7821.690082550049,39.22333423332219,78,6427,
2025-01-14 06:31:07,3603.7087440490723,5080.310821533203,36.57044834448998,54,22,
2025-01-14 06:32:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 202420 tokens (202420 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:33:13,3086.5578651428223,3850.964069366455,40.55435425394679,31,4,
2025-01-14 06:34:17,3555.9980869293213,4762.601852416992,40.609851718965714,49,7,
2025-01-14 06:35:22,3875.704288482666,5170.768022537231,39.38030126156802,51,175,
2025-01-14 06:36:27,3786.640405654907,5265.4266357421875,39.89758546542439,59,231,
2025-01-14 06:37:32,3467.6144123077393,4518.669366836548,39.00842655594582,41,4,
2025-01-14 06:38:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 518660 tokens (518660 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:39:39,4720.827579498291,5990.132093429565,40.17948367806825,51,47,
2025-01-14 06:40:45,7550.661563873291,8984.662771224976,36.95952257800429,53,8232,
2025-01-14 06:41:54,18705.545663833618,20807.57975578308,32.825347725929696,69,22333,
2025-01-14 06:43:15,28269.965887069702,31149.746894836426,32.64137090510823,94,50577,
2025-01-14 06:44:46,3991.799831390381,4812.703609466553,41.41776528265062,34,139,
2025-01-14 06:45:51,3567.115545272827,5348.029375076294,39.30566365904681,70,7,
2025-01-14 06:46:56,42967.46897697449,45522.02320098877,26.619125701369267,68,63559,
2025-01-14 06:48:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 441394 tokens (441394 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 06:49:44,4801.189184188843,6330.726861953735,32.03582410052396,49,4402,
2025-01-14 06:50:50,3089.154005050659,3910.288095474243,43.841804182590096,36,7,
2025-01-14 06:51:54,4245.832204818726,5395.777940750122,42.61070628720801,49,321,
2025-01-14 06:52:59,5471.5211391448975,7608.086109161377,42.123689783843,90,1495,
2025-01-14 06:54:07,2864.3760681152344,3904.6902656555176,40.3724183514026,42,27,
2025-01-14 06:55:11,3983.396291732788,5234.904766082764,42.348894223451275,53,745,
2025-01-14 06:56:16,4503.967046737671,5521.39949798584,42.263247989828045,43,48,
2025-01-14 06:57:22,6111.802101135254,7905.304908752441,27.320838189877403,49,2479,
2025-01-14 06:58:29,4083.237886428833,6605.300426483154,41.23609083768399,104,2171,
2025-01-14 06:59:36,5467.2205448150635,6876.366138458252,36.90179370717813,52,3127,
2025-01-14 07:00:43,6908.627271652222,9197.827816009521,38.44136775911272,88,2267,
2025-01-14 07:01:52,3220.454216003418,4087.5377655029297,35.75203337428495,31,4,
2025-01-14 07:02:56,4353.748559951782,5795.558452606201,39.533644685334444,57,7,
2025-01-14 07:04:02,3961.5912437438965,4838.244199752808,39.92457877441324,35,13,
2025-01-14 07:05:07,4837.721347808838,6101.1505126953125,42.740821172077474,54,3419,
2025-01-14 07:06:13,8158.433437347412,10232.578039169312,46.76626688168056,97,1586,
2025-01-14 07:07:23,21130.534410476685,26509.76514816284,39.03903926796979,210,33124,
2025-01-14 07:08:50,4207.799434661865,8894.79374885559,41.81784462730176,196,1142,
2025-01-14 07:09:59,6815.285682678223,11439.165592193604,43.03744991094649,199,15129,
2025-01-14 07:11:10,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 598535 tokens (598535 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 07:12:13,3929.7072887420654,5262.924432754517,42.0036602825721,56,8,
2025-01-14 07:13:18,4928.410530090332,6627.737522125244,31.77728609803109,54,8349,
2025-01-14 07:14:25,6083.789110183716,10950.319528579712,46.85062670894567,228,2035,
2025-01-14 07:15:36,3848.6058712005615,4904.707670211792,39.76889352837224,42,116,
2025-01-14 07:16:41,3502.631187438965,4867.1300411224365,41.7735785164848,57,7,
2025-01-14 07:17:46,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78349 tokens (78349 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 07:18:46,3907.930612564087,4858.886003494263,32.59879516501546,31,4,
2025-01-14 07:19:51,3148.5698223114014,4016.792058944702,35.70514401958706,31,4,
2025-01-14 07:20:55,3052.618980407715,3904.428482055664,36.393113648093845,31,10,
2025-01-14 07:21:59,501822.40748405457,504397.4184989929,33.397915387969256,86,28872,
2025-01-14 07:31:24,3674.434185028076,5511.020660400391,38.11418680180008,70,2144,
2025-01-14 07:32:29,4217.558860778809,5308.15315246582,44.01270056690839,48,141,
2025-01-14 07:33:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 700689 tokens (700689 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 07:34:37,3347.613573074341,4475.3477573394775,28.37548107212171,32,4,
2025-01-14 07:35:41,8280.571699142456,13466.194868087769,44.54623725521929,231,9014,
2025-01-14 07:36:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-14 08:07:56,75336.55405044556,76490.41795730591,41.59935995450916,48,91,
2025-01-14 08:10:12,3631.3798427581787,4906.278133392334,36.86568594944263,47,4,
2025-01-14 08:11:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 225246 tokens (225246 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 08:12:19,7759.864568710327,10693.747997283936,33.06198162316194,97,9921,
2025-01-14 08:13:29,3366.8599128723145,5938.9660358428955,38.878644666693546,100,21,
2025-01-14 08:14:35,3307.143449783325,4370.9471225738525,37.6009230115493,40,4,
2025-01-14 08:15:40,6947.139024734497,9284.93857383728,21.387633520243146,50,15737,
2025-01-14 08:16:49,4804.150581359863,7442.97456741333,38.653582254475275,102,30,
2025-01-14 08:17:57,3650.8774757385254,5603.271722793579,30.731498051944484,60,1103,
2025-01-14 08:19:02,9429.69822883606,11960.182905197144,32.80004055166088,83,13354,
2025-01-14 08:20:14,9778.93614768982,12009.490966796875,30.934007722626763,69,19479,
2025-01-14 08:21:26,8905.201435089111,11424.998998641968,36.90772677344451,93,8338,
2025-01-14 08:22:38,7985.249042510986,10309.616804122925,39.580655660186245,92,1286,
2025-01-14 08:23:48,4424.968004226685,6012.513160705566,29.60545708459992,47,5,
2025-01-14 08:24:54,44040.194511413574,47458.05025100708,23.69906929121401,81,57002,
2025-01-14 08:26:41,29416.89968109131,33773.66042137146,32.363493982213335,141,43543,
2025-01-14 08:28:15,12258.011102676392,14709.76972579956,33.037509988164445,81,13221,
2025-01-14 08:29:30,13157.217741012573,18332.30209350586,35.55497600949273,184,21037,
2025-01-14 08:30:48,21877.851009368896,23977.473258972168,30.957949703705943,65,16924,
2025-01-14 08:32:12,4322.432041168213,6802.917003631592,37.49266833193835,93,2738,
2025-01-14 08:33:19,4234.306573867798,5795.293569564819,31.39039603473456,49,1261,
2025-01-14 08:34:25,6439.236640930176,9445.483446121216,29.605021066906133,89,9433,
2025-01-14 08:35:34,3927.92010307312,9943.869352340698,38.39793030636412,231,492,
2025-01-14 08:36:44,4448.08292388916,7520.668029785156,35.4750141146097,109,3433,
2025-01-14 08:37:52,3808.6087703704834,6228.744983673096,35.94838981450404,87,170,
2025-01-14 08:38:58,5279.50644493103,8385.578870773315,30.263299470394568,94,9131,
2025-01-14 08:40:06,4308.353424072266,6236.078500747681,26.45605466104918,51,1042,
2025-01-14 08:41:13,7029.543399810791,10256.04796409607,32.852890144130534,106,8878,
2025-01-14 08:42:23,15326.16639137268,17140.34366607666,30.316772658821,55,10516,
2025-01-14 08:43:40,3433.692455291748,4495.263338088989,42.39000968209011,45,38,
2025-01-14 08:44:44,2793.954372406006,3179.7101497650146,25.923137349912885,10,4,
2025-01-14 08:45:48,6398.1359004974365,8322.856187820435,25.97780068580387,50,607,
2025-01-14 08:46:56,6546.074628829956,13821.574449539185,40.959385244126146,298,10445,
2025-01-14 08:48:10,4884.178876876831,10357.683420181274,49.32854222808348,270,2512,
2025-01-14 08:49:20,3158.7140560150146,4689.100503921509,33.324916114989065,51,125,
2025-01-14 08:50:25,3004.0345191955566,3928.924798965454,41.085954551770165,38,24,
2025-01-14 08:51:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 75412 tokens (75412 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 08:52:30,3267.6150798797607,4499.54628944397,45.45708361411656,56,6,
2025-01-14 08:53:34,3620.121955871582,4645.533800125122,31.206973255994285,32,4,
2025-01-14 08:54:39,3579.9942016601562,4870.498180389404,27.121187208166766,35,10,
2025-01-14 08:55:44,3401.1144638061523,4670.152187347412,36.24793743060422,46,31,
2025-01-14 08:56:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 143520 tokens (143520 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 08:57:50,21178.415298461914,27029.874086380005,34.69220366366555,203,42480,
2025-01-14 08:59:17,3129.1329860687256,3586.5023136138916,21.864168403405852,10,4,
2025-01-14 09:00:20,4979.899168014526,6573.846817016602,35.760271070186064,57,1504,
2025-01-14 09:01:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 459629 tokens (459629 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:02:30,5430.707693099976,6825.535535812378,35.12978340374529,49,319,
2025-01-14 09:03:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156906 tokens (156906 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:04:38,7073.296308517456,12261.81173324585,41.24493857724291,214,465,
2025-01-14 09:05:51,3368.5667514801025,4701.8420696258545,37.50163174815036,50,4,
2025-01-14 09:06:55,12918.158769607544,16015.788316726685,28.408819925494903,88,24554,
2025-01-14 09:08:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 453493 tokens (453493 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:09:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 256136 tokens (256136 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:10:15,8835.267066955566,11471.57621383667,34.51795481105011,91,9217,
2025-01-14 09:11:27,8467.177152633667,9969.387769699097,35.94702326461323,54,234,
2025-01-14 09:12:37,4130.403995513916,5270.574331283569,34.20541543354019,39,22,
2025-01-14 09:13:42,4837.701797485352,6234.204292297363,35.80373123982903,50,21,
2025-01-14 09:14:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 161722 tokens (161722 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:15:51,7918.108701705933,9272.310733795166,36.18367041172126,49,156,
2025-01-14 09:17:00,4923.011779785156,7378.40461730957,38.2830798247229,94,610,
2025-01-14 09:18:07,16082.795858383179,20595.98469734192,35.23007914658489,159,20674,
2025-01-14 09:19:28,4175.628185272217,9150.099754333496,36.787827100705194,183,1159,
2025-01-14 09:20:37,4623.295307159424,7991.323709487915,36.81679166193267,124,4305,
2025-01-14 09:21:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151246 tokens (151246 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:22:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 540186 tokens (540186 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:23:49,65405.174255371094,68424.26824569702,36.10354641136325,109,13394,
2025-01-14 09:25:57,3846.1904525756836,5183.048486709595,35.1570614081316,47,18,
2025-01-14 09:27:03,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69562 tokens (69562 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:28:04,8631.762742996216,13596.277713775635,38.67447296061963,192,5578,
2025-01-14 09:29:17,116938.44985961914,119743.81351470947,32.0814022940288,90,14763,
2025-01-14 09:32:17,821948.6861228943,823494.9383735657,30.396075400758498,47,7,
2025-01-14 09:47:01,22565.651416778564,25125.02908706665,36.72767840840749,94,2711,
2025-01-14 09:48:26,3871.3748455047607,5239.777088165283,40.9236394491153,56,6,
2025-01-14 09:49:31,5968.595027923584,8924.224376678467,35.86376622111111,106,2749,
2025-01-14 09:50:40,6279.135942459106,7855.0004959106445,34.26690439970014,54,5036,
2025-01-14 09:51:48,5250.992059707642,6647.164583206177,35.09594922926544,49,629,
2025-01-14 09:52:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 234389 tokens (234389 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 09:53:56,261596.96674346924,263918.10750961304,40.49732845637184,94,5,
2025-01-14 09:59:20,47380.17177581787,49022.67932891846,35.92068717652155,59,14,
2025-01-14 10:01:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 72511 tokens (72511 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:02:10,53728.50441932678,56194.692611694336,36.49356536477434,90,2784,
2025-01-14 10:04:06,4509.811162948608,6181.127309799194,37.09651230069923,62,40,
2025-01-14 10:05:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 267681 tokens (267681 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:06:14,4186.576366424561,5650.238990783691,34.84409532034782,51,208,
2025-01-14 10:07:20,4292.882204055786,5234.72261428833,36.099534093685016,34,5,
2025-01-14 10:08:25,38858.043909072876,40460.14666557312,35.57824226238469,57,2015,
2025-01-14 10:10:05,53026.5736579895,54624.08447265625,36.30648347886096,58,9,
2025-01-14 10:12:00,9049.352169036865,12174.885511398315,29.434976345666104,92,2273,
2025-01-14 10:13:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 287734 tokens (287734 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:14:14,145485.09240150452,148279.09588813782,39.01212025019413,109,5,
2025-01-14 10:17:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 724858 tokens (724858 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:18:45,190971.74501419067,192541.06211662292,38.2331906706473,60,2381,
2025-01-14 10:22:57,8557.267427444458,9781.008958816528,39.22396908943832,48,129,
2025-01-14 10:24:07,67366.7356967926,68896.42381668091,39.22368175571702,60,13,
2025-01-14 10:26:16,3710.474967956543,4924.8206615448,38.70397058116146,47,52,
2025-01-14 10:27:21,4630.945444107056,5907.1924686431885,39.96091588815775,51,180,
2025-01-14 10:28:27,29139.846086502075,30959.596157073975,26.926774611746872,49,32,
2025-01-14 10:29:58,26807.56425857544,28294.69919204712,26.224923591133365,39,16,
2025-01-14 10:31:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 330155 tokens (330155 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:32:28,4415.33899307251,6416.49055480957,28.48359968823264,57,7,
2025-01-14 10:33:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140051 tokens (140051 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:34:36,4502.859354019165,9121.12808227539,41.140958047224885,190,1877,
2025-01-14 10:35:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 153054 tokens (153054 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:36:46,81447.73578643799,82287.40763664246,40.49200886241535,34,4,
2025-01-14 10:39:08,220087.7423286438,221207.6051235199,33.03976180768973,37,7,
2025-01-14 10:43:49,5087.557554244995,7841.2134647369385,33.4101292937374,92,95,
2025-01-14 10:44:57,5120.5198764801025,6256.806373596191,28.16191170203681,32,4,
2025-01-14 10:46:04,5090.149164199829,7585.000514984131,30.863562262254085,77,4235,
2025-01-14 10:47:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 525901 tokens (525901 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 10:48:14,36997.59364128113,39758.65411758423,32.958350887642894,91,34183,
2025-01-14 10:49:53,7232.53321647644,9771.342039108276,36.631352140801326,93,2192,
2025-01-14 10:51:03,9118.287801742554,11876.147508621216,39.8860028034195,110,4906,
2025-01-14 10:52:15,18956.88223838806,22452.5306224823,25.74629656961909,90,23861,
2025-01-14 10:53:38,4286.873340606689,5781.030893325806,38.14858740717775,57,12,
2025-01-14 10:54:43,21093.624114990234,22839.938640594482,34.93070641377915,61,52,
2025-01-14 10:56:06,5716.819286346436,7257.789373397827,31.149209451461072,48,227,
2025-01-14 10:57:13,11476.574420928955,14666.971206665039,37.29943577301608,119,2732,
2025-01-14 10:58:28,5881.1140060424805,7634.458541870117,32.50929799321735,57,827,
2025-01-14 10:59:36,14569.856643676758,17809.768438339233,32.09963930849371,104,28379,
2025-01-14 11:00:54,4144.239664077759,5711.251974105835,33.82232523690283,53,9,
2025-01-14 11:01:59,8895.02239227295,14328.72462272644,39.751902264613584,216,7180,
2025-01-14 11:03:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 650491 tokens (650491 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:04:17,3572.9315280914307,5076.420068740845,33.92110988619252,51,51,
2025-01-14 11:05:22,3930.6492805480957,6405.28416633606,27.478801172055608,68,1662,
2025-01-14 11:06:28,7245.036363601685,9066.130876541138,32.39809882506719,59,543,
2025-01-14 11:07:37,5473.544120788574,7330.204010009766,33.93190124144194,63,1164,
2025-01-14 11:08:44,5923.701047897339,7647.6945877075195,32.482720327458914,56,16,
2025-01-14 11:09:52,5159.667253494263,7019.78611946106,27.417602677500263,51,8543,
2025-01-14 11:10:59,5120.805263519287,6765.958070755005,33.43154493497429,55,663,
2025-01-14 11:12:06,4854.861736297607,6410.314083099365,31.50209011594043,49,112,
2025-01-14 11:13:12,7194.826126098633,11668.015241622925,30.179810536397802,135,14322,
2025-01-14 11:14:24,3312.095880508423,4778.997421264648,18.406143323075934,27,6,
2025-01-14 11:15:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82126 tokens (82126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:16:31,4451.076507568359,6451.770782470703,35.487680896905445,71,857,
2025-01-14 11:17:37,3768.1751251220703,5971.060276031494,23.151456615405262,51,95,
2025-01-14 11:18:43,4372.135877609253,5596.251010894775,33.493581514637505,41,8,
2025-01-14 11:19:49,3627.0668506622314,4661.815166473389,32.85823178494068,34,4,
2025-01-14 11:20:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 594853 tokens (594853 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:21:56,23887.608766555786,30860.99648475647,34.84676455975122,243,43213,
2025-01-14 11:23:27,4344.493627548218,6178.423166275024,31.08080152281743,57,5710,
2025-01-14 11:24:33,4534.4507694244385,5965.651750564575,34.93569432866692,50,1360,
2025-01-14 11:25:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70112 tokens (70112 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:26:40,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 445485 tokens (445485 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:27:42,6153.151273727417,9066.637516021729,31.57729000551307,92,13754,
2025-01-14 11:28:51,3603.5866737365723,5885.56694984436,28.922248229319283,66,3780,
2025-01-14 11:29:57,3470.996856689453,4915.274620056152,33.92699191447635,49,323,
2025-01-14 11:31:02,2919.9368953704834,3927.7684688568115,31.751337070443647,32,5,
2025-01-14 11:32:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 747418 tokens (747418 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:33:11,17348.8507270813,20797.675609588623,27.255660464749727,94,36628,
2025-01-14 11:34:31,4878.928661346436,6451.241016387939,34.344320851294576,54,668,
2025-01-14 11:35:38,8703.231811523438,11769.9875831604,32.607748202466844,100,21814,
2025-01-14 11:36:50,3568.2318210601807,4818.917751312256,29.583766079900386,37,5,
2025-01-14 11:37:54,6084.0489864349365,7596.256256103516,33.725535528722425,51,4316,
2025-01-14 11:39:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74891 tokens (74891 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:40:03,3666.318416595459,5305.54723739624,19.52137468176508,32,5,
2025-01-14 11:41:08,4107.228517532349,5729.773998260498,30.199461637285065,49,582,
2025-01-14 11:42:14,3491.048812866211,4961.500406265259,34.003159454179404,50,6,
2025-01-14 11:43:19,3091.5186405181885,8086.683034896851,40.23891590558992,201,214,
2025-01-14 11:44:27,7568.870067596436,10545.482873916626,32.251423428725715,96,1853,
2025-01-14 11:45:38,6531.731605529785,10776.329040527344,38.40175717396242,163,8,
2025-01-14 11:46:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 392434 tokens (392434 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:47:51,3515.369653701782,4969.199419021606,37.143275841598054,54,8,
2025-01-14 11:48:56,3963.101625442505,5188.01736831665,36.737220712350286,45,41,
2025-01-14 11:50:01,7014.538526535034,8540.069341659546,36.053024596236014,55,455,
2025-01-14 11:51:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 242590 tokens (242590 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:52:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 481814 tokens (481814 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:53:14,4826.537847518921,6446.27046585083,34.57360762276426,56,1290,
2025-01-14 11:54:21,7023.735523223877,11799.306869506836,38.73882025529653,185,136,
2025-01-14 11:55:33,4963.700294494629,6219.717979431152,33.439019612309515,42,5,
2025-01-14 11:56:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 239569 tokens (239569 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 11:57:40,4461.687326431274,6936.5434646606445,36.769814048709,91,559,
2025-01-14 11:58:47,8835.31379699707,12411.543607711792,31.317897877937664,112,637,
2025-01-14 12:00:00,3969.045877456665,7097.087144851685,30.370443315511118,95,206,
2025-01-14 12:01:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 201281 tokens (201281 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:02:08,3830.7595252990723,5462.841033935547,36.762869796942375,60,11,
2025-01-14 12:03:14,5847.0025062561035,9711.4417552948,33.38129847224007,129,4533,
2025-01-14 12:04:23,4000.4873275756836,6160.653591156006,37.03418637202753,80,247,
2025-01-14 12:05:29,3705.962657928467,5627.016305923462,27.589026498721747,53,8,
2025-01-14 12:06:35,3777.0352363586426,5679.235219955444,33.64525315523591,64,249,
2025-01-14 12:07:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 522880 tokens (522880 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:08:43,3968.916416168213,5527.675628662109,37.20908241318709,58,1064,
2025-01-14 12:09:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173387 tokens (173387 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:10:50,8456.696033477783,14252.112865447998,37.443380914885545,217,17453,
2025-01-14 12:12:05,2995.490550994873,3907.073497772217,30.715800574140253,28,4,
2025-01-14 12:13:09,17899.319887161255,21594.619750976562,26.790789285983653,99,40220,
2025-01-14 12:14:30,3262.8495693206787,4418.001413345337,31.16473404446345,36,6,
2025-01-14 12:15:35,10138.824224472046,16853.546142578125,38.57196219870446,259,17810,
2025-01-14 12:16:51,4699.469327926636,7592.900514602661,32.4873805303755,94,5657,
2025-01-14 12:17:59,39703.14311981201,46113.33751678467,32.13631088899389,206,61835,
2025-01-14 12:19:45,4567.609548568726,11893.025398254395,39.45168519168791,289,7110,
2025-01-14 12:20:57,3380.8610439300537,6155.844211578369,34.59480443672603,96,3345,
2025-01-14 12:22:03,3476.59969329834,5182.219743728638,26.383367144779573,45,16,
2025-01-14 12:23:08,33898.698806762695,38057.151794433594,23.806930195800714,99,56480,
2025-01-14 12:24:46,3137.655258178711,4530.70592880249,35.17459991463111,49,13,
2025-01-14 12:25:51,4572.792530059814,5987.79034614563,35.33574358320257,50,3319,
2025-01-14 12:26:57,3298.43807220459,4408.785581588745,33.32290088219474,37,41,
2025-01-14 12:28:01,4435.663938522339,5912.612676620483,33.85357846907037,50,67,
2025-01-14 12:29:07,5296.810865402222,7184.149503707886,26.492331044993023,50,5544,
2025-01-14 12:30:15,3736.079216003418,5085.269451141357,35.57689549620298,48,268,
2025-01-14 12:31:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 345651 tokens (345651 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:32:22,4731.676340103149,6322.384834289551,32.06118543176884,51,1508,
2025-01-14 12:33:28,5735.30387878418,11221.306562423706,35.72728839242375,196,11459,
2025-01-14 12:34:39,2708.233594894409,3697.409152984619,32.350172563687316,32,4,
2025-01-14 12:35:43,3662.8332138061523,5164.788007736206,30.6267540047828,46,24,
2025-01-14 12:36:48,3352.091073989868,4763.3795738220215,34.720044842587214,49,5,
2025-01-14 12:37:53,3737.8618717193604,5123.692035675049,33.91468970905068,47,122,
2025-01-14 12:38:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 498221 tokens (498221 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:40:00,4384.55057144165,6210.696458816528,30.665677034435156,56,681,
2025-01-14 12:41:06,5089.389324188232,7604.719161987305,36.97328223219247,93,6,
2025-01-14 12:42:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 245631 tokens (245631 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:43:16,2773.6711502075195,3679.858684539795,35.31277885385966,32,4,
2025-01-14 12:44:19,3927.1953105926514,5321.080684661865,34.436117124804944,48,52,
2025-01-14 12:45:25,4676.6862869262695,6174.620866775513,32.0441230516428,48,255,
2025-01-14 12:46:31,2755.838632583618,3689.0885829925537,33.21725330541543,31,4,
2025-01-14 12:47:35,4071.129322052002,6727.656364440918,37.266701381278985,99,5,
2025-01-14 12:48:41,7397.352695465088,11709.436178207397,40.119816949828405,173,211,
2025-01-14 12:49:53,3924.9677658081055,5515.153646469116,33.958294220014835,54,10,
2025-01-14 12:50:59,2986.910581588745,3952.7907371520996,32.09507910628839,31,4,
2025-01-14 12:52:03,3878.720998764038,5026.285409927368,27.013734216952678,31,8,
2025-01-14 12:53:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 113906 tokens (113906 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:54:09,6739.315986633301,9150.184869766235,32.353480749495866,78,15112,
2025-01-14 12:55:18,9720.7510471344,12750.688076019287,32.34390651216507,98,21496,
2025-01-14 12:56:31,3708.749771118164,5469.204425811768,35.2181749383319,62,191,
2025-01-14 12:57:37,5066.976547241211,7841.475963592529,34.600836256886815,96,2247,
2025-01-14 12:58:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 358021 tokens (358021 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 12:59:47,3443.2034492492676,4974.15828704834,33.31254374121088,51,202,
2025-01-14 13:00:52,3492.561101913452,4503.6115646362305,30.66117977585056,31,5,
2025-01-14 13:01:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109016 tokens (109016 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:02:57,5520.214796066284,11493.981122970581,36.49289042629345,218,1502,
2025-01-14 13:04:09,3328.007459640503,4298.018932342529,34.02021618164626,33,4,
2025-01-14 13:05:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 404251 tokens (404251 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:06:15,3897.9852199554443,6224.668025970459,37.822087210383636,88,83,
2025-01-14 13:07:21,4415.791988372803,5957.056283950806,36.98262534435986,57,12,
2025-01-14 13:08:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110478 tokens (110478 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:09:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 533516 tokens (533516 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:10:31,4317.053318023682,5561.87629699707,33.73973706256419,42,8,
2025-01-14 13:11:36,3164.3764972686768,4427.445888519287,31.668885555364906,40,14,
2025-01-14 13:12:41,5848.210096359253,7779.4201374053955,29.515173797006007,57,14535,
2025-01-14 13:13:49,11969.011068344116,15001.729011535645,21.762656876209157,66,27996,
2025-01-14 13:15:04,4498.870849609375,7017.833232879639,34.538030650164586,87,390,
2025-01-14 13:16:11,7498.744010925293,14590.75403213501,38.21201594322822,271,324,
2025-01-14 13:17:25,4397.902250289917,4855.895757675171,102.62154210073692,47,235,
2025-01-14 13:18:30,7330.993175506592,11874.433994293213,37.19647877907947,169,6204,
2025-01-14 13:19:42,5499.968767166138,7847.383737564087,28.9680354166235,68,13937,
2025-01-14 13:20:50,4353.068113327026,6937.033414840698,35.991195371517236,93,1980,
2025-01-14 13:21:57,7176.882266998291,11370.964527130127,37.910558291005486,159,8519,
2025-01-14 13:23:08,4177.018880844116,5690.22011756897,32.381681174180606,49,173,
2025-01-14 13:24:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 169874 tokens (169874 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:25:15,13266.390085220337,15579.814195632935,27.664620469692277,64,17491,
2025-01-14 13:26:30,33950.225830078125,38185.6005191803,23.37455532676527,99,59385,
2025-01-14 13:28:09,5271.154880523682,10823.499202728271,36.0208208270104,200,3323,
2025-01-14 13:29:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 536850 tokens (536850 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:30:22,3466.794729232788,5011.8443965911865,20.71130830034158,32,20,
2025-01-14 13:31:27,7744.59171295166,13341.022729873657,35.91574690945602,201,12896,
2025-01-14 13:32:40,3993.5498237609863,5929.541349411011,32.54146475607493,63,45,
2025-01-14 13:33:46,6103.08575630188,7754.7361850738525,32.08911466780916,53,517,
2025-01-14 13:34:54,4774.912118911743,9790.672063827515,37.08311443184976,186,477,
2025-01-14 13:36:04,3533.0562591552734,9310.617685317993,39.80918298133249,230,826,
2025-01-14 13:37:13,7333.544492721558,9755.669116973877,34.26743577474718,83,281,
2025-01-14 13:38:23,5059.9305629730225,12447.010040283203,34.655102978967264,256,3725,
2025-01-14 13:39:35,4727.205038070679,6222.14150428772,32.77731268673593,49,239,
2025-01-14 13:40:41,4849.603652954102,6555.393934249878,31.0706424940699,53,45,
2025-01-14 13:41:48,3993.0412769317627,5024.413347244263,30.05704817138123,31,4,
2025-01-14 13:42:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 289643 tokens (289643 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:43:55,4184.711933135986,6731.561899185181,36.90833824256153,94,395,
2025-01-14 13:45:01,6008.226633071899,9711.25602722168,36.456637425908454,135,10635,
2025-01-14 13:46:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 138072 tokens (138072 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:47:13,2786.8192195892334,3836.27986907959,30.491853139553136,32,4,
2025-01-14 13:48:16,5129.452228546143,6931.326150894165,35.51857830130622,64,116,
2025-01-14 13:49:23,3132.819175720215,4445.0109004974365,36.58002035346568,48,5,
2025-01-14 13:50:28,4502.095699310303,9005.166292190552,36.19752269878188,163,311,
2025-01-14 13:51:37,3329.9620151519775,4369.065761566162,30.795770018564504,32,5,
2025-01-14 13:52:41,3122.757911682129,4510.697603225708,30.26068081768758,42,23,
2025-01-14 13:53:46,11493.265390396118,15219.73967552185,26.298316451872,98,23135,
2025-01-14 13:55:01,4742.215156555176,6368.124723434448,33.21217926261801,54,26,
2025-01-14 13:56:07,3071.0973739624023,4121.002674102783,30.47893938217222,32,5,
2025-01-14 13:57:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 336683 tokens (336683 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:58:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 757103 tokens (757103 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 13:59:20,3771.0793018341064,4776.005268096924,30.848043578060555,31,4,
2025-01-14 14:00:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 312198 tokens (312198 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:01:28,4933.158874511719,7182.861328125,27.114696835586845,61,2162,
2025-01-14 14:02:35,3488.133192062378,4603.005647659302,34.084616414398795,38,25,
2025-01-14 14:03:40,3885.810136795044,4944.667816162109,27.388005550788293,29,17,
2025-01-14 14:04:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 328078 tokens (328078 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:05:57,4404.167413711548,6511.595010757446,24.20011964894529,51,7491,
2025-01-14 14:07:04,4151.283979415894,5873.098611831665,29.61991322401823,51,39,
2025-01-14 14:08:09,3421.4837551116943,4702.328681945801,37.475262613283476,48,65,
2025-01-14 14:09:14,3575.603723526001,4917.3290729522705,35.77483277074933,48,58,
2025-01-14 14:10:19,4595.014810562134,7677.722454071045,27.57316289106424,85,124,
2025-01-14 14:11:27,33427.23369598389,36550.57334899902,21.131227254226566,66,59162,
2025-01-14 14:13:03,3331.9716453552246,5611.000537872314,36.85780389875865,84,178,
2025-01-14 14:14:09,24524.99556541443,30613.969802856445,31.69663599711328,193,47377,
2025-01-14 14:15:40,5174.860000610352,6729.916334152222,32.15317601139093,50,6673,
2025-01-14 14:16:46,7215.320110321045,9843.62006187439,35.76456330429307,94,652,
2025-01-14 14:17:56,7974.8899936676025,9526.408195495605,31.58196915915525,49,264,
2025-01-14 14:19:06,3833.322763442993,5249.6337890625,21.887847682637595,31,17,
2025-01-14 14:20:11,4062.3738765716553,6048.6695766448975,35.744929618174154,71,17,
2025-01-14 14:21:17,3422.189474105835,4432.478904724121,30.68427626826536,31,6,
2025-01-14 14:22:21,21934.502840042114,23293.333530426025,35.32448916534152,48,120,
2025-01-14 14:23:45,66497.1079826355,155483.49142074585,45.9171374555501,4086,47405,
2025-01-14 14:27:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 260567 tokens (260567 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:28:23,4614.646673202515,6574.855804443359,31.629278229488186,62,297,
2025-01-14 14:29:29,17739.32456970215,19435.00852584839,27.12769666379555,46,6,
2025-01-14 14:30:49,47852.52594947815,51560.93335151672,40.44863029815507,150,435,
2025-01-14 14:32:40,11019.928455352783,13393.091917037964,29.49649323788808,70,22077,
2025-01-14 14:33:54,22711.253881454468,27574.031352996826,43.39084015972373,211,617,
2025-01-14 14:35:21,10633.813619613647,12156.930208206177,33.4839764611373,51,9632,
2025-01-14 14:36:33,3633.1281661987305,4528.531312942505,39.08853808173569,35,7,
2025-01-14 14:37:38,12572.706699371338,15301.812648773193,32.24499218115265,88,23492,
2025-01-14 14:38:53,7959.888458251953,13541.640281677246,38.87668367649423,217,4,
2025-01-14 14:40:07,3671.985864639282,4721.644639968872,31.438788276350184,33,4,
2025-01-14 14:41:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 308413 tokens (308413 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:42:14,4038.6314392089844,5581.688642501831,33.051269837026936,51,159,
2025-01-14 14:43:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 149417 tokens (149417 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:44:22,19100.06332397461,24315.138816833496,43.91115724280014,229,20990,
2025-01-14 14:45:46,4520.544767379761,5734.897136688232,39.527242020645296,48,27,
2025-01-14 14:46:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 165070 tokens (165070 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:47:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 211786 tokens (211786 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:48:55,4484.766006469727,6086.596488952637,37.45714709274184,60,15,
2025-01-14 14:50:01,3862.7452850341797,6142.248630523682,29.39236747889588,67,3147,
2025-01-14 14:51:07,24759.351015090942,27201.853275299072,38.075706833563096,93,1331,
2025-01-14 14:52:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 177522 tokens (177522 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:53:36,4338.957786560059,9547.2731590271,42.24014566456482,220,1169,
2025-01-14 14:54:45,5022.370338439941,7307.027339935303,40.268626730307375,92,8838,
2025-01-14 14:55:53,5447.981119155884,11008.698463439941,32.90942313191091,183,6168,
2025-01-14 14:57:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140445 tokens (140445 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 14:58:10,6766.811847686768,8381.393432617188,35.30326403571387,57,14,
2025-01-14 14:59:18,14434.48805809021,15421.273469924927,34.45531276833963,34,8,
2025-01-14 15:00:34,9250.319242477417,10824.300289154053,31.766583279749096,50,8664,
2025-01-14 15:01:45,3998.565435409546,5450.15287399292,33.06724674253445,48,190,
2025-01-14 15:02:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95277 tokens (95277 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 15:03:51,8791.996240615845,10276.56078338623,34.35350806966434,51,869,
2025-01-14 15:05:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87056 tokens (87056 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 15:06:03,7017.672300338745,8722.312450408936,35.19804458291648,60,1064,
2025-01-14 15:07:11,3315.1614665985107,4845.372200012207,37.24977139118649,57,11,
2025-01-14 15:08:16,3547.4753379821777,4966.920852661133,40.86102594301989,58,8,
2025-01-14 15:09:21,10110.220670700073,12547.05286026001,36.52282680001546,89,1155,
2025-01-14 15:10:34,4033.9486598968506,6614.687919616699,37.973615362691994,98,674,
2025-01-14 15:11:40,21966.089963912964,25387.532234191895,25.72014754258176,88,45369,
2025-01-14 15:13:06,4878.138542175293,6058.033466339111,30.511191516069026,36,2964,
2025-01-14 15:14:12,3871.617078781128,5208.01854133606,36.66562883455831,49,191,
2025-01-14 15:15:17,5048.5169887542725,7863.338470458984,32.684133113934806,92,716,
2025-01-14 15:16:25,3849.660634994507,5389.357805252075,36.3707884133034,56,7,
2025-01-14 15:17:30,3516.6995525360107,4765.7599449157715,39.22948826088641,49,11,
2025-01-14 15:18:35,5399.445533752441,6701.19309425354,37.641706800001835,49,289,
2025-01-14 15:19:42,3935.37974357605,5422.3034381866455,34.29900282364939,51,64,
2025-01-14 15:20:47,3359.3647480010986,4673.884391784668,36.51523978891792,48,175,
2025-01-14 15:21:52,3627.8913021087646,4694.534540176392,33.7507413117989,36,7,
2025-01-14 15:22:56,4544.520854949951,5788.699388504028,38.57967221383001,48,294,
2025-01-14 15:24:02,7670.061111450195,12485.275745391846,40.911987310260194,197,339,
2025-01-14 15:25:15,4882.05099105835,8521.422386169434,40.391590755884685,147,123,
2025-01-14 15:26:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 402328 tokens (402328 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 15:27:25,5244.6160316467285,6680.858850479126,23.672877283829006,34,4,
2025-01-14 15:28:32,29593.026876449585,35696.35009765625,33.26056848745193,203,41949,
2025-01-14 15:30:08,12601.299047470093,17969.157695770264,39.494333567657115,212,10933,
2025-01-14 15:31:26,3459.6567153930664,4846.118927001953,36.78426975721053,51,37,
2025-01-14 15:32:31,6030.616521835327,7317.791700363159,38.06785651044194,49,236,
2025-01-14 15:33:38,17825.5295753479,22284.205198287964,35.212249842134455,157,36550,
2025-01-14 15:35:00,3963.2575511932373,6139.060258865356,39.525642511958715,86,99,
2025-01-14 15:36:06,7131.7009925842285,8819.169521331787,34.96361502148421,59,5641,
2025-01-14 15:37:15,4339.823246002197,8580.666542053223,39.61476250641888,168,213,
2025-01-14 15:38:24,3753.368377685547,5440.3791427612305,32.0092800341896,54,62,
2025-01-14 15:39:29,4850.6574630737305,6646.629333496094,33.408095632305,60,63,
2025-01-14 15:40:36,3602.7097702026367,4970.904588699341,35.082723126184405,48,65,
2025-01-14 15:41:41,4285.934209823608,5249.931812286377,36.307144240383906,35,9,
2025-01-14 15:42:46,8797.90210723877,10894.10376548767,31.008467026163352,65,19264,
2025-01-14 15:43:57,6596.1339473724365,8394.891738891602,37.247927606425684,67,488,
2025-01-14 15:45:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 138751 tokens (138751 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 15:46:07,8381.345272064209,11430.45973777771,29.516766593063853,90,5504,
2025-01-14 15:47:19,7112.410306930542,11949.621677398682,37.004791870956964,179,857,
2025-01-14 15:48:31,4669.5520877838135,6419.039249420166,29.15139997500641,51,2117,
2025-01-14 15:49:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110985 tokens (110985 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 15:50:38,14406.407117843628,17381.44016265869,28.907241937995348,86,10030,
2025-01-14 15:51:56,3659.2562198638916,5045.210361480713,34.63318053511088,48,87,
2025-01-14 15:53:01,2808.1631660461426,3621.9146251678467,31.950787563640432,26,4,
2025-01-14 15:54:04,4443.181037902832,7578.993082046509,36.03532303890295,113,11,
2025-01-14 15:55:12,7311.902046203613,8603.78623008728,34.0587806158653,44,97,
2025-01-14 15:56:21,4852.39839553833,6414.698362350464,32.644179148301,51,3873,
2025-01-14 15:57:27,14341.188669204712,17449.549198150635,28.310744259077858,88,32638,
2025-01-14 15:58:45,3836.1473083496094,5148.286819458008,37.34359005667654,49,8,
2025-01-14 15:59:50,3488.9190196990967,5096.9297885894775,29.85054635742442,48,189,
2025-01-14 16:00:55,9513.258934020996,12322.330951690674,34.88696600997004,98,11290,
2025-01-14 16:02:07,5087.109327316284,6338.7651443481445,38.3492005924007,48,7,
2025-01-14 16:03:13,3138.6287212371826,4635.317802429199,38.08406215845656,57,1509,
2025-01-14 16:04:18,9148.850202560425,11454.551935195923,24.72132418222499,57,1364,
2025-01-14 16:05:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 312355 tokens (312355 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:06:32,4006.5360069274902,5739.723920822144,34.61828894546914,60,351,
2025-01-14 16:07:37,8094.15078163147,14613.555431365967,40.647883393891206,265,23,
2025-01-14 16:08:52,3469.1126346588135,4505.425214767456,31.843674035627764,33,46,
2025-01-14 16:09:56,3813.1110668182373,5168.518543243408,35.41370461272498,48,66,
2025-01-14 16:11:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247280 tokens (247280 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:12:03,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 484957 tokens (484957 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:13:06,4243.087291717529,5912.468910217285,35.9414524127329,60,14,
2025-01-14 16:14:12,4675.942659378052,7306.532621383667,35.353286275408315,93,737,
2025-01-14 16:15:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 127973 tokens (127973 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:16:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 377293 tokens (377293 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:17:23,3052.518844604492,4634.570121765137,27.179902839289387,43,4,
2025-01-14 16:18:27,4549.107074737549,6125.367164611816,30.451827276695678,48,225,
2025-01-14 16:19:33,11308.343410491943,13346.750736236572,28.453586909482205,58,14499,
2025-01-14 16:20:47,3622.2875118255615,5031.430006027222,35.48257199377637,50,6,
2025-01-14 16:21:52,4264.081001281738,5718.015670776367,35.07722944506648,51,44,
2025-01-14 16:22:58,4648.701190948486,7004.323720932007,36.932912167642,87,16,
2025-01-14 16:24:05,4705.995321273804,6587.601900100708,34.544947244245144,65,719,
2025-01-14 16:25:11,3521.0397243499756,5343.89328956604,27.978111337733772,51,106,
2025-01-14 16:26:17,4310.11700630188,9273.698329925537,38.480280174106355,191,956,
2025-01-14 16:27:26,5755.273103713989,8229.716300964355,36.37181896113404,90,284,
2025-01-14 16:28:34,3990.164041519165,5522.98378944397,33.27201392665115,51,58,
2025-01-14 16:29:40,3703.6688327789307,5331.865072250366,35.00806513255676,57,727,
2025-01-14 16:30:45,4357.6226234436035,6827.113389968872,28.34592497727557,70,502,
2025-01-14 16:31:52,5570.831060409546,7342.767238616943,29.34642942535689,52,1988,
2025-01-14 16:32:59,3064.7034645080566,4503.950119018555,22.928661947262697,33,5,
2025-01-14 16:34:04,3415.05765914917,5131.709337234497,33.204173407837246,57,2533,
2025-01-14 16:35:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 124811 tokens (124811 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 16:36:10,3288.119316101074,4807.594537734985,33.56421958968113,51,55,
2025-01-14 16:37:15,3777.066469192505,5277.915954589844,33.98075589605051,51,67,
2025-01-14 16:38:20,5171.250581741333,8967.170715332031,39.51611064538114,150,341,
2025-01-14 16:39:29,3876.024007797241,5164.879560470581,35.69057828442214,46,4,
2025-01-14 16:40:34,5286.895275115967,10184.537649154663,37.77327658316653,185,168,
2025-01-14 16:41:44,4090.710163116455,5632.534027099609,33.077708285203464,51,47,
2025-01-14 16:42:50,3350.9368896484375,4728.304386138916,33.397041905814994,46,26,
2025-01-14 16:43:55,4083.428382873535,6770.424127578735,37.588448064729285,101,420,
2025-01-14 16:45:01,4007.9853534698486,5418.5631275177,34.02860932811754,48,45,
2025-01-14 16:46:07,5188.254117965698,6683.448076248169,32.771668002381645,49,266,
2025-01-14 16:47:14,4492.758989334106,6230.968475341797,34.51827900088594,60,34,
2025-01-14 16:48:20,8930.214643478394,11936.21039390564,32.93417829547131,99,12383,
2025-01-14 16:49:32,4448.548793792725,7078.7513256073,34.97829497431487,92,7585,
2025-01-14 16:50:39,6488.135099411011,8680.866956710815,30.555491669877863,67,324,
2025-01-14 16:51:47,10989.92943763733,16363.358497619629,35.359171560483176,190,21993,
2025-01-14 16:53:04,3734.910488128662,5760.5109214782715,39.494462324787804,80,35,
2025-01-14 16:54:10,4408.065557479858,5783.815145492554,37.070699816578376,51,72,
2025-01-14 16:55:15,4114.286184310913,5692.171335220337,34.22302311982388,54,1816,
2025-01-14 16:56:21,3493.60990524292,4967.774152755737,32.560822229262925,48,75,
2025-01-14 16:57:26,3000.5269050598145,4818.89533996582,35.19638747100681,64,19,
2025-01-14 16:58:31,5764.32204246521,7341.243267059326,29.170765972688294,46,68,
2025-01-14 16:59:38,3800.049304962158,5207.423686981201,34.10606347057305,48,37,
2025-01-14 17:00:43,7009.5884799957275,8705.640316009521,35.37627726108722,60,75,
2025-01-14 17:01:52,3369.0237998962402,5130.850076675415,32.92038537762691,58,9,
2025-01-14 17:02:57,9830.564737319946,11531.702280044556,34.68267469160857,59,453,
2025-01-14 17:04:09,3796.156883239746,6173.99525642395,36.587852640083696,87,188,
2025-01-14 17:05:15,4705.141544342041,7531.828165054321,32.90071114305748,93,1769,
2025-01-14 17:06:23,4819.439888000488,6480.499982833862,29.499233743807046,49,589,
2025-01-14 17:07:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 217644 tokens (217644 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:08:31,4653.979778289795,5989.121437072754,35.951241341502396,48,126,
2025-01-14 17:09:37,4907.034397125244,8542.253971099854,34.66090491536291,126,2640,
2025-01-14 17:10:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 148641 tokens (148641 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:11:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87310 tokens (87310 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:12:48,3815.694808959961,5703.055143356323,39.208199224801206,74,1184,
2025-01-14 17:13:53,4317.669868469238,5726.808786392212,36.19231528653853,51,62,
2025-01-14 17:14:59,3206.4990997314453,4155.267715454102,31.619932935017733,30,5,
2025-01-14 17:16:03,3813.875198364258,5404.991865158081,34.56691840883526,55,40,
2025-01-14 17:17:09,5242.516756057739,6925.129413604736,32.09294768929416,54,4161,
2025-01-14 17:18:16,6210.8941078186035,8527.719259262085,29.782135245298957,69,13275,
2025-01-14 17:19:24,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 130147 tokens (130147 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:20:26,4116.780519485474,5786.878824234009,34.72849462519094,58,1353,
2025-01-14 17:21:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 679911 tokens (679911 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:22:35,11197.882652282715,15111.873149871826,37.81307085215539,148,26683,
2025-01-14 17:23:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 264901 tokens (264901 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:24:52,11192.986488342285,14225.224256515503,27.70231308430872,84,25168,
2025-01-14 17:26:06,3121.8366622924805,4511.078596115112,31.67194923272277,44,32,
2025-01-14 17:27:11,8441.259860992432,9339.004516601562,32.30317197524628,29,25,
2025-01-14 17:28:20,4305.366992950439,5918.326139450073,34.718796270524585,56,548,
2025-01-14 17:29:26,7692.608833312988,13132.865905761719,40.25545063101683,219,5224,
2025-01-14 17:30:39,4682.548999786377,10065.663576126099,38.267808919659075,206,2876,
2025-01-14 17:31:49,26119.75884437561,33153.36465835571,33.126678713908824,233,49358,
2025-01-14 17:33:22,7295.710325241089,8927.933692932129,32.47104596656666,53,4778,
2025-01-14 17:34:31,17845.137357711792,22941.7622089386,36.88715679254803,188,39393,
2025-01-14 17:35:54,4475.3687381744385,6821.3605880737305,31.543161585653696,74,1966,
2025-01-14 17:37:01,3879.9469470977783,6692.242622375488,31.646743542074887,89,177,
2025-01-14 17:38:08,3819.9617862701416,5281.359434127808,39.00375786395926,57,9,
2025-01-14 17:39:13,3872.1046447753906,4805.412292480469,33.21519980708,31,5,
2025-01-14 17:40:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 232003 tokens (232003 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:41:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 723840 tokens (723840 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:42:23,3831.6733837127686,6379.015922546387,34.93837151589023,89,613,
2025-01-14 17:43:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176735 tokens (176735 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:44:31,3911.72456741333,5218.235492706299,37.50447015130192,49,12,
2025-01-14 17:45:36,3283.8082313537598,4244.3177700042725,39.56233485550687,38,15,
2025-01-14 17:46:41,8703.123807907104,11754.800796508789,36.04575464928329,110,21510,
2025-01-14 17:47:52,3876.323699951172,6653.111457824707,17.646289263938638,49,11,
2025-01-14 17:48:59,8399.57880973816,9910.072803497314,29.129543170507766,44,40,
2025-01-14 17:50:09,4123.367547988892,5783.399343490601,39.75827461789846,66,1585,
2025-01-14 17:51:15,9889.150381088257,18171.25964164734,40.93160200317327,339,8,
2025-01-14 17:52:33,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 238514 tokens (238514 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:53:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 335007 tokens (335007 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 17:54:37,3742.140293121338,4912.501573562622,31.614169588769347,37,14,
2025-01-14 17:55:41,4694.483518600464,6094.585418701172,34.28321895466852,48,154,
2025-01-14 17:56:48,4598.683834075928,9181.387186050415,41.242032373439166,189,4682,
2025-01-14 17:57:57,4315.861225128174,6684.499025344849,38.41870630945586,91,655,
2025-01-14 17:59:03,4811.10954284668,7058.330297470093,41.38445224335997,93,5473,
2025-01-14 18:00:11,13489.490270614624,17084.8171710968,43.94593436797368,158,243,
2025-01-14 18:01:28,8078.016996383667,9438.770294189453,35.27457921829032,48,9877,
2025-01-14 18:02:37,3335.552930831909,5563.487529754639,40.39615886548806,90,2850,
2025-01-14 18:03:43,15235.796451568604,17381.925106048584,25.16158567068036,54,36709,
2025-01-14 18:05:00,3814.427137374878,5743.724584579468,19.69628895485204,38,16,
2025-01-14 18:06:06,5090.435266494751,8576.282262802124,34.71179318202368,121,296,
2025-01-14 18:07:14,8021.7719078063965,10569.223403930664,34.93687716347337,89,18475,
2025-01-14 18:08:25,8204.92172241211,9610.236883163452,33.44445524580532,47,61,
2025-01-14 18:09:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106391 tokens (106391 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:10:36,4182.3272705078125,5548.853874206543,39.51624494820669,54,14,
2025-01-14 18:11:41,4472.527027130127,6797.701597213745,42.14737304497344,98,1723,
2025-01-14 18:12:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118670 tokens (118670 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:13:49,3163.9115810394287,4269.819498062134,34.36090782522161,38,8,
2025-01-14 18:14:54,19726.171016693115,22367.029905319214,29.157180768586652,77,39970,
2025-01-14 18:16:16,3612.9658222198486,4751.4777183532715,46.55199491546542,53,36,
2025-01-14 18:17:21,4156.483888626099,5610.974073410034,37.12641072790856,54,396,
2025-01-14 18:18:26,4176.530838012695,5880.8088302612305,39.312835291385596,67,5845,
2025-01-14 18:19:32,4719.613790512085,7092.111825942993,38.35619614474075,91,5216,
2025-01-14 18:20:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199283 tokens (199283 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:21:46,20860.20302772522,23903.29909324646,26.94624101061842,82,45560,
2025-01-14 18:23:10,4768.659830093384,9060.566663742065,43.10438394179349,185,1507,
2025-01-14 18:24:19,4844.531536102295,7477.582693099976,40.63726590181638,107,4724,
2025-01-14 18:25:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80237 tokens (80237 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:26:28,3391.3822174072266,4436.03253364563,33.50403427438635,35,14,
2025-01-14 18:27:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140536 tokens (140536 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:28:34,3125.4937648773193,4321.940898895264,38.447164686266944,46,47,
2025-01-14 18:29:38,7104.473829269409,8841.278553009033,28.788498393959824,50,1020,
2025-01-14 18:30:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 602906 tokens (602906 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:31:51,3128.4096240997314,3826.6656398773193,35.8034867371098,25,5,
2025-01-14 18:32:55,3942.7080154418945,5246.221303939819,36.82356016125601,48,128,
2025-01-14 18:34:00,2791.930913925171,3608.290195465088,30.623771377771234,25,4,
2025-01-14 18:35:04,5674.799919128418,8137.670516967773,37.35478432391471,92,11457,
2025-01-14 18:36:12,6254.169940948486,11356.561183929443,45.07690395486562,230,4388,
2025-01-14 18:37:23,3954.4713497161865,6274.842977523804,41.80364853523468,97,394,
2025-01-14 18:38:30,4518.905401229858,5859.929323196411,38.77634033831722,52,873,
2025-01-14 18:39:35,3394.862174987793,4825.18196105957,40.55037241657049,58,14,
2025-01-14 18:40:40,3538.5091304779053,5053.915023803711,36.95379584218117,56,511,
2025-01-14 18:41:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 445821 tokens (445821 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:42:53,3531.6104888916016,7700.690984725952,44.8540152167477,187,7,
2025-01-14 18:44:00,3395.8630561828613,4828.784227371216,41.17463066797313,59,10,
2025-01-14 18:45:05,6319.769382476807,9530.501127243042,39.55484608984253,127,11404,
2025-01-14 18:46:15,9198.520421981812,13928.141832351685,38.269447868100116,181,15893,
2025-01-14 18:47:29,6263.415813446045,8735.734701156616,35.99859243174581,89,13327,
2025-01-14 18:48:38,3683.117151260376,4915.978670120239,42.17829756588479,52,5,
2025-01-14 18:49:42,3103.450059890747,3948.167324066162,36.69866985642962,31,22,
2025-01-14 18:50:46,3773.5915184020996,7457.375288009644,37.190022153389165,137,705,
2025-01-14 18:51:54,3979.616641998291,5131.609201431274,34.722446488446245,40,23,
2025-01-14 18:52:59,14658.908128738403,16639.489889144897,32.81864010837877,65,24773,
2025-01-14 18:54:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 722267 tokens (722267 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 18:55:24,11687.323331832886,14215.752363204956,42.71426987270156,108,1642,
2025-01-14 18:56:38,3528.270483016968,4947.209119796753,41.580374563552475,59,1798,
2025-01-14 18:57:43,2723.9601612091064,3650.776147842407,33.44784773578285,31,6,
2025-01-14 18:58:46,7360.260248184204,9688.496351242065,38.22636367639394,89,9539,
2025-01-14 18:59:56,8820.235252380371,10044.373035430908,40.02817385302214,49,89,
2025-01-14 19:01:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 593116 tokens (593116 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:02:10,3230.412244796753,5922.854423522949,34.54113174084898,93,168,
2025-01-14 19:03:16,6012.195110321045,9696.481227874756,45.870487418119545,169,180,
2025-01-14 19:04:25,4170.221328735352,7083.213567733765,39.13501672740368,114,4,
2025-01-14 19:05:32,7025.251388549805,11262.747526168823,41.76994957674545,177,10617,
2025-01-14 19:06:44,2861.7498874664307,6329.259634017944,48.449755668914364,168,34,
2025-01-14 19:07:50,8889.111280441284,10015.684843063354,43.49471852149077,49,27,
2025-01-14 19:09:00,2813.1556510925293,3775.265693664551,40.53590366413886,39,15,
2025-01-14 19:10:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 320834 tokens (320834 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:11:06,8539.242267608643,10686.13076210022,42.3869242550254,91,165,
2025-01-14 19:12:17,2990.262031555176,4302.723169326782,32.00094752619548,42,13,
2025-01-14 19:13:21,3687.3297691345215,4668.608665466309,31.591426368063225,31,4,
2025-01-14 19:14:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 227354 tokens (227354 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:15:29,5373.5339641571045,8363.808870315552,42.805428937782615,128,192,
2025-01-14 19:16:37,4242.6087856292725,7497.376441955566,48.23693012152158,157,298,
2025-01-14 19:17:45,8556.015014648438,10490.258693695068,46.529814715155275,90,878,
2025-01-14 19:18:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118809 tokens (118809 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:19:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 266228 tokens (266228 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:20:58,4192.617654800415,8879.754781723022,37.97627318765216,178,4,
2025-01-14 19:22:07,6424.553871154785,8252.251863479614,38.29954417740543,70,11835,
2025-01-14 19:23:15,6658.060312271118,8215.589761734009,41.0907158269593,64,4267,
2025-01-14 19:24:24,3412.824869155884,5229.010105133057,31.384463914184366,57,423,
2025-01-14 19:25:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 471373 tokens (471373 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:26:31,7783.347845077515,10388.3798122406,41.07435200364332,107,13455,
2025-01-14 19:27:42,3304.367780685425,4189.0857219696045,35.03941601433231,31,6,
2025-01-14 19:28:46,3027.2440910339355,4617.351770401001,33.331076057123624,53,7,
2025-01-14 19:29:50,21959.480047225952,24554.960250854492,34.290379050310605,89,43688,
2025-01-14 19:31:15,3356.3456535339355,5049.962043762207,28.34171910294892,48,314,
2025-01-14 19:32:20,3094.2747592926025,4266.48211479187,41.8014780150649,49,11,
2025-01-14 19:33:24,4598.755836486816,6155.703783035278,32.756393759380295,51,5,
2025-01-14 19:34:31,16094.521284103394,18733.52551460266,33.34591092464883,88,28209,
2025-01-14 19:35:49,3819.7121620178223,5480.663299560547,30.103233544830182,50,10,
2025-01-14 19:36:55,3115.3674125671387,3985.362768173218,39.08066839772268,34,5,
2025-01-14 19:37:59,3411.6106033325195,4223.793983459473,38.168719969564464,31,46,
2025-01-14 19:39:03,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 184255 tokens (184255 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:40:04,3805.2337169647217,4932.244777679443,42.59053142704706,48,21,
2025-01-14 19:41:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 384448 tokens (384448 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:42:16,4392.897129058838,6383.409738540649,48.22878264759729,96,540,
2025-01-14 19:43:22,3638.6520862579346,4969.848155975342,41.31622775274244,55,284,
2025-01-14 19:44:27,3685.9538555145264,4866.77098274231,44.037301628645025,52,8,
2025-01-14 19:45:32,2796.4026927948,3427.828550338745,34.841778709496175,22,4,
2025-01-14 19:46:36,13484.094381332397,17776.561975479126,42.166887933368265,181,26646,
2025-01-14 19:47:53,4787.883520126343,6782.466888427734,52.14121487865785,104,26,
2025-01-14 19:49:00,8229.832172393799,9444.831848144531,38.68313789545608,47,7,
2025-01-14 19:50:10,3147.4592685699463,6683.435678482056,44.96636333723508,159,1628,
2025-01-14 19:51:16,3486.1934185028076,4798.748254776001,38.85551947285266,51,2838,
2025-01-14 19:52:21,3379.857301712036,4440.154075622559,45.27034428575058,48,85,
2025-01-14 19:53:26,3106.9343090057373,6744.431018829346,49.209666504050304,179,304,
2025-01-14 19:54:32,3563.506603240967,4670.247554779053,34.335044661706746,38,52,
2025-01-14 19:55:37,3519.4222927093506,6486.829042434692,45.49426869521523,135,357,
2025-01-14 19:56:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 152875 tokens (152875 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 19:57:46,3900.5916118621826,5448.844432830811,42.628696751676934,66,390,
2025-01-14 19:58:52,3758.7075233459473,4448.962450027466,40.56472314455368,28,6,
2025-01-14 19:59:56,3285.627841949463,7357.547044754028,46.66104373317036,190,2923,
2025-01-14 20:01:04,9224.30396080017,11891.413450241089,40.118338007349465,107,8158,
2025-01-14 20:02:15,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:03:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:04:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:05:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:06:21,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:07:22,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:08:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:09:24,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:10:24,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:11:27,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:12:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:13:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:14:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:15:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:16:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:17:48,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:18:49,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:19:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:20:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:21:57,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:22:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:23:59,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:25:05,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:26:06,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:27:07,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:28:08,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:29:09,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:30:10,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:31:15,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:32:16,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:33:17,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:34:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:35:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:36:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:37:21,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:38:22,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:39:25,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:40:31,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:41:33,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:42:34,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:43:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:44:37,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:45:37,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:46:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:47:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:48:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:49:42,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:50:43,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:51:44,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:52:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:53:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:54:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:55:47,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:56:48,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:57:51,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:58:53,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 20:59:54,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:00:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:01:56,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:02:57,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:03:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:04:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:05:59,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:07:01,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:08:02,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:09:03,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:10:09,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:11:09,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:12:10,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:13:11,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:14:13,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:15:14,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:16:17,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:17:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:18:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:19:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:20:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:21:21,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:22:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:23:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:24:25,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:25:26,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:26:27,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:27:28,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:28:29,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:29:30,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:30:35,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:31:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:32:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:33:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:34:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:35:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:36:42,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:37:43,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:38:44,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:39:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:40:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:41:48,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:42:49,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:43:50,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:44:51,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:45:52,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:46:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:48:04,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:49:04,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:50:05,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:51:06,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:52:07,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:53:08,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:54:09,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:55:15,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:56:16,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:57:16,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:58:17,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 21:59:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:00:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:01:21,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:02:22,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:03:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:04:24,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:05:25,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:06:26,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:07:27,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:08:28,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:09:28,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:10:29,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:11:30,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:12:33,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:13:34,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:14:34,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:15:35,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:16:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:17:37,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:18:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:19:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:20:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:21:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:22:43,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:23:44,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:24:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:25:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:26:52,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:27:53,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:28:54,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:29:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:30:56,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:31:57,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:32:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:33:59,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:35:01,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:36:02,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:37:03,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:38:04,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:39:05,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:40:05,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:41:06,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:42:07,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:43:08,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:44:09,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:45:15,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:46:16,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:47:17,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:48:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:49:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:50:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:51:22,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:52:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:53:23,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:54:24,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:55:25,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:56:26,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:57:27,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:58:28,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 22:59:29,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:00:30,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:01:31,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:02:32,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:03:33,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:04:34,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:05:35,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:06:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:07:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:08:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:09:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:10:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:11:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:12:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:13:42,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:14:43,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:15:44,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:16:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:17:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:18:47,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:19:47,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:20:48,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:21:49,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:22:50,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:23:51,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:24:52,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:25:53,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:26:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:27:56,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:28:56,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:29:57,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:30:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:32:00,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:33:01,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:34:02,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:35:03,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:36:03,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:37:05,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:38:06,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:39:06,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:40:07,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:41:08,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:42:10,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:43:11,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:44:11,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:45:12,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:46:13,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:47:14,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:48:15,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:49:16,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:50:17,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:51:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:52:18,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:53:19,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:54:20,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:55:21,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:56:22,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:57:28,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:58:29,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-14 23:59:29,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:00:30,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:01:31,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:02:32,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:03:32,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:04:33,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:05:34,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:06:35,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:07:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:08:36,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:09:37,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:10:38,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:11:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:12:39,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:13:40,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:14:41,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:15:42,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:16:43,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:17:45,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:18:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:19:46,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:20:47,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:21:48,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:22:49,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:23:50,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:24:50,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:25:51,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:26:52,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:27:53,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:28:53,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:29:55,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:30:57,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:31:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:32:58,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:33:59,,,,,,"litellm.BadRequestError: DeepseekException - Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:35:01,7557.767152786255,8992.921352386475,39.71698652024853,57,16544,
2025-01-15 00:36:10,36250.10633468628,38849.87187385559,31.925955917748077,83,64008,
2025-01-15 00:37:48,5943.511962890625,7763.947486877441,48.3400806238262,88,12547,
2025-01-15 00:38:56,2642.7369117736816,3705.4266929626465,48.93243627676654,52,18,
2025-01-15 00:40:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 197054 tokens (197054 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:41:01,2902.2340774536133,4272.63617515564,44.51248294372032,61,11,
2025-01-15 00:42:05,3676.6815185546875,4394.235134124756,44.59597067820116,32,4,
2025-01-15 00:43:10,2425.4186153411865,3075.43683052063,47.69097123138645,31,7,
2025-01-15 00:44:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 562789 tokens (562789 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:45:15,2304.4800758361816,3048.624277114868,47.03389469387572,35,5,
2025-01-15 00:46:19,2910.822868347168,4009.140729904175,49.16609470727177,54,10,
2025-01-15 00:47:23,2847.9020595550537,3920.185327529907,50.35982712104239,54,105,
2025-01-15 00:48:26,3108.5939407348633,4871.530294418335,50.4839552568326,89,1336,
2025-01-15 00:49:31,3307.6443672180176,4398.14019203186,47.68473094234621,52,105,
2025-01-15 00:50:36,2749.2151260375977,3510.899066925049,48.5765788325413,37,7,
2025-01-15 00:51:39,2636.4200115203857,3585.317850112915,50.58500298746271,48,139,
2025-01-15 00:52:43,2551.133394241333,3211.212635040283,39.38921025380223,26,4,
2025-01-15 00:53:46,5508.9099407196045,7302.928924560547,50.16669322378775,90,5125,
2025-01-15 00:54:53,3092.9489135742188,4864.292860031128,51.937965059818524,92,15,
2025-01-15 00:55:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 421579 tokens (421579 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 00:57:01,2568.2291984558105,5634.075880050659,57.08047993742638,175,5,
2025-01-15 00:58:06,2806.727409362793,3564.8951530456543,51.439803823035696,39,9,
2025-01-15 00:59:10,5535.655498504639,9347.008466720581,54.311422144901655,207,16277,
2025-01-15 01:00:19,3081.7980766296387,4384.192228317261,52.97935337823106,69,13,
2025-01-15 01:01:24,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173813 tokens (173813 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:02:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 606011 tokens (606011 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:03:29,19732.224941253662,21998.60668182373,37.06348250885188,84,48819,
2025-01-15 01:04:51,4799.774408340454,6160.097122192383,25.729188848793825,35,5,
2025-01-15 01:05:57,2617.748260498047,3409.9714756011963,42.91719726437594,34,6,
2025-01-15 01:07:01,2629.0740966796875,3280.8406352996826,46.02875143532207,30,9,
2025-01-15 01:08:04,3937.9122257232666,5000.435590744019,48.94010024803963,52,13,
2025-01-15 01:09:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 577159 tokens (577159 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:10:12,5968.199729919434,7213.273286819458,40.96143534441407,51,10555,
2025-01-15 01:11:19,2846.527576446533,4319.842100143433,40.04576012184747,59,9,
2025-01-15 01:12:23,2999.929666519165,4096.832990646362,53.7877848505438,59,887,
2025-01-15 01:13:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 307167 tokens (307167 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:14:29,6057.731866836548,7245.443344116211,45.46558742000348,54,3832,
2025-01-15 01:15:36,2801.2876510620117,4647.4597454071045,52.54114732700993,97,57,
2025-01-15 01:16:41,2583.050012588501,4160.28356552124,55.159871433263945,87,33,
2025-01-15 01:17:45,2846.534490585327,3924.367904663086,48.2449322138463,52,117,
2025-01-15 01:18:49,2962.0935916900635,4214.582681655884,51.098249487942866,64,23,
2025-01-15 01:19:53,2899.2249965667725,3903.2514095306396,49.799486701152546,50,1673,
2025-01-15 01:20:57,7908.610582351685,10038.771390914917,42.250331354421974,90,25144,
2025-01-15 01:22:07,6953.2012939453125,8543.594360351562,36.468971869363315,58,13666,
2025-01-15 01:23:16,4279.5703411102295,5550.893306732178,50.34126003433781,64,2412,
2025-01-15 01:24:21,3512.4576091766357,4801.619291305542,44.9904777686376,58,436,
2025-01-15 01:25:26,20212.14723587036,22352.43320465088,36.91095542948042,79,48958,
2025-01-15 01:26:48,2533.2088470458984,4317.57926940918,56.042175294273584,100,60,
2025-01-15 01:27:53,3035.3517532348633,4314.679861068726,40.6463359020897,52,2247,
2025-01-15 01:28:57,2621.689796447754,3615.445613861084,54.339304539175274,54,8,
2025-01-15 01:30:01,2775.3195762634277,4066.95818901062,51.872094360432115,67,5,
2025-01-15 01:31:05,5476.318359375,7191.727161407471,57.129239329940766,98,538,
2025-01-15 01:32:12,3285.9866619110107,7423.821687698364,51.71786662985186,214,6,
2025-01-15 01:33:19,3262.8965377807617,4034.888744354248,45.33724524933832,35,6,
2025-01-15 01:34:23,2590.6050205230713,3460.6094360351562,40.22968087972173,35,13,
2025-01-15 01:35:27,6091.472625732422,9423.374652862549,53.12281050246118,177,6,
2025-01-15 01:36:36,3921.4725494384766,6610.1696491241455,56.16103056668345,151,105,
2025-01-15 01:37:43,2847.26619720459,3598.1171131134033,43.95013617324755,33,26,
2025-01-15 01:38:47,3798.0897426605225,5547.889471054077,54.863421477459646,96,861,
2025-01-15 01:39:52,3371.1767196655273,4549.016237258911,44.14862909868129,52,5363,
2025-01-15 01:40:57,4485.359668731689,6468.340158462524,44.37764287431033,88,10432,
2025-01-15 01:42:03,2887.3565196990967,4519.689559936523,52.07270691992888,85,229,
2025-01-15 01:43:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 309162 tokens (309162 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:44:09,2942.5485134124756,3968.395233154297,51.66463856641147,53,8,
2025-01-15 01:45:13,3055.760145187378,4480.912685394287,51.92426628889592,74,850,
2025-01-15 01:46:18,3295.501708984375,4545.141220092773,39.21130819282294,49,3200,
2025-01-15 01:47:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67240 tokens (67240 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:48:23,3426.6879558563232,4360.498666763306,44.97699534759743,42,6,
2025-01-15 01:49:28,3576.4079093933105,4396.24285697937,41.47176221276046,34,4,
2025-01-15 01:50:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 214887 tokens (214887 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:51:33,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 309044 tokens (309044 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:52:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 393955 tokens (393955 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 01:53:37,2941.077947616577,3548.8052368164062,37.845922683977555,23,6,
2025-01-15 01:54:41,38451.69448852539,44999.91297721863,48.71554004357293,319,59909,
2025-01-15 01:56:26,12759.741067886353,15153.346061706543,38.43574868765966,92,33584,
2025-01-15 01:57:41,3876.1672973632812,5322.157382965088,43.568763456479736,63,8674,
2025-01-15 01:58:46,3778.317451477051,4917.242765426636,44.77905563723166,51,341,
2025-01-15 01:59:51,4294.748306274414,7828.735589981079,48.953203877560895,173,7366,
2025-01-15 02:00:59,4184.417486190796,5558.55917930603,37.11407655813205,51,22,
2025-01-15 02:02:05,4073.4012126922607,4950.056552886963,39.92447019398367,35,7,
2025-01-15 02:03:09,6765.3326988220215,9515.191316604614,44.729572351317344,123,17766,
2025-01-15 02:04:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 278406 tokens (278406 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:05:21,3051.7494678497314,4164.878129959106,47.61354352296093,53,5,
2025-01-15 02:06:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 708589 tokens (708589 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:07:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 624521 tokens (624521 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:08:31,7596.831321716309,9961.464881896973,38.90666255830817,92,18221,
2025-01-15 02:09:41,3390.9435272216797,5262.368679046631,53.435212144329334,100,561,
2025-01-15 02:10:46,4062.8247261047363,5935.2147579193115,44.862447766074524,84,1409,
2025-01-15 02:11:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 81418 tokens (81418 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:12:53,4656.95858001709,6955.407619476318,39.15683944037972,90,9628,
2025-01-15 02:14:00,5580.212593078613,9664.58797454834,46.763576351611036,191,5256,
2025-01-15 02:15:10,3554.487466812134,5446.2714195251465,49.68854919463416,94,205,
2025-01-15 02:16:15,3536.8881225585938,4629.913806915283,35.68077178621271,39,9,
2025-01-15 02:17:20,4837.0983600616455,8477.54454612732,46.42288097730206,169,5683,
2025-01-15 02:18:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 630138 tokens (630138 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:19:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 422883 tokens (422883 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:20:34,13549.44372177124,15017.621040344238,29.969132095546886,44,19115,
2025-01-15 02:21:49,5004.35996055603,9431.405067443848,49.69454674354075,220,9292,
2025-01-15 02:22:58,3501.4660358428955,4669.831037521362,41.93894881274847,49,85,
2025-01-15 02:24:03,4119.146823883057,4869.903802871704,37.29569059447052,28,4,
2025-01-15 02:25:08,7063.956499099731,9357.384443283081,50.14328018966802,115,32,
2025-01-15 02:26:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144967 tokens (144967 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:27:19,7240.775346755981,10043.142080307007,44.96199533468584,126,8762,
2025-01-15 02:28:29,3049.7827529907227,4191.431283950806,48.1759477706743,55,11,
2025-01-15 02:29:33,3273.731231689453,5164.390563964844,46.015693316519545,87,226,
2025-01-15 02:30:38,37138.006925582886,39484.39073562622,38.78308382903449,91,21956,
2025-01-15 02:32:18,33339.118003845215,35202.967166900635,22.534012318437362,42,54675,
2025-01-15 02:33:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 275629 tokens (275629 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:34:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 287786 tokens (287786 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:35:57,3635.0913047790527,4439.619302749634,38.531909490033144,31,4,
2025-01-15 02:37:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 574935 tokens (574935 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:38:04,3825.835943222046,5984.756708145142,46.7826340090799,101,235,
2025-01-15 02:39:10,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 701905 tokens (701905 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:40:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250251 tokens (250251 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 02:41:14,3339.083671569824,4482.190370559692,41.990830814320546,48,73,
2025-01-15 02:42:19,4609.458208084106,5828.567028045654,42.65410859847617,52,701,
2025-01-15 02:43:25,39493.91722679138,41726.27401351929,30.013123528612013,67,51263,
2025-01-15 02:45:06,3989.8836612701416,5747.683763504028,27.87575216188048,49,1012,
2025-01-15 02:46:12,8540.36545753479,9860.432386398315,47.724852901388935,63,687,
2025-01-15 02:47:22,3433.6795806884766,4863.605737686157,50.352250462480896,72,11,
2025-01-15 02:48:27,3234.74383354187,4300.5876541137695,45.03474061916938,48,40,
2025-01-15 02:49:31,4126.582622528076,5779.592752456665,47.7916006500292,79,106,
2025-01-15 02:50:37,375363.1489276886,377357.52630233765,36.10149258370417,72,47774,
2025-01-15 02:57:54,3383.0251693725586,4393.905162811279,47.483381124912675,48,50,
2025-01-15 02:58:59,4984.2119216918945,7084.584712982178,46.658383886127886,98,1219,
2025-01-15 03:00:06,5520.495891571045,6507.427453994751,45.59586673820525,45,9,
2025-01-15 03:01:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 639261 tokens (639261 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 03:02:15,5545.288801193237,8644.819498062134,52.58860645086057,163,229,
2025-01-15 03:03:24,8073.9288330078125,9702.234983444214,39.918783075639304,65,1511,
2025-01-15 03:04:33,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 291556 tokens (291556 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 03:05:35,3236.332893371582,4447.319507598877,44.59173979759988,54,9,
2025-01-15 03:06:40,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 683751 tokens (683751 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 03:07:42,6877.28476524353,11178.720951080322,53.00555213412875,228,6,
2025-01-15 03:08:53,4085.937976837158,9476.774454116821,47.673492060677006,257,3636,
2025-01-15 03:10:03,4585.658073425293,5695.854902267456,44.13631774745984,49,450,
2025-01-15 03:11:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-15 03:42:10,4327.953577041626,6257.944822311401,47.668610013378675,92,3987,
2025-01-15 03:43:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 206548 tokens (206548 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 03:44:17,12006.403684616089,13283.424854278564,42.28590823930705,54,7610,
2025-01-15 03:45:31,3631.2265396118164,4913.701057434082,43.6655849467419,56,13,
2025-01-15 03:46:36,4094.8355197906494,5387.384414672852,46.419907391950666,60,885,
2025-01-15 03:47:41,2950.388193130493,3985.6669902801514,46.36432247251095,48,91,
2025-01-15 03:48:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-15 04:19:46,4213.1617069244385,6224.557638168335,52.20255165529018,105,100,
2025-01-15 04:20:52,6270.860433578491,7417.918682098389,43.58976544087218,50,233,
2025-01-15 04:21:59,4054.5108318328857,5291.762351989746,52.5358012829592,65,1319,
2025-01-15 04:23:05,29004.01782989502,30418.899536132812,38.16573481862833,54,12612,
2025-01-15 04:24:35,4014.965295791626,5896.2836265563965,27.108649911080228,51,119,
2025-01-15 04:25:41,3647.2294330596924,4863.063335418701,42.76899986018448,52,7,
2025-01-15 04:26:46,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 655394 tokens (655394 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 04:27:49,3768.96333694458,5144.064903259277,46.54201665373811,64,24,
2025-01-15 04:28:54,3886.8038654327393,5066.4825439453125,42.384422903229655,50,51,
2025-01-15 04:29:59,11549.96919631958,13057.948350906372,37.798930990938544,57,12256,
2025-01-15 04:31:12,3175.6930351257324,3955.690622329712,39.74371268393821,31,5,
2025-01-15 04:32:16,3465.0261402130127,4369.128942489624,30.969929447728187,28,7,
2025-01-15 04:33:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 344220 tokens (344220 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 04:34:22,6531.156778335571,9076.440811157227,34.573744566513,88,7594,
2025-01-15 04:35:31,3875.8623600006104,5693.556070327759,26.957236921494868,49,3329,
2025-01-15 04:36:37,4369.853973388672,6475.624084472656,23.2693966649457,49,196,
2025-01-15 04:37:44,7087.613582611084,8513.676881790161,34.36032610067672,49,480,
2025-01-15 04:38:52,3206.8517208099365,4088.2821083068848,35.1701058186031,31,19,
2025-01-15 04:39:56,19359.561920166016,21959.19370651245,32.3122683916921,84,37372,
2025-01-15 04:41:18,4200.995445251465,6474.379301071167,39.14869007808175,89,150,
2025-01-15 04:42:25,3375.0855922698975,4277.121067047119,38.80113474322509,35,13,
2025-01-15 04:43:29,3751.508951187134,5044.152498245239,43.32207446317972,56,561,
2025-01-15 04:44:34,2631.3390731811523,4379.350900650024,27.45976843274807,48,26,
2025-01-15 04:45:38,3626.168727874756,5105.3786277771,38.534084989400824,57,2132,
2025-01-15 04:46:43,28454.850435256958,31685.595512390137,25.69066825713484,83,51184,
2025-01-15 04:48:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 229580 tokens (229580 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 04:49:17,2911.4198684692383,4245.564222335815,38.22674799184461,51,248,
2025-01-15 04:50:21,7514.8632526397705,10338.743686676025,38.24524533626675,108,20181,
2025-01-15 04:51:32,18941.344022750854,22339.29443359375,35.903996600626286,122,40805,
2025-01-15 04:52:54,3006.9611072540283,4771.099805831909,21.54025646091932,38,13,
2025-01-15 04:53:59,3310.3978633880615,4467.625856399536,41.478429738887286,48,51,
2025-01-15 04:55:03,3180.82594871521,4708.574056625366,37.30981547604185,57,13,
2025-01-15 04:56:08,6674.426555633545,10606.656789779663,44.504006525447295,175,1866,
2025-01-15 04:57:19,3423.0575561523438,4863.943338394165,40.94703461380823,59,138,
2025-01-15 04:58:24,6724.778413772583,9095.297574996948,39.23191236807629,93,13422,
2025-01-15 04:59:33,3628.1075477600098,5978.063583374023,45.10722685597118,106,1079,
2025-01-15 05:00:39,3352.3550033569336,4368.947744369507,38.363445287986416,39,4,
2025-01-15 05:01:43,7958.841800689697,10319.276809692383,45.330627444476384,107,683,
2025-01-15 05:02:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 165320 tokens (165320 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:03:55,3170.576572418213,4462.269067764282,42.579793718832775,55,46,
2025-01-15 05:05:00,3993.5014247894287,6000.149965286255,44.3525601040053,89,16,
2025-01-15 05:06:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 108663 tokens (108663 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:07:07,3849.0777015686035,5523.568391799927,32.84580817370922,55,4357,
2025-01-15 05:08:13,2556.2663078308105,3319.3249702453613,40.62597219184502,31,5,
2025-01-15 05:09:16,3663.057327270508,5274.609327316284,56.46730604871275,91,55,
2025-01-15 05:10:21,4167.730093002319,5678.38978767395,45.01344693305068,68,191,
2025-01-15 05:11:27,7240.209102630615,9185.146808624268,45.24566505591064,88,1191,
2025-01-15 05:12:36,4744.065761566162,7077.6636600494385,37.710009962382834,88,9408,
2025-01-15 05:13:43,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87066 tokens (87066 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:14:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173362 tokens (173362 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:15:45,4803.135871887207,7214.935541152954,37.73116032796547,91,7784,
2025-01-15 05:16:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 299306 tokens (299306 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:17:54,3452.4636268615723,5745.611190795898,40.55561075207956,93,2024,
2025-01-15 05:19:00,3677.295684814453,5564.181089401245,45.04778074671371,85,99,
2025-01-15 05:20:06,3136.7835998535156,4512.79354095459,46.51129187975751,64,271,
2025-01-15 05:21:10,3615.027904510498,9137.903690338135,52.327810946175674,289,928,
2025-01-15 05:22:19,18800.493001937866,22325.480699539185,40.85120640222065,144,43376,
2025-01-15 05:23:42,3650.026559829712,4804.171562194824,42.45567056096727,49,268,
2025-01-15 05:24:47,4077.733278274536,5437.574863433838,47.06430565035454,64,296,
2025-01-15 05:25:52,3402.9316902160645,5228.817701339722,46.55274178243508,85,154,
2025-01-15 05:26:57,3646.040439605713,5803.122043609619,44.96816431049789,97,999,
2025-01-15 05:28:03,3618.159294128418,4712.94903755188,47.49770475323729,52,60,
2025-01-15 05:29:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 711145 tokens (711145 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:30:11,6488.777875900269,11138.71955871582,49.24792946249165,229,6212,
2025-01-15 05:31:22,3801.5084266662598,5281.861782073975,47.28600759020862,70,18,
2025-01-15 05:32:27,3456.3260078430176,4577.1331787109375,42.826278460397624,48,91,
2025-01-15 05:33:32,2587.28289604187,3291.724681854248,45.42603894954483,32,4,
2025-01-15 05:34:35,10481.414079666138,15377.432823181152,47.1811919237379,231,28250,
2025-01-15 05:35:50,3187.6440048217773,4210.8423709869385,45.93439703793802,47,31,
2025-01-15 05:36:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78921 tokens (78921 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:37:56,3638.3368968963623,4779.318332672119,42.945484004903854,49,1368,
2025-01-15 05:39:00,9261.148929595947,10803.585290908813,32.41624825120292,50,24981,
2025-01-15 05:40:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 443392 tokens (443392 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:41:13,3370.387315750122,6753.853797912598,49.65321834446728,168,381,
2025-01-15 05:42:20,3742.8596019744873,5544.642210006714,43.84546706568444,79,9580,
2025-01-15 05:43:26,3919.1842079162598,8180.79948425293,49.04243730317642,209,4351,
2025-01-15 05:44:34,2887.183427810669,5197.451114654541,41.98647652494101,97,158,
2025-01-15 05:45:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 384051 tokens (384051 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:46:41,5494.394779205322,7194.590330123901,41.75990224279803,71,13700,
2025-01-15 05:47:48,3225.9678840637207,4507.5953006744385,40.57341418110011,52,31,
2025-01-15 05:48:53,3015.3353214263916,3970.550298690796,43.96915982230706,42,39,
2025-01-15 05:49:57,2627.833604812622,3510.0772380828857,44.2054762757952,39,10,
2025-01-15 05:51:00,2893.383502960205,3683.875799179077,43.011171851554714,34,6,
2025-01-15 05:52:04,8336.016178131104,11635.510444641113,38.4907472908969,127,13359,
2025-01-15 05:53:16,3206.516981124878,4386.269330978394,41.534140623736924,49,361,
2025-01-15 05:54:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80898 tokens (80898 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 05:55:21,3342.2601222991943,4525.094270706177,41.42592608269911,49,3443,
2025-01-15 05:56:25,3178.3299446105957,5411.771535873413,44.32620955358143,99,1391,
2025-01-15 05:57:31,3349.287509918213,4559.614658355713,44.6160363086233,54,2062,
2025-01-15 05:58:35,4720.280408859253,5751.194477081299,43.65058290222844,45,15,
2025-01-15 05:59:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 257027 tokens (257027 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:00:43,3009.274959564209,3791.28098487854,39.64163829497273,31,4,
2025-01-15 06:01:47,6284.695386886597,9473.909616470337,49.228427034985295,157,8619,
2025-01-15 06:02:56,3665.2474403381348,4802.118301391602,44.85997640289725,51,6,
2025-01-15 06:04:01,3268.418788909912,4449.555158615112,41.4854721747582,49,249,
2025-01-15 06:05:05,3064.791679382324,3898.5674381256104,39.57898710048786,33,8,
2025-01-15 06:06:09,12048.059463500977,14941.559076309204,32.832214519565696,95,30635,
2025-01-15 06:07:24,3912.6100540161133,4951.012372970581,51.03994765089114,53,9,
2025-01-15 06:08:29,4301.320314407349,6637.521982192993,46.65693099316851,109,498,
2025-01-15 06:09:36,8477.57625579834,9968.709945678711,35.54342602523589,53,21565,
2025-01-15 06:10:46,3039.8900508880615,4106.446743011475,42.19185002759607,45,10,
2025-01-15 06:11:50,3342.3140048980713,4482.797622680664,42.08740857963828,48,55,
2025-01-15 06:12:54,3377.0365715026855,4643.414497375488,43.430952858794775,55,15,
2025-01-15 06:13:59,2644.3729400634766,3526.273012161255,37.41920546791974,33,4,
2025-01-15 06:15:02,2912.248373031616,3912.3520851135254,40.995748245600026,41,23,
2025-01-15 06:16:06,3006.380319595337,3828.5274505615234,37.70614629959096,31,6,
2025-01-15 06:17:10,5525.829076766968,6657.672643661499,43.29220170808814,49,196,
2025-01-15 06:18:17,3310.614824295044,4603.66415977478,40.215016220326504,52,167,
2025-01-15 06:19:21,3060.417652130127,4253.786563873291,42.73615601859769,51,41,
2025-01-15 06:20:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 699995 tokens (699995 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:21:29,6154.123544692993,8336.41242980957,42.615806108104685,93,1921,
2025-01-15 06:22:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156163 tokens (156163 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:23:40,2911.316394805908,4270.2484130859375,27.22726339675924,37,4,
2025-01-15 06:24:44,33918.55049133301,36659.54661369324,25.902991770328736,71,56880,
2025-01-15 06:26:21,3755.0740242004395,5143.805027008057,38.884420302295716,54,8,
2025-01-15 06:27:26,4228.012323379517,5444.38624382019,40.283665389872915,49,9,
2025-01-15 06:28:31,4904.522657394409,8039.2091274261475,46.256619724565915,145,6891,
2025-01-15 06:29:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 160282 tokens (160282 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:30:41,7783.016681671143,9306.69617652893,35.44052419307518,54,3282,
2025-01-15 06:31:50,4960.244178771973,7025.926828384399,43.085030518457714,89,491,
2025-01-15 06:32:57,3494.722604751587,4759.030818939209,40.33826516959705,51,131,
2025-01-15 06:34:02,8233.575105667114,13399.171590805054,44.3317631678861,229,16300,
2025-01-15 06:35:15,3838.3777141571045,5107.984781265259,42.532844530393504,54,5,
2025-01-15 06:36:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 375402 tokens (375402 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:37:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 169766 tokens (169766 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:38:24,3260.2334022521973,5437.95371055603,45.460383329532526,99,29,
2025-01-15 06:39:29,6850.171089172363,12613.123178482056,45.289288537407145,261,8537,
2025-01-15 06:40:42,5210.101127624512,8091.949939727783,31.576951440969278,91,12242,
2025-01-15 06:41:50,4478.687286376953,6490.663290023804,40.25892945700232,81,151,
2025-01-15 06:42:56,3198.9176273345947,4588.232040405273,35.988973791389256,50,187,
2025-01-15 06:44:01,3475.0988483428955,5006.945848464966,41.77962942441376,64,68,
2025-01-15 06:45:06,10988.317251205444,13171.285629272461,29.317877731545966,64,29025,
2025-01-15 06:46:19,3237.457752227783,4051.2850284576416,34.40533491297193,28,13,
2025-01-15 06:47:23,9657.288074493408,11339.394330978394,29.724638266599484,50,25406,
2025-01-15 06:48:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78638 tokens (78638 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:49:36,3156.5794944763184,4723.194122314453,41.49073986989218,65,335,
2025-01-15 06:50:40,4700.6707191467285,6259.573221206665,33.99827758950012,53,466,
2025-01-15 06:51:47,12669.090509414673,15251.561164855957,33.3014432589185,86,27825,
2025-01-15 06:53:02,4171.876668930054,5969.069719314575,34.4982415699497,62,799,
2025-01-15 06:54:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118727 tokens (118727 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:55:09,4908.429384231567,7688.823699951172,33.088831853761405,92,4314,
2025-01-15 06:56:17,3239.3314838409424,4812.764406204224,22.87990767723885,36,5,
2025-01-15 06:57:22,3815.617084503174,6216.750383377075,41.64700062545408,100,16,
2025-01-15 06:58:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120958 tokens (120958 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 06:59:29,3400.0070095062256,5014.438152313232,35.306553800055084,57,1983,
2025-01-15 07:00:34,5281.947135925293,6993.1793212890625,33.30933142067047,57,2230,
2025-01-15 07:01:41,4215.4388427734375,5617.142200469971,36.38430322647584,51,93,
2025-01-15 07:02:47,4470.286846160889,5888.750076293945,36.6593922883163,52,185,
2025-01-15 07:03:52,3800.0476360321045,7230.223655700684,36.441278605894226,125,6,
2025-01-15 07:05:00,3712.5470638275146,5211.325883865356,34.02770263240865,51,11,
2025-01-15 07:06:05,3783.261299133301,5182.9588413238525,34.293123016333325,48,98,
2025-01-15 07:07:10,5166.297197341919,7589.7932052612305,31.77228258201598,77,5,
2025-01-15 07:08:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 314594 tokens (314594 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:09:20,4332.545280456543,5849.964141845703,33.609704807089805,51,61,
2025-01-15 07:10:26,6704.014778137207,9445.361137390137,32.830583299414506,90,9909,
2025-01-15 07:11:35,4145.922899246216,5858.270168304443,33.28764032271876,57,435,
2025-01-15 07:12:41,4245.331525802612,5888.57889175415,31.036106344505207,51,109,
2025-01-15 07:13:47,3971.8732833862305,5803.950071334839,37.116348205109986,68,17,
2025-01-15 07:14:53,3786.0093116760254,5279.343366622925,34.1517692113527,51,56,
2025-01-15 07:15:58,3200.1559734344482,4250.962734222412,33.30774154303566,35,4,
2025-01-15 07:17:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 237883 tokens (237883 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:18:03,5036.902666091919,6848.849534988403,28.146520670917976,51,3812,
2025-01-15 07:19:10,4228.746652603149,5726.346731185913,37.393160431051086,56,10,
2025-01-15 07:20:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 212511 tokens (212511 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:21:18,3876.2621879577637,5631.108283996582,27.922676587198612,49,1039,
2025-01-15 07:22:23,8524.939775466919,14029.72388267517,36.15037322524947,199,5843,
2025-01-15 07:23:37,12827.443361282349,14863.937616348267,25.04303651882839,51,22325,
2025-01-15 07:24:52,3548.31862449646,4709.2695236206055,35.31587772655293,41,60,
2025-01-15 07:25:57,12375.64206123352,22955.546617507935,39.69790065364238,420,6,
2025-01-15 07:27:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 438842 tokens (438842 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:28:22,3174.6137142181396,4114.41707611084,36.17778077695562,34,4,
2025-01-15 07:29:26,6228.107213973999,10948.072910308838,43.64438499202731,206,8412,
2025-01-15 07:30:37,3419.523239135742,4370.153903961182,32.60993059348912,31,6,
2025-01-15 07:31:42,3869.2729473114014,5156.207084655762,37.29794603090559,48,6,
2025-01-15 07:32:47,4272.8941440582275,5229.600429534912,33.4480921530225,32,5,
2025-01-15 07:33:52,4608.657598495483,6208.107471466064,38.76332797154211,62,15,
2025-01-15 07:34:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 352995 tokens (352995 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:36:00,7829.428672790527,10344.193458557129,28.23325680471398,71,16716,
2025-01-15 07:37:10,5345.8263874053955,6980.456590652466,33.0349946383794,54,142,
2025-01-15 07:38:17,4690.946340560913,6316.976308822632,31.979730395491888,52,130,
2025-01-15 07:39:24,3172.471046447754,5317.276954650879,16.31849290704458,35,11,
2025-01-15 07:40:29,4119.396924972534,5777.517557144165,30.757713890337158,51,32,
2025-01-15 07:41:35,3684.113025665283,6358.742237091064,33.649524059457626,90,300,
2025-01-15 07:42:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 239881 tokens (239881 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:43:43,6340.490341186523,7764.861583709717,25.27430976226958,36,11414,
2025-01-15 07:44:51,4339.358329772949,11497.303247451782,40.793831659532984,292,587,
2025-01-15 07:46:02,4118.610858917236,5851.014852523804,32.902256177172504,57,6,
2025-01-15 07:47:08,3117.9940700531006,4058.1350326538086,29.782767812332864,28,5,
2025-01-15 07:48:12,4283.607959747314,5825.148105621338,33.73249807290423,52,169,
2025-01-15 07:49:18,4981.328964233398,7327.028751373291,31.120777006595873,73,1630,
2025-01-15 07:50:25,3874.1278648376465,5258.559465408325,34.67126868544018,48,325,
2025-01-15 07:51:30,3556.4897060394287,5220.839023590088,35.4492890271541,59,1910,
2025-01-15 07:52:36,4628.995418548584,6143.0113315582275,37.648217241450446,57,16,
2025-01-15 07:53:42,3441.9469833374023,4553.9281368255615,36.87112850014376,41,22,
2025-01-15 07:54:46,13302.94919013977,17037.17589378357,26.77930611508444,100,31032,
2025-01-15 07:56:03,4414.05987739563,5958.266258239746,38.20732819906402,59,143,
2025-01-15 07:57:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 368105 tokens (368105 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 07:58:13,4037.7917289733887,6730.921030044556,36.760210495880635,99,2371,
2025-01-15 07:59:19,9419.728517532349,15975.631713867188,35.387953887071205,232,2262,
2025-01-15 08:00:35,3709.1314792633057,4821.238040924072,34.169387458026165,38,19,
2025-01-15 08:01:40,6453.806400299072,9555.128812789917,35.14629745072394,109,4019,
2025-01-15 08:02:50,5796.318292617798,11946.720600128174,35.7700177972673,220,5572,
2025-01-15 08:04:02,3164.4012928009033,4644.639730453491,32.42720819770092,48,83,
2025-01-15 08:05:06,3709.3591690063477,9211.88735961914,38.70947910151081,213,300,
2025-01-15 08:06:16,19812.103271484375,23507.020950317383,28.41741254521343,105,40135,
2025-01-15 08:07:39,3584.2156410217285,5317.928791046143,27.686240944370795,48,200,
2025-01-15 08:08:45,5128.995895385742,6944.077730178833,38.56575425866659,70,22,
2025-01-15 08:09:51,5157.978296279907,6781.215667724609,31.418695070215975,51,11,
2025-01-15 08:10:58,3439.777374267578,4579.293251037598,34.22506065518479,39,20,
2025-01-15 08:12:03,11307.191848754883,13738.715171813965,21.385770601854304,52,26355,
2025-01-15 08:13:17,3864.8362159729004,5462.110996246338,31.92938411715817,51,849,
2025-01-15 08:14:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70611 tokens (70611 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 08:15:23,32354.151964187622,37232.935667037964,26.031078181597298,127,54384,
2025-01-15 08:17:00,3487.9307746887207,4754.156589508057,30.80019341223468,39,25,
2025-01-15 08:18:05,5657.295227050781,7345.376014709473,34.35854517392142,58,8,
2025-01-15 08:19:12,4078.413963317871,5069.059133529663,32.302181408867774,32,33,
2025-01-15 08:20:17,4179.455757141113,5953.682661056519,28.744914129896245,51,182,
2025-01-15 08:21:23,10484.60578918457,14590.44623374939,34.09776923633931,140,13769,
2025-01-15 08:22:38,65488.70015144348,69335.85453033447,22.35418481563274,86,47795,
2025-01-15 08:24:47,4452.07953453064,6685.995817184448,26.411016589175947,59,358,
2025-01-15 08:25:54,6108.364105224609,8888.033866882324,34.17672174961703,95,571,
2025-01-15 08:27:03,80884.21630859375,82938.26389312744,26.776400125358375,55,16132,
2025-01-15 08:29:26,3793.257236480713,4818.17102432251,31.222138271145436,32,4,
2025-01-15 08:30:31,3668.6410903930664,5193.000078201294,32.14465909401943,49,110,
2025-01-15 08:31:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 654014 tokens (654014 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 08:32:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 223207 tokens (223207 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 08:33:40,4512.597322463989,6035.3288650512695,35.46258712697863,54,5451,
2025-01-15 08:34:46,4655.988931655884,6377.083539962769,28.470253618540713,49,25,
2025-01-15 08:35:53,9824.107885360718,18842.394828796387,41.47129076129378,374,5,
2025-01-15 08:37:11,4331.212997436523,6736.732244491577,37.413959630621164,90,360,
2025-01-15 08:38:18,4514.115571975708,7159.105062484741,31.758160212513374,84,371,
2025-01-15 08:39:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73685 tokens (73685 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 08:40:26,23032.989978790283,26515.584230422974,25.842803811499632,90,42140,
2025-01-15 08:41:53,4832.3352336883545,7702.847957611084,31.701653590179173,91,253,
2025-01-15 08:43:01,3947.270631790161,5727.876901626587,31.449962267708067,56,73,
2025-01-15 08:44:06,5322.565078735352,6394.713401794434,27.981203117870113,30,6,
2025-01-15 08:45:13,3202.3050785064697,4707.512378692627,34.54673651500951,52,98,
2025-01-15 08:46:17,3780.653953552246,5002.036809921265,32.749763754596806,40,10,
2025-01-15 08:47:22,3413.1202697753906,4340.347528457642,33.433011928549554,31,5,
2025-01-15 08:48:27,4100.475788116455,5958.395004272461,31.217718992110285,58,79,
2025-01-15 08:49:33,3690.898656845093,4688.864469528198,33.06726501108995,33,11,
2025-01-15 08:50:37,3784.4765186309814,4724.45273399353,34.04341458539736,32,7,
2025-01-15 08:51:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71341 tokens (71341 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 08:52:43,3353.165864944458,5758.0530643463135,30.354854073054483,73,7,
2025-01-15 08:53:49,98815.90557098389,102989.31288719177,33.06650646440903,138,25536,
2025-01-15 08:56:32,3278.468132019043,4748.56162071228,30.610298151854543,45,23,
2025-01-15 08:57:36,3634.2315673828125,5097.2137451171875,38.27797826404389,56,11,
2025-01-15 08:58:42,525129.0924549103,528231.4593791962,33.84512617706211,105,40740,
2025-01-15 09:08:30,3387.0158195495605,4273.121356964111,34.98454607388051,31,4,
2025-01-15 09:09:34,2513.5397911071777,2992.7022457122803,20.86974867060778,10,4,
2025-01-15 09:10:37,3097.315549850464,4107.527017593384,30.686644321373834,31,4,
2025-01-15 09:11:41,4417.470455169678,7225.802183151245,37.74482869806317,106,228,
2025-01-15 09:12:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 65583 tokens (65583 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:13:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 122236 tokens (122236 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:14:50,4988.717079162598,6982.886552810669,32.093561177085114,64,83,
2025-01-15 09:15:57,4342.928171157837,5850.85654258728,31.8317507047754,48,212,
2025-01-15 09:17:03,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 132708 tokens (132708 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:18:05,4797.656059265137,6066.951513290405,32.301384102478465,41,25,
2025-01-15 09:19:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 105198 tokens (105198 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:20:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 485298 tokens (485298 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:21:14,16446.367025375366,19588.91463279724,29.27561058509347,92,12558,
2025-01-15 09:22:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151386 tokens (151386 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:23:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 732741 tokens (732741 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:24:38,4099.30682182312,5540.095806121826,40.25572143600966,58,10,
2025-01-15 09:25:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 134139 tokens (134139 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:26:45,2816.7243003845215,3185.7151985168457,27.100939482832146,10,4,
2025-01-15 09:27:48,3314.716577529907,4703.787088394165,44.63416328766822,62,172,
2025-01-15 09:28:53,3108.5920333862305,4234.705209732056,43.512500367860255,49,4,
2025-01-15 09:29:57,12913.116216659546,13877.951383590698,48.71298394884665,47,24,
2025-01-15 09:31:11,4518.063545227051,5625.407934188843,53.28062397581331,59,12,
2025-01-15 09:32:16,212242.4328327179,216302.5562763214,38.91507295151818,158,42135,
2025-01-15 09:36:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 89982 tokens (89982 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:37:54,379950.48236846924,381650.13432502747,30.006143200796426,51,15616,
2025-01-15 09:45:15,3343.9431190490723,4636.422634124756,36.36421270262672,47,42,
2025-01-15 09:46:20,3694.575309753418,5347.204208374023,43.56694964011341,72,20,
2025-01-15 09:47:25,3506.432056427002,4600.760221481323,43.86252820022898,48,35,
2025-01-15 09:48:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70724 tokens (70724 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:49:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86610 tokens (86610 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:50:34,5209.044933319092,6927.450180053711,35.498029417632765,61,230,
2025-01-15 09:51:41,5426.559925079346,9733.811378479004,47.361989938847294,204,5846,
2025-01-15 09:52:50,3680.906057357788,5275.982618331909,33.22724519732936,53,142,
2025-01-15 09:53:56,3758.7287425994873,5146.300792694092,34.59279825990107,48,50,
2025-01-15 09:55:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86600 tokens (86600 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 09:56:02,3240.2429580688477,5182.588815689087,27.801433914638025,54,677,
2025-01-15 09:57:07,49070.401191711426,52320.26767730713,22.46246124988702,73,65230,
2025-01-15 09:58:59,2933.22491645813,4057.4581623077393,26.684853975589647,30,5,
2025-01-15 10:00:03,3782.81569480896,5086.09676361084,26.088002667956008,34,13,
2025-01-15 10:01:09,24178.449392318726,26331.3410282135,40.410765943563824,87,10908,
2025-01-15 10:02:35,3841.992139816284,4897.780418395996,43.56934144209388,46,9,
2025-01-15 10:03:40,11182.444095611572,17458.494186401367,46.20740685699429,290,13985,
2025-01-15 10:04:57,3482.5949668884277,4891.552925109863,39.03594119261581,55,12,
2025-01-15 10:06:02,34654.40034866333,36815.309047698975,25.91502363102675,56,57002,
2025-01-15 10:07:39,3241.666793823242,4669.620752334595,38.51664801387414,55,18,
2025-01-15 10:08:44,7799.549102783203,11673.45404624939,45.43219376016066,176,4820,
2025-01-15 10:09:55,3551.6769886016846,4832.334995269775,36.69988377481095,47,5,
2025-01-15 10:11:00,18364.152193069458,20848.751544952393,34.210747070984866,85,29090,
2025-01-15 10:12:21,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125757 tokens (125757 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:13:22,5742.900371551514,8220.206499099731,42.78841392319482,106,6050,
2025-01-15 10:14:30,3229.7465801239014,4604.853630065918,42.17853439297374,58,1797,
2025-01-15 10:15:35,3585.455894470215,4943.045139312744,37.56659106850514,51,1527,
2025-01-15 10:16:40,6685.816526412964,9009.22679901123,43.040181572482304,100,20,
2025-01-15 10:17:49,5257.4403285980225,7439.19825553894,39.41775525966904,86,555,
2025-01-15 10:18:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 396213 tokens (396213 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:19:59,3496.6721534729004,10379.075527191162,7.264904029155615,50,12,
2025-01-15 10:21:09,3193.7623023986816,4587.7649784088135,43.0415242614383,60,8,
2025-01-15 10:22:14,3234.853506088257,3976.33695602417,37.762137512873764,28,4,
2025-01-15 10:23:18,2687.716484069824,3444.2837238311768,39.65278751623324,30,5,
2025-01-15 10:24:21,21250.553131103516,24076.470613479614,31.848063703659832,90,44542,
2025-01-15 10:25:45,4121.471643447876,6367.908477783203,38.72799745368378,87,6716,
2025-01-15 10:26:51,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 493496 tokens (493496 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:27:54,18013.681173324585,19955.989837646484,26.25741260223702,51,40545,
2025-01-15 10:29:14,3314.1353130340576,4626.3628005981445,42.67552732335601,56,77,
2025-01-15 10:30:19,4203.406572341919,4978.668451309204,39.98648823194382,31,4,
2025-01-15 10:31:24,3890.242576599121,6593.540668487549,34.03250284386213,92,1205,
2025-01-15 10:32:30,22053.574323654175,24468.665599822998,31.468789916943635,76,44557,
2025-01-15 10:33:55,4132.493495941162,5657.547950744629,36.064285984521014,55,5,
2025-01-15 10:35:00,26515.904426574707,29262.47477531433,29.127235002996105,80,50042,
2025-01-15 10:36:30,3897.075653076172,5159.082412719727,42.788994264382495,54,1926,
2025-01-15 10:37:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 129016 tokens (129016 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:38:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 339314 tokens (339314 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:39:38,6522.3119258880615,10164.309978485107,44.4810781500777,162,415,
2025-01-15 10:40:48,3417.786121368408,4302.457332611084,36.17163031116429,32,4,
2025-01-15 10:41:52,11880.838871002197,14246.971130371094,35.078343432136826,83,28413,
2025-01-15 10:43:07,20605.781316757202,22523.666381835938,28.677422334347273,55,41996,
2025-01-15 10:44:29,4099.636554718018,6109.715938568115,43.281872695674615,87,4023,
2025-01-15 10:45:35,6177.295446395874,7454.323053359985,39.93648980012479,51,5387,
2025-01-15 10:46:43,3465.5239582061768,4802.364349365234,39.645720125233744,53,87,
2025-01-15 10:47:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 157399 tokens (157399 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:48:49,3517.6188945770264,4673.280954360962,41.534633410890166,48,137,
2025-01-15 10:49:53,3684.828996658325,3919.6252822875977,42.590111565007156,10,4,
2025-01-15 10:50:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 723198 tokens (723198 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 10:52:00,23339.73479270935,28320.473194122314,38.950048038050944,194,34035,
2025-01-15 10:53:29,4218.501329421997,5323.972702026367,43.420391689490096,48,196,
2025-01-15 10:54:34,9034.566164016724,12087.18228340149,39.63813177543816,121,10983,
2025-01-15 10:55:46,4650.398969650269,7036.046266555786,36.468089860915256,87,8047,
2025-01-15 10:56:53,9252.56371498108,14633.9852809906,44.41205675273371,239,10917,
2025-01-15 10:58:08,4271.733522415161,5606.946229934692,41.94088304030078,56,13,
2025-01-15 10:59:14,4105.239152908325,5447.2150802612305,38.003662331409544,51,117,
2025-01-15 11:00:19,4716.595888137817,8031.038999557495,33.791498672616214,112,4043,
2025-01-15 11:01:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 360833 tokens (360833 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:02:29,3783.292770385742,5398.293018341064,37.77089203375003,61,315,
2025-01-15 11:03:35,5161.436080932617,6642.310619354248,33.76383258860794,50,4636,
2025-01-15 11:04:42,3305.9096336364746,4574.648857116699,40.197385763879886,51,54,
2025-01-15 11:05:46,11161.735534667969,14279.657363891602,31.431192110548245,98,24208,
2025-01-15 11:07:00,9151.794910430908,11302.8564453125,34.86650604076209,75,21221,
2025-01-15 11:08:12,3943.800210952759,5237.8997802734375,37.86416529426861,49,83,
2025-01-15 11:09:17,3633.726119995117,4856.619596481323,40.068902927500986,49,124,
2025-01-15 11:10:22,3389.2440795898438,4601.171493530273,42.90686009892996,52,7,
2025-01-15 11:11:26,3697.6327896118164,4637.761831283569,40.41998312532477,38,26,
2025-01-15 11:12:31,3289.140224456787,4593.534708023071,41.39851914457743,54,2159,
2025-01-15 11:13:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 145966 tokens (145966 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:14:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 91711 tokens (91711 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:15:38,3556.9567680358887,5055.617094039917,39.36849396508373,59,261,
2025-01-15 11:16:43,3367.929220199585,4228.322982788086,40.679048967884476,35,4,
2025-01-15 11:17:47,5809.735536575317,8153.644323348999,45.65024057411475,107,124,
2025-01-15 11:18:56,4309.304237365723,6190.982341766357,46.76676621479335,88,809,
2025-01-15 11:20:02,9340.656042098999,11712.358951568604,40.47724511223412,96,20502,
2025-01-15 11:21:13,3030.639171600342,4023.256778717041,39.2900546196083,39,30,
2025-01-15 11:22:18,4476.149082183838,5874.8345375061035,36.46280856495308,51,1764,
2025-01-15 11:23:23,12528.498888015747,15304.993867874146,39.618296016371694,110,27863,
2025-01-15 11:24:39,2685.8086585998535,3748.804807662964,29.16285259106759,31,5,
2025-01-15 11:25:42,3270.235061645508,4432.661056518555,41.292951303314815,48,40,
2025-01-15 11:26:47,4514.840602874756,6933.72368812561,38.447496932393186,93,6741,
2025-01-15 11:27:54,2918.3855056762695,3823.6637115478516,39.76678082660787,36,4,
2025-01-15 11:28:58,7027.271032333374,8269.919157028198,40.23665187784294,50,360,
2025-01-15 11:30:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 294107 tokens (294107 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:31:08,3630.861282348633,6884.475231170654,51.02018942969577,166,671,
2025-01-15 11:32:15,5143.527269363403,7146.1920738220215,47.436795108446226,95,2557,
2025-01-15 11:33:22,9914.100885391235,12059.825897216797,39.613650179564644,85,22266,
2025-01-15 11:34:34,3258.577585220337,4472.494602203369,49.42677230863686,60,15,
2025-01-15 11:35:38,5733.688116073608,9188.947439193726,48.332117616340916,167,15925,
2025-01-15 11:36:47,3104.0239334106445,4100.966215133667,46.14108644333739,46,17,
2025-01-15 11:37:52,4110.759973526001,5158.175706863403,47.73653708703035,50,21,
2025-01-15 11:38:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 333588 tokens (333588 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:39:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 134893 tokens (134893 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:41:00,4168.1318283081055,5793.33233833313,51.07062143287291,83,30,
2025-01-15 11:42:06,6320.55139541626,7751.703500747681,45.417953659752705,65,4333,
2025-01-15 11:43:14,2701.153039932251,3477.8566360473633,43.77474260459197,34,6,
2025-01-15 11:44:17,3959.177255630493,5381.44588470459,47.1078378798366,67,25,
2025-01-15 11:45:23,4945.409536361694,6398.418664932251,42.67006915572131,62,9884,
2025-01-15 11:46:29,4929.481267929077,6081.753730773926,45.12821548439773,52,3133,
2025-01-15 11:47:35,3034.8846912384033,4094.0675735473633,46.26207694480741,49,9,
2025-01-15 11:48:39,3534.498453140259,4447.4778175354,42.71728531984719,39,30,
2025-01-15 11:49:44,3181.5614700317383,4405.179977416992,41.67965725606891,51,50,
2025-01-15 11:50:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 246376 tokens (246376 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:51:50,3315.741539001465,5197.121143341064,49.43181045732915,93,137,
2025-01-15 11:52:55,6546.141862869263,11010.624885559082,21.055069427359065,94,13174,
2025-01-15 11:54:06,2482.084274291992,3224.374771118164,40.41544399163356,30,4,
2025-01-15 11:55:10,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 345868 tokens (345868 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 11:56:12,5642.850637435913,6909.06834602356,44.22620187681866,56,1263,
2025-01-15 11:57:19,4710.598707199097,6145.831108093262,35.53431483864666,51,8944,
2025-01-15 11:58:25,3000.9381771087646,4307.290315628052,40.570990345737854,53,42,
2025-01-15 11:59:29,3362.315893173218,4548.464298248291,47.21163031573243,56,309,
2025-01-15 12:00:34,2410.022258758545,3216.261148452759,17.364580373082532,14,4,
2025-01-15 12:01:37,3484.487771987915,4549.98779296875,42.23369227020354,45,8,
2025-01-15 12:02:41,3463.9337062835693,4446.083784103394,42.76332196931813,42,60,
2025-01-15 12:03:46,9832.802534103394,15038.734197616577,42.259486720103176,220,14920,
2025-01-15 12:05:01,4707.32855796814,7504.273176193237,49.33955399073039,138,4,
2025-01-15 12:06:08,3018.327236175537,4282.910346984863,45.074143022138195,57,13,
2025-01-15 12:07:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 362467 tokens (362467 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:08:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 180114 tokens (180114 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:09:17,7484.687089920044,9678.183794021606,41.03037849644877,90,15828,
2025-01-15 12:10:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 149259 tokens (149259 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:11:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 92832 tokens (92832 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:12:29,16436.80500984192,18382.39574432373,32.89489349724199,64,37536,
2025-01-15 12:13:48,5274.885892868042,6624.359846115112,42.23868112670721,57,881,
2025-01-15 12:14:54,15533.455610275269,17271.846532821655,29.33747486744549,51,35288,
2025-01-15 12:16:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109502 tokens (109502 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:17:13,3616.344451904297,7212.048053741455,47.834866008455144,172,313,
2025-01-15 12:18:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 194561 tokens (194561 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:19:21,5757.730960845947,8184.200763702393,41.21213455130568,100,6172,
2025-01-15 12:20:29,3318.9010620117188,4491.889238357544,44.331222640277616,52,59,
2025-01-15 12:21:34,3273.125171661377,4765.633344650269,31.490614825832388,47,68,
2025-01-15 12:22:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 233738 tokens (233738 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:23:40,3398.7934589385986,4329.00857925415,40.85076577459789,38,4,
2025-01-15 12:24:45,6374.570369720459,7745.307207107544,37.206266446604594,51,12234,
2025-01-15 12:25:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 610844 tokens (610844 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:26:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 162186 tokens (162186 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:27:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 335802 tokens (335802 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:28:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 390077 tokens (390077 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:30:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128465 tokens (128465 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:31:02,3238.0857467651367,4198.784828186035,33.309077336340515,32,4,
2025-01-15 12:32:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 112238 tokens (112238 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:33:08,3518.67413520813,5769.317865371704,46.20900172078386,104,735,
2025-01-15 12:34:13,3731.982946395874,4772.263765335083,46.14138713904806,48,40,
2025-01-15 12:35:18,4018.173933029175,5358.198165893555,44.02905451484565,59,3254,
2025-01-15 12:36:24,3853.9793491363525,6149.248361587524,42.69652030693496,98,351,
2025-01-15 12:37:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95389 tokens (95389 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:38:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69046 tokens (69046 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:39:32,5496.260404586792,9898.545503616333,50.65551071400603,223,2163,
2025-01-15 12:40:42,3877.753496170044,5111.576080322266,47.008379279953516,58,15,
2025-01-15 12:41:47,5025.606155395508,9974.968433380127,47.68291079635804,236,8958,
2025-01-15 12:42:57,13171.525955200195,15221.405267715454,29.75784946341612,61,32181,
2025-01-15 12:44:12,2995.5694675445557,4156.4576625823975,41.347651053024386,48,6,
2025-01-15 12:45:17,4349.048614501953,6380.565643310547,43.31738240540794,88,5555,
2025-01-15 12:46:23,4094.6555137634277,5287.423849105835,38.56574544862881,46,5,
2025-01-15 12:47:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 192062 tokens (192062 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:48:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 359856 tokens (359856 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:49:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 152419 tokens (152419 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:50:33,3925.748586654663,5206.763744354248,39.81217528415864,51,1335,
2025-01-15 12:51:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 312193 tokens (312193 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:52:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 394705 tokens (394705 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 12:53:43,3805.263042449951,5177.2966384887695,39.357636836228174,54,14,
2025-01-15 12:54:49,4829.536199569702,8393.432140350342,43.77232180517302,156,2472,
2025-01-15 12:55:57,3401.0543823242188,4764.02735710144,49.89093786772597,68,15,
2025-01-15 12:57:02,3469.874382019043,4239.354372024536,37.68778964582688,29,14,
2025-01-15 12:58:06,5122.133016586304,9645.833015441895,48.19064041716953,218,466,
2025-01-15 12:59:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125528 tokens (125528 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:00:17,4243.276357650757,5977.323532104492,34.60113478106579,60,544,
2025-01-15 13:01:23,2701.249837875366,3736.679792404175,44.42598922196832,46,23,
2025-01-15 13:02:27,23079.60057258606,51832.76104927063,49.14242387878636,1413,8,
2025-01-15 13:04:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82597 tokens (82597 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:05:20,3391.049861907959,8586.390256881714,46.38772085716579,241,1713,
2025-01-15 13:06:28,2858.579397201538,3777.117967605591,40.28137869455411,37,7,
2025-01-15 13:07:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144837 tokens (144837 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:08:33,3478.3012866973877,4677.751541137695,40.01833325084245,48,51,
2025-01-15 13:09:38,13269.484043121338,18738.781690597534,46.25822478627519,253,5,
2025-01-15 13:10:57,3439.5511150360107,4595.206260681152,38.93895178814685,45,4,
2025-01-15 13:12:01,3370.922803878784,4469.089508056641,26.40764821012387,29,23,
2025-01-15 13:13:06,8461.83729171753,12591.978073120117,39.46596705225276,163,10990,
2025-01-15 13:14:18,3551.1293411254883,4644.400596618652,40.24618755768167,44,22,
2025-01-15 13:15:23,4970.268726348877,6144.835948944092,41.71749309650762,49,522,
2025-01-15 13:16:29,25208.829641342163,28375.738382339478,35.04995220198362,111,48152,
2025-01-15 13:17:58,3928.91788482666,6149.063348770142,45.492514630370714,101,328,
2025-01-15 13:19:04,20543.97749900818,22931.32734298706,32.672211907577456,78,39112,
2025-01-15 13:20:27,4204.526662826538,5434.208869934082,44.72700319001191,55,88,
2025-01-15 13:21:32,4365.35382270813,5766.2718296051025,35.69088251692173,50,436,
2025-01-15 13:22:38,3544.647216796875,4668.247699737549,41.82981470156737,47,80,
2025-01-15 13:23:43,3275.348901748657,4511.65771484375,47.72270437213321,59,8,
2025-01-15 13:24:47,3476.086139678955,4901.4732837677,41.39226331925345,59,13,
2025-01-15 13:25:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 90190 tokens (90190 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:26:53,32261.479139328003,37103.1174659729,42.96066454516387,208,41276,
2025-01-15 13:28:30,4641.983985900879,6017.366409301758,36.353525499014346,50,533,
2025-01-15 13:29:36,39777.395725250244,42682.445764541626,30.29207717931961,88,57646,
2025-01-15 13:31:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 520704 tokens (520704 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:32:22,4882.801055908203,8703.326940536499,46.85218878380015,179,216,
2025-01-15 13:33:31,3733.440399169922,4936.064720153809,39.912713523646694,48,387,
2025-01-15 13:34:36,3133.6569786071777,4260.89882850647,40.80756938194732,46,28,
2025-01-15 13:35:40,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 158260 tokens (158260 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:36:41,4348.031997680664,5835.555791854858,42.35226370612427,63,727,
2025-01-15 13:37:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199848 tokens (199848 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:38:49,3656.418800354004,4848.275661468506,43.629400221243806,52,8,
2025-01-15 13:39:54,3561.4750385284424,4619.884252548218,41.571822521168926,44,41,
2025-01-15 13:40:58,19106.64987564087,20842.901945114136,26.49384891097938,46,40218,
2025-01-15 13:42:19,3534.048557281494,5056.473731994629,31.528643113146412,48,50,
2025-01-15 13:43:24,4454.658985137939,6218.699932098389,30.044654060509338,53,94,
2025-01-15 13:44:30,12877.347230911255,17444.31781768799,42.697888303595725,195,15201,
2025-01-15 13:45:48,3312.5643730163574,4648.344039916992,44.16896099107104,59,9,
2025-01-15 13:46:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 107185 tokens (107185 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:47:54,6140.216588973999,8429.218053817749,39.31845452363811,90,11306,
2025-01-15 13:49:02,4741.513013839722,6862.7989292144775,43.841331960934745,93,162,
2025-01-15 13:50:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 270063 tokens (270063 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 13:51:11,3316.1871433258057,4611.364126205444,42.46523890326977,55,5,
2025-01-15 13:52:15,6137.359619140625,7551.653146743774,37.47454044410489,53,4045,
2025-01-15 13:53:23,3300.2588748931885,4513.567686080933,40.385431596785686,49,1839,
2025-01-15 13:54:27,3397.260904312134,5412.888526916504,44.65110469349095,90,38,
2025-01-15 13:55:33,6379.448175430298,9841.358184814453,48.23926663238363,167,373,
2025-01-15 13:56:43,3225.8548736572266,4598.080635070801,34.979666866590264,48,87,
2025-01-15 13:57:47,3355.1692962646484,4411.397695541382,29.349712639072823,31,25,
2025-01-15 13:58:52,3802.6680946350098,5449.171781539917,44.3363720230868,73,118,
2025-01-15 13:59:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 671659 tokens (671659 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:01:00,5724.247932434082,7100.179195404053,39.97292704962716,55,2763,
2025-01-15 14:02:07,3017.1921253204346,4245.281457901001,46.41356169117333,57,10,
2025-01-15 14:03:11,3440.481424331665,4395.413398742676,39.793410439982274,38,4,
2025-01-15 14:04:16,3187.0970726013184,4263.689279556274,38.083129094873165,41,33,
2025-01-15 14:05:20,7669.61407661438,11018.361568450928,58.23072670464517,195,311,
2025-01-15 14:06:31,4028.416872024536,6161.051511764526,44.076935752792814,94,4531,
2025-01-15 14:07:37,3587.7716541290283,4676.6417026519775,40.408862434673395,44,29,
2025-01-15 14:08:42,21397.619247436523,26286.909103393555,43.155551463761356,211,43868,
2025-01-15 14:10:08,5507.064580917358,6844.982624053955,33.63434720896851,45,14,
2025-01-15 14:11:15,3670.9632873535156,4711.9505405426025,46.11007469395131,48,41,
2025-01-15 14:12:20,3517.5843238830566,4866.456031799316,39.292098491616024,53,142,
2025-01-15 14:13:25,17061.32197380066,21524.43504333496,41.450888005242405,185,34753,
2025-01-15 14:14:46,4835.034608840942,6566.004037857056,30.618680556435542,53,2467,
2025-01-15 14:15:53,5341.629266738892,6613.316297531128,43.249635066055575,55,1509,
2025-01-15 14:16:59,3155.8587551116943,4699.190378189087,37.58103516621295,58,17,
2025-01-15 14:18:04,3140.5413150787354,4458.707809448242,36.414216417296295,48,61,
2025-01-15 14:19:09,29437.503814697266,31577.900171279907,26.630581679276563,57,51249,
2025-01-15 14:20:40,3083.446502685547,4534.532070159912,26.187290985285955,38,8,
2025-01-15 14:21:45,3884.063243865967,4858.3478927612305,42.08215745417901,41,6,
2025-01-15 14:22:50,13855.59630393982,16322.509288787842,40.94185754436825,101,6937,
2025-01-15 14:24:06,3580.4429054260254,4588.455677032471,50.59459704932364,51,97,
2025-01-15 14:25:11,7744.4822788238525,10988.649129867554,59.49139143010131,193,6038,
2025-01-15 14:26:22,360835.44874191284,362977.9405593872,41.54041535846607,89,12071,
2025-01-15 14:33:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 757213 tokens (757213 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:34:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 281426 tokens (281426 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:35:30,14641.342163085938,17306.81800842285,33.765078065685515,90,19223,
2025-01-15 14:36:47,15186.498165130615,20352.75888442993,33.486502017548865,173,13081,
2025-01-15 14:38:08,4065.7505989074707,9611.160039901733,39.31179515590714,218,931,
2025-01-15 14:39:17,3364.874839782715,4520.0793743133545,37.22284557813905,43,9,
2025-01-15 14:40:22,4672.843456268311,6000.769853591919,35.39352790540721,47,210,
2025-01-15 14:41:28,4636.589527130127,6274.990797042847,31.127905560471675,51,8313,
2025-01-15 14:42:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 706904 tokens (706904 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:43:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 614607 tokens (614607 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:44:40,8528.715133666992,9453.831434249878,33.50930037711789,31,4,
2025-01-15 14:45:50,5899.420261383057,8283.14757347107,40.27306290999759,96,1776,
2025-01-15 14:46:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 286772 tokens (286772 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:48:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 154451 tokens (154451 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:49:03,4354.05158996582,5236.062288284302,54.42110860050803,48,99,
2025-01-15 14:50:09,6225.590705871582,7756.102800369263,38.54918900158316,59,2455,
2025-01-15 14:51:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 410674 tokens (410674 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 14:52:43,12801.111936569214,15677.558422088623,31.63625690869331,91,22232,
2025-01-15 14:53:59,6507.120609283447,8269.56033706665,33.47632209483283,59,1456,
2025-01-15 14:55:07,3675.4891872406006,5081.8493366241455,31.286438270656834,44,12,
2025-01-15 14:56:12,391060.1348876953,396815.90127944946,36.48514997079295,210,48263,
2025-01-15 15:03:49,5010.080575942993,6706.44211769104,33.601327663478685,57,1701,
2025-01-15 15:04:56,3546.018362045288,4854.392051696777,36.68676646408698,48,146,
2025-01-15 15:06:01,5952.29172706604,8584.664821624756,34.949453096208096,92,8130,
2025-01-15 15:07:09,7283.64634513855,9546.23818397522,39.77739089091484,90,2705,
2025-01-15 15:08:19,4524.269342422485,6265.976667404175,35.02310584853359,61,316,
2025-01-15 15:09:25,4375.220537185669,6763.128280639648,38.1086749475394,91,179,
2025-01-15 15:10:32,4458.707332611084,6337.810277938843,27.140609899424337,51,6943,
2025-01-15 15:11:38,5800.5311489105225,8881.030797958374,34.08537963393515,105,8984,
2025-01-15 15:12:47,3493.319511413574,4636.129379272461,34.12641166029526,39,18,
2025-01-15 15:13:52,5135.37859916687,6983.5357666015625,35.17016904478005,65,521,
2025-01-15 15:14:59,5530.17520904541,12779.98685836792,40.966581528742815,297,6228,
2025-01-15 15:16:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 307650 tokens (307650 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 15:17:14,3779.5610427856445,5621.952295303345,15.197640545533908,28,8,
2025-01-15 15:18:19,3927.7517795562744,5476.763725280762,35.506504744400786,55,32,
2025-01-15 15:19:25,8039.489507675171,9721.296787261963,29.72992245121251,50,8406,
2025-01-15 15:20:34,3922.1625328063965,4844.644546508789,33.60499125135365,31,4,
2025-01-15 15:21:39,6012.9125118255615,8859.295129776001,37.942896123278835,108,4990,
2025-01-15 15:22:48,10493.64686012268,13719.696521759033,45.56656450398133,147,25,
2025-01-15 15:24:02,5361.706018447876,6754.673719406128,43.073504115511604,60,1240,
2025-01-15 15:25:09,841100.7037162781,843997.241973877,29.690614226960296,86,34686,
2025-01-15 15:40:13,4927.763938903809,6496.608257293701,31.233181919725965,49,1269,
2025-01-15 15:41:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 625800 tokens (625800 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 15:42:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 348409 tokens (348409 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 15:43:25,5294.9042320251465,6551.047325134277,39.00829473075245,49,774,
2025-01-15 15:44:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 131242 tokens (131242 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 15:45:34,4144.611597061157,5274.92880821228,42.46595515529349,48,42,
2025-01-15 15:46:39,229742.53106117249,232155.10511398315,41.449504890222606,100,14438,
2025-01-15 15:51:31,5884.107351303101,7244.930028915405,37.47732958821971,51,2225,
2025-01-15 15:52:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 153738 tokens (153738 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 15:53:40,4467.847585678101,6668.850898742676,29.077648188947684,64,249,
2025-01-15 15:54:47,260615.14329910278,262608.8354587555,25.58067942088106,51,31956,
2025-01-15 16:00:10,4912.395715713501,7753.7713050842285,32.02674097026136,91,30,
2025-01-15 16:01:17,4890.810251235962,6370.92137336731,36.48374719476498,54,2509,
2025-01-15 16:02:24,5140.762805938721,6614.604234695435,34.60345122950031,51,2699,
2025-01-15 16:03:30,4524.708271026611,6899.178743362427,38.74548075954708,92,168,
2025-01-15 16:04:37,5112.208843231201,6505.890607833862,36.59371981130865,51,95,
2025-01-15 16:05:44,5016.902685165405,7057.471990585327,26.463203115214714,54,16,
2025-01-15 16:06:51,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 311357 tokens (311357 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:07:52,5355.443477630615,7661.377906799316,36.86141241693627,85,1992,
2025-01-15 16:09:00,3743.5660362243652,6415.543794631958,35.928442779109325,96,2975,
2025-01-15 16:10:07,3213.5772705078125,4736.592769622803,22.980724766319295,35,16,
2025-01-15 16:11:11,4078.235387802124,5468.986988067627,34.513711859714206,48,28,
2025-01-15 16:12:17,4379.924058914185,5378.859281539917,37.03943875634332,37,8,
2025-01-15 16:13:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 553331 tokens (553331 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:14:25,14111.764192581177,17347.14126586914,33.99912823398112,110,1083,
2025-01-15 16:15:43,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 116740 tokens (116740 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:16:50,3860.33034324646,6592.103958129883,15.374626861894019,42,24,
2025-01-15 16:17:57,7008.209943771362,8785.97617149353,34.312722926545085,61,193,
2025-01-15 16:19:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 137796 tokens (137796 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:20:07,37241.535663604736,40725.66866874695,25.831390439793633,90,57719,
2025-01-15 16:21:48,9478.888750076294,11577.243566513062,44.32043583454776,93,1610,
2025-01-15 16:23:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144051 tokens (144051 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:24:01,22157.140970230103,25718.66011619568,26.954789814550637,96,33185,
2025-01-15 16:25:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 507623 tokens (507623 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:26:30,4891.201019287109,6369.010448455811,35.18721627676384,52,66,
2025-01-15 16:27:36,5731.186389923096,8630.357027053833,32.07813945440638,93,7623,
2025-01-15 16:28:45,4263.734340667725,8167.680501937866,46.87564644602099,183,2923,
2025-01-15 16:29:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 79914 tokens (79914 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:30:54,4723.004579544067,7913.0706787109375,29.1530009438639,93,532,
2025-01-15 16:32:02,6602.21791267395,10586.009502410889,43.676983617380884,174,437,
2025-01-15 16:33:12,9052.860736846924,14589.871406555176,35.398161876817795,196,4,
2025-01-15 16:34:27,9936.300277709961,11419.551134109497,35.73232388259188,53,5337,
2025-01-15 16:35:39,3396.9452381134033,4242.316484451294,35.48736739031357,30,4,
2025-01-15 16:36:43,4351.041078567505,8417.883396148682,39.34256297774601,160,1195,
2025-01-15 16:37:51,10032.0303440094,12250.336170196533,22.9905179880719,51,26245,
2025-01-15 16:39:03,4066.3669109344482,5401.2205600738525,37.4572898176782,50,32,
2025-01-15 16:40:09,3928.7350177764893,5138.266801834106,28.93681709015159,35,13,
2025-01-15 16:41:14,26915.278673171997,30618.94178390503,27.27029888525957,101,49458,
2025-01-15 16:42:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 100316 tokens (100316 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:43:46,4416.435956954956,5922.999620437622,41.15325591795899,62,781,
2025-01-15 16:44:52,3654.127359390259,6377.168416976929,38.192593427941674,104,1105,
2025-01-15 16:45:58,3617.652654647827,5012.948274612427,35.83470003386706,50,9,
2025-01-15 16:47:03,7535.851955413818,9663.928747177124,34.303273398096145,73,9151,
2025-01-15 16:48:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 648853 tokens (648853 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 16:49:16,7603.316307067871,12322.166681289673,39.84021214722316,188,5718,
2025-01-15 16:50:28,5190.7148361206055,6581.418037414551,36.67209506136774,51,1597,
2025-01-15 16:51:34,12865.753650665283,14930.293321609497,28.093429647430217,58,15319,
2025-01-15 16:52:49,3246.9348907470703,4235.816717147827,33.37102484743848,33,4,
2025-01-15 16:53:54,3325.543165206909,5227.62656211853,31.544358200813345,60,15,
2025-01-15 16:54:59,8433.284759521484,14029.8433303833,39.13119057490291,219,6542,
2025-01-15 16:56:13,3082.74507522583,4777.944326400757,28.90515670416829,49,20,
2025-01-15 16:57:18,3562.293767929077,5015.995979309082,28.20385060918257,41,17,
2025-01-15 16:58:23,4860.124588012695,6574.925899505615,40.23789784714361,69,19,
2025-01-15 16:59:29,12197.095155715942,15230.899333953857,35.2692506548486,107,20218,
2025-01-15 17:00:45,15299.065828323364,17135.86115837097,29.943455920358023,55,19651,
2025-01-15 17:02:02,3984.5354557037354,6219.16651725769,38.48510005951313,86,81,
2025-01-15 17:03:08,5714.425563812256,6373.657464981079,42.47367269447335,28,7,
2025-01-15 17:04:14,7383.1634521484375,9244.467496871948,27.400144616123608,51,12232,
2025-01-15 17:05:24,16455.989122390747,18544.183015823364,22.986371213401352,48,37592,
2025-01-15 17:06:42,4414.730072021484,5623.6114501953125,34.74286291302331,42,6,
2025-01-15 17:07:48,3937.183380126953,5077.925443649292,33.31164968412323,38,6,
2025-01-15 17:08:53,4284.188270568848,5688.518285751343,27.77125004690004,39,675,
2025-01-15 17:09:58,3603.2721996307373,4533.6854457855225,33.318528221859374,31,4,
2025-01-15 17:11:03,18066.439151763916,19914.62206840515,29.217887208986873,54,31934,
2025-01-15 17:12:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 241087 tokens (241087 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:13:26,3433.5031509399414,4441.541433334351,29.760774490370075,30,6,
2025-01-15 17:14:30,21892.470598220825,25204.19669151306,28.987904583789103,96,40324,
2025-01-15 17:15:55,13422.410726547241,16180.026054382324,35.17531942214422,97,10664,
2025-01-15 17:17:11,4560.5714321136475,5977.26583480835,34.5875581260128,49,413,
2025-01-15 17:18:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 149011 tokens (149011 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:19:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 101424 tokens (101424 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:20:27,3652.4620056152344,4640.733242034912,31.367906762426088,31,4,
2025-01-15 17:21:31,3736.9370460510254,4637.550592422485,31.089916549457875,28,8,
2025-01-15 17:22:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106759 tokens (106759 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:23:39,8074.797868728638,10599.707126617432,37.625114527654105,95,1046,
2025-01-15 17:24:49,9884.527444839478,15399.357795715332,37.71648206131415,208,6918,
2025-01-15 17:26:05,7069.750070571899,9785.53581237793,36.45353846440176,99,352,
2025-01-15 17:27:15,3995.411157608032,4943.113088607788,34.821074982075245,33,7,
2025-01-15 17:28:20,4602.225065231323,7145.550489425659,35.77992778050674,91,4114,
2025-01-15 17:29:27,6889.701366424561,10187.981605529785,39.71766814924689,131,1236,
2025-01-15 17:30:37,18838.491439819336,25210.036993026733,36.725783100178234,234,37504,
2025-01-15 17:32:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78483 tokens (78483 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:33:04,6917.369604110718,8484.517574310303,31.266990055673926,49,286,
2025-01-15 17:34:13,8028.812408447266,13419.2636013031,38.40123815133412,207,12564,
2025-01-15 17:35:26,4063.406229019165,5709.2413902282715,30.987307357398446,51,43,
2025-01-15 17:36:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 645741 tokens (645741 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:37:35,16472.482681274414,19514.87159729004,29.253327716088393,89,35719,
2025-01-15 17:38:54,4548.293352127075,6685.609817504883,40.705249507644275,87,25,
2025-01-15 17:40:01,4779.573440551758,7450.577974319458,34.81836096654425,93,6427,
2025-01-15 17:41:08,4820.098400115967,11644.873857498169,39.12216624082368,267,457,
2025-01-15 17:42:20,4643.540382385254,6448.837518692017,38.220854956408274,69,41,
2025-01-15 17:43:26,3399.6615409851074,4738.027811050415,35.86462172097162,48,41,
2025-01-15 17:44:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 227880 tokens (227880 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:45:36,3657.733201980591,5093.002557754517,34.8366665802873,50,1632,
2025-01-15 17:46:41,6003.163814544678,7618.012189865112,31.581912444182297,51,6019,
2025-01-15 17:47:49,12329.891443252563,15310.09292602539,30.199300456780716,90,29475,
2025-01-15 17:49:04,37192.18921661377,40111.671924591064,24.66190322116474,72,59782,
2025-01-15 17:50:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88709 tokens (88709 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 17:51:45,4032.5069427490234,8691.369771957397,41.8557075296278,195,774,
2025-01-15 17:52:54,44000.82731246948,47579.503774642944,25.148962458970008,90,65031,
2025-01-15 17:54:41,5736.374378204346,8168.967008590698,40.286235671294996,98,5152,
2025-01-15 17:55:50,3947.4434852600098,5330.21092414856,36.159370400122626,50,6,
2025-01-15 17:56:55,10304.670810699463,13059.138774871826,34.85246561175569,96,1588,
2025-01-15 17:58:08,5018.8281536102295,7087.394237518311,31.906159785482,66,2627,
2025-01-15 17:59:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 601437 tokens (601437 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:00:18,5589.5984172821045,8201.203346252441,35.61028659747039,93,573,
2025-01-15 18:01:26,10362.59412765503,13042.769432067871,26.11769457197324,70,25778,
2025-01-15 18:02:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110314 tokens (110314 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:03:40,4895.11775970459,10410.068273544312,36.08373266461978,199,2134,
2025-01-15 18:04:51,4590.157985687256,7165.015697479248,37.671984574437744,97,304,
2025-01-15 18:05:58,8787.235260009766,11488.685846328735,32.94526298231222,89,18861,
2025-01-15 18:07:10,7202.8443813323975,12146.541833877563,39.64643910381151,196,20,
2025-01-15 18:08:22,3583.6992263793945,4349.692344665527,40.470337474259274,31,4,
2025-01-15 18:09:26,3028.7342071533203,4007.796287536621,28.59879936217942,28,11,
2025-01-15 18:10:30,3615.1416301727295,5132.731199264526,36.2416829425856,55,18,
2025-01-15 18:11:35,6946.515083312988,12523.877382278442,38.010799484789416,212,14462,
2025-01-15 18:12:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 218067 tokens (218067 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:13:49,5730.694055557251,7311.228036880493,37.96185384749987,60,1486,
2025-01-15 18:14:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103226 tokens (103226 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:15:58,3995.6231117248535,6275.8612632751465,28.505794430203558,65,79,
2025-01-15 18:17:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78212 tokens (78212 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:18:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 301101 tokens (301101 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:19:07,3901.548385620117,6461.421966552734,35.939274769374514,92,1371,
2025-01-15 18:20:14,4945.677042007446,6372.132778167725,34.35087311709916,49,6,
2025-01-15 18:21:20,4625.861167907715,6729.471445083618,24.719407660343816,52,3180,
2025-01-15 18:22:27,7917.679071426392,10842.804670333862,24.272462018902154,71,17502,
2025-01-15 18:23:38,7017.95768737793,12269.777297973633,39.03409012495524,205,215,
2025-01-15 18:24:50,4817.035436630249,7295.050144195557,37.93359244923798,94,521,
2025-01-15 18:25:57,4350.295543670654,6378.490447998047,37.96479314473744,77,4,
2025-01-15 18:27:04,5929.956674575806,8227.486371994019,30.467506069088298,70,4613,
2025-01-15 18:28:12,8410.263299942017,11010.940790176392,34.22185193442777,89,4703,
2025-01-15 18:29:23,3839.254379272461,5244.966506958008,34.85777709030418,49,400,
2025-01-15 18:30:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 205452 tokens (205452 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:31:31,3612.9372119903564,4875.693559646606,37.220165305234666,47,9,
2025-01-15 18:32:36,4783.996820449829,7677.322864532471,36.981659989144234,107,5223,
2025-01-15 18:33:44,14722.816467285156,20571.02656364441,36.592392625091165,214,34432,
2025-01-15 18:35:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87996 tokens (87996 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:36:05,2909.548044204712,4178.725481033325,22.061533074497174,28,5,
2025-01-15 18:37:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298836 tokens (298836 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:38:11,3770.6222534179688,5645.867586135864,35.19539488966097,66,192,
2025-01-15 18:39:17,5307.725429534912,7122.591018676758,38.01934446981634,69,175,
2025-01-15 18:40:24,5705.667972564697,7572.5531578063965,27.853882183583636,52,2123,
2025-01-15 18:41:31,33027.62508392334,36113.06619644165,22.687193645017132,70,57870,
2025-01-15 18:43:07,3199.3889808654785,5339.951038360596,35.50469360787189,76,21,
2025-01-15 18:44:13,9453.30548286438,12066.916465759277,34.43511700441122,90,8591,
2025-01-15 18:45:25,27959.139585494995,31548.804998397827,25.907707070892236,93,52938,
2025-01-15 18:46:56,6442.356586456299,9288.773536682129,31.61869872678335,90,255,
2025-01-15 18:48:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125357 tokens (125357 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:49:10,4741.091728210449,8803.116798400879,39.143037586556794,159,7,
2025-01-15 18:50:18,4891.670703887939,6441.802501678467,31.610215382873836,49,3509,
2025-01-15 18:51:25,4209.275722503662,6493.178367614746,36.34130385446577,83,112,
2025-01-15 18:52:31,4305.074691772461,5728.3313274383545,40.04899648567711,57,1412,
2025-01-15 18:53:37,20875.163793563843,23975.659132003784,30.962794496025197,96,42780,
2025-01-15 18:55:01,4440.329551696777,5464.94460105896,30.255265154749907,31,4,
2025-01-15 18:56:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 122162 tokens (122162 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:57:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146449 tokens (146449 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 18:58:11,3865.546941757202,4740.916967391968,50.26445812797148,44,63,
2025-01-15 18:59:16,3625.15926361084,4699.29313659668,30.722427464528003,33,26,
2025-01-15 19:00:20,5347.5182056427,7152.03595161438,50.9831505982008,92,654,
2025-01-15 19:01:28,14313.468933105469,15552.21676826477,45.20694075949562,56,10,
2025-01-15 19:02:43,4491.392374038696,5826.760053634644,35.94515632917201,48,280,
2025-01-15 19:03:49,2928.8547039031982,4306.276321411133,21.779823707338604,30,4,
2025-01-15 19:04:53,5092.452764511108,6942.126989364624,32.97878036054207,61,929,
2025-01-15 19:06:00,5869.469881057739,10834.625005722046,41.28773318313192,205,1186,
2025-01-15 19:07:11,9005.236387252808,10648.557424545288,29.817667326119015,49,9,
2025-01-15 19:08:22,3401.552200317383,5293.005704879761,31.72163622064931,60,4,
2025-01-15 19:09:27,17130.32555580139,20035.226345062256,36.14581275483651,105,5924,
2025-01-15 19:10:47,36854.71606254578,39719.99645233154,24.081412850893468,69,38865,
2025-01-15 19:12:27,3772.594928741455,5471.456527709961,38.849544918828634,66,22,
2025-01-15 19:13:32,3813.4725093841553,5563.974618911743,37.13220318114424,65,1085,
2025-01-15 19:14:38,4559.718608856201,6185.842514038086,34.43772016483351,56,724,
2025-01-15 19:15:44,3546.051025390625,5347.810745239258,28.305661092415,51,76,
2025-01-15 19:16:49,4454.112529754639,6225.541591644287,27.096766691180193,48,147,
2025-01-15 19:17:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67248 tokens (67248 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:19:01,10740.857124328613,12834.360361099243,23.40574360686722,49,7,
2025-01-15 19:20:14,4724.471092224121,5977.295637130737,34.32244377300667,43,14,
2025-01-15 19:21:20,5782.724857330322,12155.664205551147,16.475913901379514,105,185,
2025-01-15 19:22:32,4177.963495254517,5975.809097290039,41.16037546061638,74,503,
2025-01-15 19:23:38,4405.444622039795,5701.689958572388,33.94419155072784,44,49,
2025-01-15 19:24:44,13229.331970214844,15909.908771514893,19.025755940014697,51,26799,
2025-01-15 19:26:00,3637.615203857422,5092.833757400513,35.046282138052625,51,5,
2025-01-15 19:27:05,3872.1706867218018,5384.391784667969,35.70906402069146,54,2438,
2025-01-15 19:28:10,3841.9299125671387,5106.990098953247,30.82857275859034,39,20,
2025-01-15 19:29:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120108 tokens (120108 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:30:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 152543 tokens (152543 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:31:27,8864.558219909668,10136.216163635254,32.241374500348726,41,66,
2025-01-15 19:32:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 318863 tokens (318863 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:33:39,4842.245817184448,7555.373907089233,33.90919888460875,92,2844,
2025-01-15 19:34:47,3993.5896396636963,5113.355875015259,42.866089800367924,48,82,
2025-01-15 19:35:52,5327.985048294067,5431.708812713623,327.79373357943587,34,4,
2025-01-15 19:36:57,3568.5462951660156,5043.955087661743,33.21113460162729,49,8,
2025-01-15 19:38:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 282849 tokens (282849 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:39:04,5212.937831878662,6569.412469863892,36.12304913623719,49,227,
2025-01-15 19:40:11,17151.78155899048,20090.635538101196,21.777196299955698,64,35936,
2025-01-15 19:41:31,4294.882297515869,6117.296934127808,27.984849866446634,51,2876,
2025-01-15 19:42:37,6768.834590911865,9182.374477386475,36.46096776487881,88,305,
2025-01-15 19:43:46,7512.140989303589,12572.996139526367,38.333442519385315,194,8251,
2025-01-15 19:44:59,4106.371164321899,5598.418712615967,36.19187609787698,54,57,
2025-01-15 19:46:04,3655.465841293335,5156.78071975708,35.30238776707087,53,15,
2025-01-15 19:47:09,3506.760597229004,4942.742824554443,34.1229849977071,49,929,
2025-01-15 19:48:14,4933.318376541138,7515.496730804443,39.88880157326927,103,120,
2025-01-15 19:49:22,3932.4138164520264,5463.442087173462,34.61725757358215,53,30,
2025-01-15 19:50:27,3399.524450302124,4392.184019088745,35.25881490547893,35,5,
2025-01-15 19:51:32,3867.295265197754,5559.1583251953125,36.054927518831235,61,56,
2025-01-15 19:52:37,4178.977727890015,6435.894012451172,28.80035934192341,65,882,
2025-01-15 19:53:44,3608.6316108703613,5468.465805053711,25.271069941069467,47,5,
2025-01-15 19:54:49,4734.079837799072,6143.742799758911,36.178860746327096,51,98,
2025-01-15 19:55:55,21058.137893676758,25162.388563156128,32.89272777705971,135,41745,
2025-01-15 19:57:20,3052.685499191284,4263.803482055664,37.98143587233899,46,36,
2025-01-15 19:58:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 384143 tokens (384143 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 19:59:27,8180.465936660767,9563.91453742981,35.41873545049774,49,1951,
2025-01-15 20:00:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 184272 tokens (184272 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:01:43,4029.918670654297,5423.728942871094,35.15543038871966,49,1857,
2025-01-15 20:02:48,8046.2281703948975,10001.503705978394,31.70908594296807,62,10761,
2025-01-15 20:03:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146484 tokens (146484 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:05:00,4803.509712219238,6050.451755523682,34.48436134693829,43,9,
2025-01-15 20:06:06,3465.996503829956,5526.171922683716,39.80243587491358,82,87,
2025-01-15 20:07:11,4333.24408531189,6843.436479568481,35.853825470080764,90,3293,
2025-01-15 20:08:18,3701.3986110687256,4872.103214263916,35.87583057704727,42,6,
2025-01-15 20:09:23,6063.086748123169,8303.256034851074,33.92600748982195,76,5839,
2025-01-15 20:10:31,3218.792200088501,4150.154113769531,34.358287073953996,32,10,
2025-01-15 20:11:35,2892.0392990112305,4241.631507873535,27.41568879624068,37,4,
2025-01-15 20:12:40,4785.025596618652,11433.055400848389,39.25975178900997,261,4124,
2025-01-15 20:13:51,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 229506 tokens (229506 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:14:55,3640.8779621124268,5023.565053939819,37.60793046189181,52,7,
2025-01-15 20:16:00,3794.4984436035156,6081.496953964233,41.10190696415181,94,782,
2025-01-15 20:17:06,4660.53318977356,6498.277425765991,41.89919276684218,77,82,
2025-01-15 20:18:12,11341.00317955017,12224.812030792236,33.9439913481737,30,4,
2025-01-15 20:19:24,3368.297338485718,4957.921504974365,35.228452851028,56,1924,
2025-01-15 20:20:29,4505.485534667969,6824.113607406616,37.95347819456822,88,150,
2025-01-15 20:21:36,4024.3923664093018,6149.666786193848,36.230615342279435,77,3020,
2025-01-15 20:22:42,5023.38433265686,8030.534029006958,36.246948441674796,109,2195,
2025-01-15 20:23:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 130368 tokens (130368 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:24:52,3553.210973739624,4817.401647567749,37.96895594447797,48,115,
2025-01-15 20:25:57,3621.476173400879,5771.175861358643,40.00558798131513,86,31,
2025-01-15 20:27:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 393849 tokens (393849 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:28:04,3132.1020126342773,4110.037565231323,36.812241772371316,36,23,
2025-01-15 20:29:09,3373.0807304382324,4723.628759384155,34.060247406308186,46,28,
2025-01-15 20:30:13,3418.6906814575195,4642.212390899658,38.41370335915782,47,55,
2025-01-15 20:31:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 138325 tokens (138325 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:32:20,6563.0316734313965,12063.380479812622,38.90662347662899,214,1179,
2025-01-15 20:33:32,3457.7226638793945,4830.293655395508,35.699428519814205,49,66,
2025-01-15 20:34:37,3081.1779499053955,4031.1267375946045,33.68602646237527,32,4,
2025-01-15 20:35:41,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125031 tokens (125031 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:36:42,9758.221626281738,15245.262622833252,39.183231934137694,215,14963,
2025-01-15 20:37:58,2946.614980697632,3919.9283123016357,31.84996957651091,31,4,
2025-01-15 20:39:02,9678.003311157227,12115.70930480957,32.40751764392908,79,25301,
2025-01-15 20:40:14,7167.515516281128,11141.448259353638,41.01730213832807,163,6,
2025-01-15 20:41:25,6064.432859420776,7102.10108757019,54.93085212954266,57,2698,
2025-01-15 20:42:32,2931.9441318511963,3446.653366088867,19.428444906007698,10,4,
2025-01-15 20:43:35,3858.445167541504,5206.79783821106,27.44089198979878,37,15,
2025-01-15 20:44:41,3453.0105590820312,4767.108678817749,36.52695280444768,48,65,
2025-01-15 20:45:45,6735.821962356567,11448.57382774353,40.74052814241153,192,4,
2025-01-15 20:46:57,4978.729963302612,6783.9484214782715,25.481680508899334,46,42,
2025-01-15 20:48:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 91571 tokens (91571 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:49:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93216 tokens (93216 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:50:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86247 tokens (86247 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:51:13,3239.1064167022705,4556.561231613159,36.43388711076719,48,5,
2025-01-15 20:52:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 161515 tokens (161515 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 20:53:20,4927.4914264678955,8933.071851730347,38.44636323583734,154,4336,
2025-01-15 20:54:29,4709.646463394165,6301.386117935181,35.80987621775131,57,871,
2025-01-15 20:55:35,6772.359609603882,8934.667825698853,37.92243834141658,82,141,
2025-01-15 20:56:44,4397.394180297852,6925.071239471436,37.979535262066626,96,701,
2025-01-15 20:57:51,3240.8924102783203,4804.832696914673,27.494655881320334,43,40,
2025-01-15 20:58:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 246933 tokens (246933 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:00:00,3419.3546772003174,4736.985683441162,34.15220178248797,45,9,
2025-01-15 21:01:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 168532 tokens (168532 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:02:07,9484.983682632446,13165.350198745728,35.322568942749996,130,24814,
2025-01-15 21:03:20,7591.361999511719,12484.302282333374,40.67084176331719,199,756,
2025-01-15 21:04:32,5891.01505279541,7648.035526275635,39.27102788012937,69,17,
2025-01-15 21:05:40,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 153987 tokens (153987 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:06:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 519704 tokens (519704 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:07:45,4346.240043640137,7052.738428115845,35.47018559133436,96,284,
2025-01-15 21:08:52,10013.520002365112,16090.243101119995,36.86197254008455,224,25902,
2025-01-15 21:10:08,3938.7919902801514,5421.664714813232,35.741435608837136,53,2122,
2025-01-15 21:11:13,3866.809368133545,5683.474063873291,37.43123327021577,68,675,
2025-01-15 21:12:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 642783 tokens (642783 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:13:31,6075.03867149353,8503.35431098938,38.709959476073564,94,6771,
2025-01-15 21:14:40,2980.193614959717,4054.281711578369,30.723736818132178,33,4,
2025-01-15 21:15:44,4189.077615737915,5927.711725234985,28.183043075218446,49,840,
2025-01-15 21:16:50,3078.8073539733887,3986.6957664489746,34.145165390392755,31,5,
2025-01-15 21:17:53,3310.694694519043,4743.301630020142,34.20338041492221,49,379,
2025-01-15 21:18:58,8627.366542816162,10253.42583656311,33.20912109887896,54,7226,
2025-01-15 21:20:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 192712 tokens (192712 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:21:10,3396.116256713867,4487.687349319458,27.483322161261807,30,4,
2025-01-15 21:22:14,4796.154975891113,6308.075189590454,37.70040210027576,57,21,
2025-01-15 21:23:21,3387.568235397339,5819.157123565674,40.3028655365428,98,13,
2025-01-15 21:24:27,4090.608835220337,5808.617830276489,40.16276992644312,69,21,
2025-01-15 21:25:32,6365.2026653289795,8811.569213867188,40.46817925104317,99,197,
2025-01-15 21:26:41,3499.6023178100586,5145.26629447937,30.990530705557404,51,2046,
2025-01-15 21:27:46,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 335314 tokens (335314 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:28:49,3346.297025680542,4735.712051391602,36.70609505169255,51,99,
2025-01-15 21:29:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 335761 tokens (335761 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:30:59,7972.5470542907715,10270.866870880127,40.46434239861774,93,129,
2025-01-15 21:32:09,3942.2621726989746,5552.412748336792,31.052996382151314,50,499,
2025-01-15 21:33:15,3990.579843521118,5804.562568664551,26.46111196908152,48,1466,
2025-01-15 21:34:21,4530.00807762146,6823.575496673584,40.54818673629165,93,49,
2025-01-15 21:35:28,3433.8440895080566,4332.735776901245,34.486913645737324,31,4,
2025-01-15 21:36:32,3425.3153800964355,4854.825019836426,37.77519122558832,54,59,
2025-01-15 21:37:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 75670 tokens (75670 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:38:38,8498.411417007446,10003.931999206543,35.20376979674599,53,616,
2025-01-15 21:39:48,3378.1936168670654,5830.781698226929,28.541278713703836,70,98,
2025-01-15 21:40:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 442562 tokens (442562 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:41:56,4250.377178192139,5564.169883728027,38.81890939499271,51,58,
2025-01-15 21:43:02,6089.479446411133,7854.821443557739,28.88958631383231,51,12372,
2025-01-15 21:44:09,3456.111431121826,4668.362140655518,39.59576977147231,48,164,
2025-01-15 21:45:14,3587.2673988342285,6560.272455215454,41.3722807958211,123,689,
2025-01-15 21:46:21,7102.897167205811,8774.285078048706,30.513562811567937,51,17676,
2025-01-15 21:47:29,4348.137617111206,6605.412721633911,31.896865320376715,72,104,
2025-01-15 21:48:36,4289.39414024353,5539.8430824279785,38.38621344758572,48,18,
2025-01-15 21:49:42,3382.6918601989746,4356.205940246582,30.816195281462104,30,15,
2025-01-15 21:50:46,8533.623456954956,9991.024494171143,39.79687026351173,58,313,
2025-01-15 21:51:56,5554.127216339111,7126.772880554199,34.33704185802818,54,208,
2025-01-15 21:53:03,3947.345495223999,6235.563278198242,37.58383517508396,86,1782,
2025-01-15 21:54:09,3480.8926582336426,4656.48889541626,33.17465535060379,39,11,
2025-01-15 21:55:14,3429.5058250427246,5083.195686340332,34.46837362555611,57,4,
2025-01-15 21:56:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87070 tokens (87070 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:57:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 466428 tokens (466428 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 21:58:26,3513.3490562438965,4819.554328918457,33.685363947357565,44,4,
2025-01-15 21:59:31,3033.0991744995117,5437.483072280884,39.09525433385984,94,313,
2025-01-15 22:00:37,3360.541343688965,4744.390964508057,36.85371534069824,51,25,
2025-01-15 22:01:41,3392.672061920166,4702.8093338012695,39.69049741279253,52,5,
2025-01-15 22:02:46,3763.9708518981934,5121.131896972656,37.57844375587851,51,153,
2025-01-15 22:03:51,5958.963632583618,8625.245094299316,34.12993013177361,91,15539,
2025-01-15 22:05:00,8724.791288375854,10713.55390548706,38.21471670178236,76,3902,
2025-01-15 22:06:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156522 tokens (156522 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:07:12,3058.67600440979,3967.9651260375977,31.893046238235268,29,9,
2025-01-15 22:08:16,3263.0765438079834,4097.171068191528,34.768241670730376,29,18,
2025-01-15 22:09:21,15089.441776275635,17031.44669532776,25.746587719460862,50,33051,
2025-01-15 22:10:38,3286.9434356689453,4461.261510848999,39.17166990122927,46,52,
2025-01-15 22:11:42,3737.6372814178467,5456.922769546509,33.734944196574055,58,163,
2025-01-15 22:12:48,4248.448848724365,6215.906620025635,28.46312679075282,56,12,
2025-01-15 22:13:54,3951.328754425049,5543.718576431274,39.56317676071764,63,178,
2025-01-15 22:14:59,11758.605241775513,14465.102195739746,32.14487268222128,87,26436,
2025-01-15 22:16:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 147145 tokens (147145 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:17:17,4446.197271347046,6633.106708526611,41.15396754429487,90,620,
2025-01-15 22:18:24,6894.433975219727,11808.222770690918,42.3298616724642,208,16784,
2025-01-15 22:19:36,2974.548816680908,4031.4838886260986,35.006880727219084,37,7,
2025-01-15 22:20:40,2592.649459838867,3427.100658416748,38.34855777610029,32,18,
2025-01-15 22:21:43,4765.308380126953,6206.217288970947,36.08833263562672,52,2374,
2025-01-15 22:22:50,3005.9616565704346,4500.766754150391,36.79409448699588,55,577,
2025-01-15 22:23:54,24214.31803703308,31563.131093978882,38.10139104509609,280,48989,
2025-01-15 22:25:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 637582 tokens (637582 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:26:29,2950.7386684417725,4235.975027084351,39.68141708492003,51,148,
2025-01-15 22:27:33,4661.496639251709,6039.650917053223,41.35966554552126,57,1901,
2025-01-15 22:28:39,10389.034986495972,14835.292100906372,36.21023163014934,161,26843,
2025-01-15 22:29:54,4361.320972442627,6063.496351242065,41.123847091109795,70,8,
2025-01-15 22:31:00,13930.281400680542,16616.583108901978,32.014274396951286,86,33467,
2025-01-15 22:32:16,4211.728811264038,8980.650186538696,45.50295190964482,217,533,
2025-01-15 22:33:25,3344.2912101745605,5117.1839237213135,29.894645961949426,53,87,
2025-01-15 22:34:30,4086.852550506592,5686.136245727539,43.144315299523754,69,38,
2025-01-15 22:35:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94219 tokens (94219 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:36:38,7190.746068954468,9439.644813537598,32.46033205178019,73,16610,
2025-01-15 22:37:47,3900.960922241211,6373.490810394287,38.82662873358017,96,3436,
2025-01-15 22:38:54,3437.3884201049805,5052.11329460144,43.35103837539444,70,1877,
2025-01-15 22:39:59,3658.5702896118164,5261.38710975647,41.17750648139788,66,448,
2025-01-15 22:41:04,4089.7486209869385,6265.775918960571,38.60245690769723,84,5,
2025-01-15 22:42:10,3327.8682231903076,5009.633541107178,44.001383077432344,74,400,
2025-01-15 22:43:15,3203.996419906616,4459.251880645752,37.44257760274937,47,8,
2025-01-15 22:44:20,2794.009208679199,3827.7533054351807,39.66165333244769,41,7,
2025-01-15 22:45:24,3025.8076190948486,4614.353656768799,41.54742666233418,66,267,
2025-01-15 22:46:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 404194 tokens (404194 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:47:31,2588.978052139282,3376.2316703796387,38.107160519700145,30,9,
2025-01-15 22:48:35,12438.004493713379,14463.804721832275,25.175236576687144,51,32963,
2025-01-15 22:49:49,6557.188272476196,7961.378335952759,39.168486824232545,55,5,
2025-01-15 22:50:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 361572 tokens (361572 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:51:59,4640.168190002441,6950.7434368133545,40.682509747189606,94,4130,
2025-01-15 22:53:06,3510.874032974243,4953.164100646973,40.907166541888515,59,544,
2025-01-15 22:54:11,3847.6943969726562,5249.477863311768,38.52235476926123,54,484,
2025-01-15 22:55:16,4024.402856826782,7773.563623428345,45.34348100364258,170,324,
2025-01-15 22:56:24,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 239682 tokens (239682 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 22:57:26,24925.021171569824,28085.206508636475,29.112216590877644,92,48419,
2025-01-15 22:58:54,7245.763063430786,11980.112791061401,42.24444992577541,200,2697,
2025-01-15 23:00:06,3438.9631748199463,4607.369899749756,38.51398578924074,45,14,
2025-01-15 23:01:10,3992.2802448272705,5541.711091995239,36.787701822371695,57,13,
2025-01-15 23:02:16,3875.7901191711426,6256.303071975708,44.10814059057979,105,1822,
2025-01-15 23:03:22,3521.4414596557617,4730.751276016235,42.99973364683226,52,579,
2025-01-15 23:04:27,9882.933378219604,11339.917421340942,33.63111643626922,49,19369,
2025-01-15 23:05:38,13025.957584381104,14980.433225631714,26.6056014731024,52,22317,
2025-01-15 23:06:53,3985.785722732544,6171.307325363159,42.09521419933058,92,4423,
2025-01-15 23:07:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95117 tokens (95117 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:09:00,3326.735019683838,6117.982387542725,42.99153180825021,120,433,
2025-01-15 23:10:06,7329.695463180542,8586.236000061035,28.650090421574582,36,17107,
2025-01-15 23:11:15,3828.9358615875244,5318.741798400879,39.60247341086521,59,511,
2025-01-15 23:12:20,3641.561269760132,5058.557748794556,35.28590278083911,50,63,
2025-01-15 23:13:25,2953.091621398926,4136.988639831543,41.38873503108572,49,495,
2025-01-15 23:14:29,3211.2348079681396,4505.637884140015,42.49062831545444,55,97,
2025-01-15 23:15:34,2809.908866882324,4057.9586029052734,37.65875561159188,47,4,
2025-01-15 23:16:38,3418.795108795166,5640.762090682983,40.95470398155287,91,476,
2025-01-15 23:17:44,2766.293525695801,3781.6295623779297,28.561972541391263,29,4,
2025-01-15 23:18:47,3299.0469932556152,5457.450389862061,42.16079354909974,91,41,
2025-01-15 23:19:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 213740 tokens (213740 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:20:54,4657.956838607788,6029.542446136475,41.55774141047033,57,724,
2025-01-15 23:22:00,3734.8241806030273,5197.402000427246,39.6560095564493,58,149,
2025-01-15 23:23:05,5935.022830963135,8100.833892822266,48.942404010537125,106,2130,
2025-01-15 23:24:14,3515.0861740112305,4927.641868591309,33.980961022757654,48,120,
2025-01-15 23:25:18,2819.3047046661377,3664.884090423584,36.66125324499375,31,4,
2025-01-15 23:26:22,5677.20103263855,7475.149393081665,45.05134951708906,81,131,
2025-01-15 23:27:30,3004.448652267456,4281.741142272949,44.62564404473627,57,12,
2025-01-15 23:28:34,3554.525375366211,4932.749032974243,42.808726779800814,59,5,
2025-01-15 23:29:39,7936.420440673828,10880.723714828491,37.69992071617321,111,16514,
2025-01-15 23:30:50,13683.708190917969,16497.92194366455,35.178564493681115,99,34652,
2025-01-15 23:32:06,6969.633340835571,9176.258087158203,41.692634940906544,92,10769,
2025-01-15 23:33:15,3428.745985031128,4756.210803985596,36.91246600312404,49,247,
2025-01-15 23:34:20,4370.316743850708,5306.596517562866,37.38198878443368,35,18,
2025-01-15 23:35:25,9941.150665283203,12841.48383140564,32.75485765209831,95,26387,
2025-01-15 23:36:38,3131.28662109375,4300.167560577393,39.35387980603112,46,7,
2025-01-15 23:37:43,3491.4605617523193,5034.048318862915,33.70958946115366,52,53,
2025-01-15 23:38:48,3873.0242252349854,5595.630168914795,31.92837003830933,55,13,
2025-01-15 23:39:53,3615.9465312957764,5219.674825668335,36.1657272017464,58,9,
2025-01-15 23:40:58,6156.770706176758,8116.891145706177,43.87485496587479,86,75,
2025-01-15 23:42:07,3343.132495880127,4997.804880142212,38.074002200801445,63,31,
2025-01-15 23:43:12,3262.2177600860596,5388.847589492798,45.14184775955152,96,806,
2025-01-15 23:44:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 307318 tokens (307318 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:45:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 667835 tokens (667835 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:46:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 632477 tokens (632477 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:47:30,3331.700563430786,4663.965940475464,42.03366758972828,56,11,
2025-01-15 23:48:34,3700.7081508636475,4815.2360916137695,40.37583837486675,45,45,
2025-01-15 23:49:39,3581.8960666656494,4918.8783168792725,40.38946664502979,54,5191,
2025-01-15 23:50:44,11680.268287658691,14516.195297241211,34.55660165753938,98,32884,
2025-01-15 23:51:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 92202 tokens (92202 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:53:00,13862.342357635498,16250.969648361206,36.42259315121859,87,34816,
2025-01-15 23:54:16,3987.603187561035,8557.472467422485,41.357856959473054,189,524,
2025-01-15 23:55:24,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 196249 tokens (196249 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:56:26,3756.5388679504395,4958.061456680298,40.78158867724525,49,526,
2025-01-15 23:57:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 356384 tokens (356384 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-15 23:58:33,3963.1924629211426,5261.887550354004,31.570150989825454,41,6,
2025-01-15 23:59:39,3753.4890174865723,4905.9858322143555,41.648705130120014,48,5,
2025-01-16 00:00:44,3255.488157272339,4879.033088684082,38.8039768909991,63,310,
2025-01-16 00:01:48,2217.588186264038,2888.7813091278076,14.898841569372987,10,4,
2025-01-16 00:02:51,3151.4463424682617,3966.47047996521,38.035683329827854,31,4,
2025-01-16 00:03:55,3200.1025676727295,7152.102947235107,46.30566356885431,183,102,
2025-01-16 00:05:03,3340.3353691101074,4615.913391113281,39.981874193716,51,9,
2025-01-16 00:06:07,3441.936254501343,4563.594579696655,42.79378035342611,48,73,
2025-01-16 00:07:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 189147 tokens (189147 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:08:14,9680.694818496704,11637.457370758057,37.30651934014007,73,18711,
2025-01-16 00:09:26,3152.993679046631,4350.356817245483,40.08808895871353,48,75,
2025-01-16 00:10:30,4327.587604522705,9554.99243736267,44.76408609678534,234,438,
2025-01-16 00:11:40,3812.220573425293,5093.163251876831,43.71780325696959,56,11,
2025-01-16 00:12:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247317 tokens (247317 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:13:47,3244.8112964630127,4405.079364776611,38.78414069035411,45,56,
2025-01-16 00:14:51,4924.506664276123,7317.347764968872,38.03010570725097,91,4522,
2025-01-16 00:15:59,6532.723188400269,8720.970392227173,43.870728970708115,96,282,
2025-01-16 00:17:07,2616.661548614502,3427.790880203247,39.45116857914896,32,5,
2025-01-16 00:18:11,6379.9848556518555,9925.522565841675,48.511682587855354,172,2328,
2025-01-16 00:19:21,2922.0783710479736,3689.4733905792236,36.487075479201465,28,9,
2025-01-16 00:20:24,23072.50475883484,28699.14698600769,41.58785836247759,234,49363,
2025-01-16 00:21:53,3801.0458946228027,5827.207088470459,43.43188501843184,88,15,
2025-01-16 00:22:59,2432.100772857666,3236.626386642456,38.532023678108125,31,4,
2025-01-16 00:24:02,3082.379102706909,4312.599897384644,43.89455960557528,54,3480,
2025-01-16 00:25:06,4107.009649276733,8446.760416030884,46.54645182563854,202,292,
2025-01-16 00:26:15,3113.0664348602295,4290.484428405762,46.712382774429784,55,597,
2025-01-16 00:27:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 475473 tokens (475473 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:28:22,3228.956699371338,4038.0964279174805,39.54817551413204,32,4,
2025-01-16 00:29:26,2949.370861053467,3790.1151180267334,41.62978183877491,35,13,
2025-01-16 00:30:29,2666.4960384368896,4979.8643589019775,42.79474181616557,99,1149,
2025-01-16 00:31:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 434327 tokens (434327 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:32:37,11110.328912734985,13808.392524719238,34.09852888247505,92,34066,
2025-01-16 00:33:50,2717.308521270752,3497.4231719970703,42.301474493878125,33,4,
2025-01-16 00:34:54,3471.618413925171,4779.908180236816,35.92476316046044,47,62,
2025-01-16 00:35:59,2792.0608520507812,3166.3084030151367,26.720281733927585,10,4,
2025-01-16 00:37:02,3527.712345123291,4856.296539306641,40.64477075402253,54,373,
2025-01-16 00:38:07,5557.68895149231,7501.201391220093,44.764313426357894,87,125,
2025-01-16 00:39:14,3019.542932510376,4160.3686809539795,45.58101889876008,52,8,
2025-01-16 00:40:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 351164 tokens (351164 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:41:21,3266.655921936035,5598.583459854126,41.59648978055301,97,464,
2025-01-16 00:42:26,3210.996389389038,3960.944652557373,41.33618480431319,31,5,
2025-01-16 00:43:30,3120.9728717803955,4305.495977401733,43.055301967493584,51,147,
2025-01-16 00:44:34,2379.9691200256348,3094.8898792266846,41.96269252766712,30,4,
2025-01-16 00:45:38,3173.2611656188965,5117.882490158081,46.7957431360392,91,141,
2025-01-16 00:46:43,2598.4647274017334,3339.374303817749,41.8404633801004,31,4,
2025-01-16 00:47:46,16031.140089035034,18556.230783462524,34.85023337743997,88,39495,
2025-01-16 00:49:05,3873.5547065734863,5088.24610710144,40.33946398130637,49,181,
2025-01-16 00:50:10,3559.6163272857666,4722.580432891846,42.133716564247386,49,444,
2025-01-16 00:51:14,4182.739496231079,7186.013698577881,44.95094050836552,135,10146,
2025-01-16 00:52:22,4185.4407787323,5211.019039154053,46.80286415223033,48,33,
2025-01-16 00:53:27,9292.78016090393,11824.236154556274,40.293017242158754,102,26851,
2025-01-16 00:54:39,24662.69087791443,27229.699850082397,31.554233303513314,81,54102,
2025-01-16 00:56:06,4179.868936538696,5675.551891326904,34.09813546161707,51,8783,
2025-01-16 00:57:12,4365.2074337005615,7653.738737106323,47.13350298337575,155,1254,
2025-01-16 00:58:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 187933 tokens (187933 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 00:59:21,4072.34525680542,6309.121131896973,43.36598989652002,97,59,
2025-01-16 01:00:27,3437.1392726898193,4191.523313522339,37.11637373598187,28,10,
2025-01-16 01:01:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 342866 tokens (342866 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:02:33,4050.1339435577393,6385.009527206421,44.97027624740406,105,5,
2025-01-16 01:03:40,2739.049196243286,3593.8422679901123,36.26608710883612,31,16,
2025-01-16 01:04:43,15439.087390899658,18790.563583374023,36.40186980111844,122,38745,
2025-01-16 01:06:02,2576.383352279663,3386.5976333618164,37.02724168220146,30,6,
2025-01-16 01:07:05,4130.514860153198,8178.32088470459,49.656529680736455,201,240,
2025-01-16 01:08:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 307971 tokens (307971 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:09:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 92322 tokens (92322 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:10:16,5796.471357345581,7289.944171905518,34.148596146376825,51,16412,
2025-01-16 01:11:23,4223.678350448608,5331.755876541138,45.123196547733954,50,4,
2025-01-16 01:12:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 282130 tokens (282130 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:13:31,2942.469358444214,4267.714500427246,40.74717823088718,54,1194,
2025-01-16 01:14:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 168624 tokens (168624 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:15:37,3021.9674110412598,4099.412202835083,44.54984641958819,48,9,
2025-01-16 01:16:41,3490.471839904785,5479.81071472168,45.24116083956983,90,259,
2025-01-16 01:17:47,3888.134002685547,5155.526161193848,40.24011010138025,51,2514,
2025-01-16 01:18:52,6137.644290924072,7478.112697601318,38.792409982192446,52,12072,
2025-01-16 01:19:59,9704.12540435791,12638.968229293823,35.7770436998761,105,31513,
2025-01-16 01:21:12,23727.077960968018,27141.653776168823,36.314906070611805,124,51446,
2025-01-16 01:22:39,3268.296957015991,4324.987173080444,44.478503998122775,47,5,
2025-01-16 01:23:43,2996.471405029297,4226.7961502075195,37.38850265369288,46,48,
2025-01-16 01:24:47,2841.9246673583984,4040.8685207366943,40.035235899286796,48,72,
2025-01-16 01:25:52,2797.236204147339,3622.114658355713,41.21819381575361,34,57,
2025-01-16 01:26:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136306 tokens (136306 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:27:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95524 tokens (95524 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:28:58,4367.068529129028,6685.162305831909,39.25639286665659,91,972,
2025-01-16 01:30:04,3011.3046169281006,3859.437942504883,41.267096745901206,35,12,
2025-01-16 01:31:08,4649.972438812256,9112.835884094238,46.382777008074214,207,2482,
2025-01-16 01:32:17,2954.3514251708984,4226.643323898315,43.229073497216,55,574,
2025-01-16 01:33:22,3204.3404579162598,5144.298315048218,44.846335027411975,87,236,
2025-01-16 01:34:27,2664.649248123169,4973.641633987427,46.34055991481748,107,473,
2025-01-16 01:35:32,3061.333179473877,4017.735481262207,40.77781904861145,39,28,
2025-01-16 01:36:36,3679.037570953369,5237.975358963013,43.619444292782354,68,47,
2025-01-16 01:37:41,10534.814834594727,15125.25725364685,46.83644415354526,215,23132,
2025-01-16 01:38:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144283 tokens (144283 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:39:58,17786.309003829956,20483.456134796143,35.22240181460497,95,39111,
2025-01-16 01:41:19,3058.971166610718,4297.146797180176,38.7667135541377,48,50,
2025-01-16 01:42:23,3283.710241317749,4600.725173950195,34.927470342386556,46,33,
2025-01-16 01:43:28,3040.240526199341,5720.528364181519,39.54799126343118,106,62,
2025-01-16 01:44:33,3134.80806350708,4428.331613540649,39.42719094575159,51,3365,
2025-01-16 01:45:38,3051.6326427459717,4139.894485473633,44.107032072070965,48,53,
2025-01-16 01:46:42,24532.732725143433,27700.770616531372,30.618320653198886,97,50805,
2025-01-16 01:48:10,4080.4200172424316,8115.226268768311,43.86827742548042,177,6907,
2025-01-16 01:49:18,3964.2648696899414,5212.068557739258,40.8718138024804,51,1785,
2025-01-16 01:50:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 270418 tokens (270418 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 01:51:25,2888.455629348755,4901.141166687012,44.2195257773358,89,590,
2025-01-16 01:52:30,5304.003000259399,6824.867248535156,33.53356491732909,51,14623,
2025-01-16 01:53:36,3591.219663619995,5326.717853546143,37.45322258317424,65,3506,
2025-01-16 01:54:42,3172.0194816589355,4432.01208114624,40.47642821136574,51,87,
2025-01-16 01:55:46,6070.479154586792,7253.937244415283,40.55910421547437,48,388,
2025-01-16 01:56:54,3002.7472972869873,4178.0686378479,39.98906373772607,47,113,
2025-01-16 01:57:58,5154.218912124634,8885.306358337402,46.367179138511815,173,387,
2025-01-16 01:59:07,3238.0216121673584,4941.1680698394775,41.100399607252115,70,429,
2025-01-16 02:00:12,3276.2067317962646,4498.7523555755615,40.08030379146476,49,231,
2025-01-16 02:01:16,19571.563005447388,22376.814603805542,33.86505511862151,95,42835,
2025-01-16 02:02:38,3543.2865619659424,4774.00803565979,41.43910794611412,51,134,
2025-01-16 02:03:43,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247878 tokens (247878 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:04:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 204795 tokens (204795 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:05:46,3779.6058654785156,6364.30287361145,43.71885743065334,113,3396,
2025-01-16 02:06:52,3494.340181350708,4938.416242599487,45.01147948106736,65,46,
2025-01-16 02:07:57,3085.850954055786,4095.1151847839355,40.62365310461851,41,6,
2025-01-16 02:09:01,3880.1162242889404,5416.268348693848,40.360586048089665,62,180,
2025-01-16 02:10:07,3395.8230018615723,4610.366106033325,39.521034564461324,48,246,
2025-01-16 02:11:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247399 tokens (247399 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:12:13,6424.199819564819,7958.680868148804,32.58430597506557,50,13962,
2025-01-16 02:13:21,4315.645694732666,5714.294195175171,37.17874790095455,52,3248,
2025-01-16 02:14:27,27105.570077896118,29547.040224075317,27.442481778797195,67,53552,
2025-01-16 02:15:56,3603.8620471954346,5033.975839614868,40.55621329396239,58,1790,
2025-01-16 02:17:02,3554.945230484009,5877.042770385742,41.77257773767111,97,6,
2025-01-16 02:18:07,8540.199518203735,12174.846649169922,44.295909396079914,161,6,
2025-01-16 02:19:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 198318 tokens (198318 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:20:21,5098.632574081421,6995.314121246338,34.79761802852679,66,711,
2025-01-16 02:21:28,4389.599084854126,5965.064525604248,37.449250535072665,59,2588,
2025-01-16 02:22:34,3228.0657291412354,4089.0817642211914,36.0039752304047,31,5,
2025-01-16 02:23:38,37056.658029556274,40291.27311706543,27.514865785324478,89,62739,
2025-01-16 02:25:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 274372 tokens (274372 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:26:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 155247 tokens (155247 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:27:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 168294 tokens (168294 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:28:23,3503.685235977173,5960.41202545166,36.63411022568435,90,75,
2025-01-16 02:29:29,3793.4703826904297,10127.130508422852,41.050513421721334,260,187,
2025-01-16 02:30:39,3743.295192718506,5392.862558364868,32.735856155132396,54,336,
2025-01-16 02:31:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 264211 tokens (264211 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:32:47,8448.906183242798,11156.538486480713,33.60869933896781,91,15631,
2025-01-16 02:33:58,3283.968925476074,4880.523681640625,40.08631696024572,64,284,
2025-01-16 02:35:03,7860.3668212890625,14721.712827682495,41.828527483174895,287,19245,
2025-01-16 02:36:18,7723.180294036865,8555.545568466187,32.43768190415139,27,7,
2025-01-16 02:37:26,7367.2754764556885,12230.99422454834,43.38244272096232,211,3475,
2025-01-16 02:38:39,4003.964424133301,5878.095865249634,28.813347247317246,54,4119,
2025-01-16 02:39:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 138176 tokens (138176 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:40:46,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94673 tokens (94673 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:41:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 97637 tokens (97637 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:42:48,3322.7062225341797,4445.118188858032,32.073784920440545,36,4,
2025-01-16 02:43:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 117423 tokens (117423 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 02:44:54,3483.6511611938477,5706.701755523682,40.93474086109726,91,235,
2025-01-16 02:45:59,4425.474405288696,7074.908971786499,40.385975691952886,107,686,
2025-01-16 02:47:06,5849.736452102661,8093.72615814209,36.987691956257855,83,1239,
2025-01-16 02:48:14,4817.559719085693,7142.359972000122,38.713003358965445,90,2434,
2025-01-16 02:49:22,4815.6418800354,6615.592479705811,37.22324380028452,67,1172,
2025-01-16 02:50:28,3580.951690673828,6188.591241836548,41.41676711102342,108,158,
2025-01-16 02:51:34,5298.981189727783,8139.332294464111,35.20691503006389,100,8178,
2025-01-16 02:52:43,3369.907855987549,4599.600553512573,39.0341425110587,48,88,
2025-01-16 02:53:47,9943.085432052612,12492.45023727417,28.24232916863489,72,24257,
2025-01-16 02:55:00,4091.116189956665,6581.1450481414795,38.5537700434453,96,1111,
2025-01-16 02:56:06,4395.123481750488,5817.742109298706,35.14645389268623,50,35,
2025-01-16 02:57:12,6143.1989669799805,9895.380020141602,40.24326061578667,151,524,
2025-01-16 02:58:22,3784.3363285064697,5406.015872955322,33.915455237916575,55,175,
2025-01-16 02:59:27,4314.522743225098,5925.083637237549,32.28688849538029,52,2706,
2025-01-16 03:00:33,3829.241991043091,5156.036138534546,37.68482103612988,50,37,
2025-01-16 03:01:38,4729.179620742798,6337.673902511597,33.57176995408295,54,4816,
2025-01-16 03:02:45,4535.510778427124,6953.039884567261,37.641739149644394,91,1351,
2025-01-16 03:03:52,3211.7815017700195,4379.235029220581,23.983824059485507,28,8,
2025-01-16 03:04:56,7470.685958862305,9205.601453781128,38.04219870840922,66,3078,
2025-01-16 03:06:05,3669.525623321533,5078.634738922119,38.322085495121,54,20,
2025-01-16 03:07:10,3875.3459453582764,4994.232177734375,28.5998692932738,32,16,
2025-01-16 03:08:15,3536.121606826782,4949.427604675293,36.08560359726612,51,65,
2025-01-16 03:09:20,6521.037578582764,9162.362813949585,35.96679376245259,95,4108,
2025-01-16 03:10:30,7196.89154624939,8862.66303062439,30.616444379305293,51,14343,
2025-01-16 03:11:38,3989.983320236206,5518.032073974609,38.611333477191266,59,8,
2025-01-16 03:12:44,3786.113977432251,7936.511754989624,40.23710712814718,167,618,
2025-01-16 03:13:52,4725.384712219238,6149.275779724121,37.221948511042434,53,158,
2025-01-16 03:14:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 157383 tokens (157383 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:16:00,10185.24980545044,12248.012065887451,27.632849937793665,57,25976,
2025-01-16 03:17:12,6038.520097732544,7581.680059432983,31.75302704588441,49,148,
2025-01-16 03:18:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247784 tokens (247784 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:19:22,3774.006128311157,4972.02467918396,31.719041389063243,38,98,
2025-01-16 03:20:27,3491.3361072540283,4755.351543426514,35.60083105967654,45,26,
2025-01-16 03:21:31,4527.188777923584,5920.124292373657,33.02378288356059,46,14,
2025-01-16 03:22:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 494734 tokens (494734 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:23:41,7878.844738006592,9746.10185623169,35.88150734360899,67,381,
2025-01-16 03:24:50,3577.8539180755615,4490.805864334106,33.95578499727619,31,5,
2025-01-16 03:25:55,3764.509916305542,4727.99015045166,32.1750243558174,31,17,
2025-01-16 03:27:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136705 tokens (136705 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:28:01,3488.6786937713623,4720.878601074219,37.33160482107866,46,5,
2025-01-16 03:29:06,24419.001817703247,27773.338079452515,27.42718464129846,92,30349,
2025-01-16 03:30:34,3425.4701137542725,5153.678178787231,36.45394398665679,63,1574,
2025-01-16 03:31:39,8098.920106887817,11477.173805236816,35.52131092423453,120,10958,
2025-01-16 03:32:50,4482.6500415802,5843.128204345703,36.01674862637675,49,176,
2025-01-16 03:33:56,4592.5774574279785,6122.963190078735,36.59208185573142,56,146,
2025-01-16 03:35:02,7468.930959701538,9306.637048721313,33.7377126682268,62,1315,
2025-01-16 03:36:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151641 tokens (151641 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:37:13,4655.061960220337,7505.828380584717,31.92124031977637,91,2095,
2025-01-16 03:38:20,12964.316606521606,15634.500741958618,35.952576725308234,96,19266,
2025-01-16 03:39:36,5544.559001922607,7527.646780014038,35.298487930458435,70,418,
2025-01-16 03:40:43,4275.203227996826,9198.250770568848,39.812737599036225,196,963,
2025-01-16 03:41:53,3800.0786304473877,5665.374517440796,35.91923429799359,67,931,
2025-01-16 03:42:58,4831.250429153442,6325.250625610352,32.12850983141383,48,7477,
2025-01-16 03:44:05,8180.403470993042,14431.70428276062,40.791510067789844,255,4210,
2025-01-16 03:45:19,6198.587417602539,9174.582481384277,34.610272460974116,103,9734,
2025-01-16 03:46:28,4276.4317989349365,5959.829568862915,33.860090002637136,57,2266,
2025-01-16 03:47:34,5558.856248855591,8186.348438262939,35.77555829812093,94,1940,
2025-01-16 03:48:42,3773.6117839813232,6019.148826599121,39.18884361729857,88,7,
2025-01-16 03:49:48,6691.421270370483,11506.356716156006,52.96021158979392,255,5,
2025-01-16 03:51:00,92631.01172447205,95691.15567207336,52.2851221183289,160,785,
2025-01-16 03:53:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 271037 tokens (271037 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:54:38,2881.8559646606445,3735.2259159088135,36.3265661682349,31,4,
2025-01-16 03:55:41,4505.614757537842,5919.19469833374,36.78603416706808,52,16,
2025-01-16 03:56:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 289980 tokens (289980 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 03:57:49,2965.688705444336,4607.286691665649,40.81389022303987,67,17,
2025-01-16 03:58:54,6285.2373123168945,8679.640054702759,37.17002090939696,89,7041,
2025-01-16 04:00:02,6796.515226364136,8765.767574310303,29.960609193399762,59,6400,
2025-01-16 04:01:11,16262.175559997559,19828.832626342773,34.205699547395646,122,32643,
2025-01-16 04:02:31,3979.3851375579834,5155.131816864014,40.82512061895118,48,33,
2025-01-16 04:03:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146856 tokens (146856 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:04:37,4219.094276428223,6243.91770362854,42.96670950725474,87,9,
2025-01-16 04:05:44,5365.546226501465,9625.83327293396,40.60759242616476,173,7245,
2025-01-16 04:06:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 679007 tokens (679007 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:07:56,4777.3706912994385,6328.562498092651,38.67993612217326,60,4,
2025-01-16 04:09:03,3790.2021408081055,5191.140174865723,37.83179463440867,53,5,
2025-01-16 04:10:08,35378.00598144531,37775.13360977173,21.27546293211191,51,57438,
2025-01-16 04:11:46,3627.375364303589,5844.714403152466,27.96144338494444,62,67,
2025-01-16 04:12:51,3572.092294692993,4855.0214767456055,28.84025129181499,37,14,
2025-01-16 04:13:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 152988 tokens (152988 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:14:58,2911.871910095215,3848.5374450683594,29.89327455162829,28,5,
2025-01-16 04:16:02,3893.9335346221924,5476.657390594482,32.222929987158096,51,32,
2025-01-16 04:17:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 158811 tokens (158811 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:18:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 252714 tokens (252714 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:19:11,38433.833599090576,42022.46117591858,38.176154272629944,137,21682,
2025-01-16 04:20:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 214584 tokens (214584 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:21:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 137263 tokens (137263 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:22:56,3338.7246131896973,4851.198673248291,31.736081475764593,48,124,
2025-01-16 04:24:01,6290.144920349121,8863.543272018433,36.527574496588954,94,1616,
2025-01-16 04:25:10,3939.0571117401123,5323.21572303772,38.290409471436284,53,7102,
2025-01-16 04:26:15,4150.2110958099365,5338.310241699219,44.60907171204971,53,9,
2025-01-16 04:27:20,6502.185106277466,9024.647235870361,44.00462496454386,111,326,
2025-01-16 04:28:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 209421 tokens (209421 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:29:31,7399.139642715454,12502.2554397583,46.63817351311452,238,92,
2025-01-16 04:30:43,3513.5796070098877,4579.980850219727,36.57160027553143,39,10,
2025-01-16 04:31:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 409640 tokens (409640 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:32:50,6556.382894515991,8732.821464538574,41.35195968295404,90,4671,
2025-01-16 04:33:58,6017.040491104126,7432.904958724976,36.02039684327833,51,7949,
2025-01-16 04:35:06,4136.360168457031,6045.167922973633,47.67373759074308,91,161,
2025-01-16 04:36:12,3528.252363204956,4672.497749328613,42.82298237268542,49,38,
2025-01-16 04:37:17,4018.094778060913,5027.67539024353,39.62041219623246,40,77,
2025-01-16 04:38:22,8015.7506465911865,9651.890516281128,39.72765483205168,65,5132,
2025-01-16 04:39:31,5884.7010135650635,7486.1743450164795,41.212050618528984,66,1040,
2025-01-16 04:40:39,3368.34979057312,4729.608774185181,39.669159689740006,54,30,
2025-01-16 04:41:44,5280.247449874878,7552.05512046814,38.29549531245343,87,6,
2025-01-16 04:42:51,3598.7606048583984,5103.182792663574,31.905937302099968,48,5,
2025-01-16 04:43:56,4566.426038742065,6027.7674198150635,34.89944284103757,51,188,
2025-01-16 04:45:02,12843.933582305908,15489.1676902771,34.40149199867759,91,26796,
2025-01-16 04:46:18,3578.97686958313,5255.895614624023,38.76156802005031,65,138,
2025-01-16 04:47:23,4270.542860031128,6830.177068710327,37.89603986034126,97,104,
2025-01-16 04:48:30,4352.631092071533,5752.788066864014,34.99607607015804,49,162,
2025-01-16 04:49:36,3129.0841102600098,4081.714868545532,32.54146449752642,31,4,
2025-01-16 04:50:40,4592.352867126465,5724.1315841674805,35.342597804434945,40,22,
2025-01-16 04:51:45,7662.075042724609,10335.575580596924,35.907978562218986,96,12095,
2025-01-16 04:52:56,4090.4080867767334,5681.531667709351,35.82374158931776,57,1062,
2025-01-16 04:54:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 702223 tokens (702223 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:55:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 180702 tokens (180702 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 04:56:06,4604.716777801514,8358.009576797485,45.826427409556494,172,179,
2025-01-16 04:57:14,44370.18895149231,47571.810483932495,29.984805832696754,96,65155,
2025-01-16 04:59:02,4030.4269790649414,4901.624917984009,41.322411809957615,36,4,
2025-01-16 05:00:07,4062.6494884490967,6734.2846393585205,34.81014238352974,93,1606,
2025-01-16 05:01:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 593849 tokens (593849 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:02:17,3365.6535148620605,4888.613224029541,36.11389038654576,55,64,
2025-01-16 05:03:22,3616.1293983459473,4747.435092926025,27.40196584222683,31,17,
2025-01-16 05:04:27,4683.489084243774,6175.397634506226,36.19524802006163,54,8,
2025-01-16 05:05:33,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 541527 tokens (541527 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:06:35,3759.31715965271,4966.343879699707,32.3108008731418,39,54,
2025-01-16 05:07:40,3347.3494052886963,4459.091663360596,31.48211714170215,35,13,
2025-01-16 05:08:45,3338.4335041046143,4313.363552093506,31.797153102366188,31,5,
2025-01-16 05:09:49,10508.311033248901,21643.112659454346,38.34823594836817,427,6,
2025-01-16 05:11:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 215635 tokens (215635 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:12:12,3153.459072113037,4447.736978530884,37.0863164409944,48,6,
2025-01-16 05:13:17,5074.982643127441,7736.231803894043,37.9521021514971,101,195,
2025-01-16 05:14:25,24215.248346328735,27764.474391937256,23.385380061294324,83,49055,
2025-01-16 05:15:52,5481.332540512085,8079.025030136108,32.72134801925784,85,4812,
2025-01-16 05:17:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 300832 tokens (300832 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:18:03,5695.194721221924,8596.896886825562,32.73940417666444,95,9448,
2025-01-16 05:19:11,3917.9112911224365,5087.184429168701,36.774983193275595,43,9,
2025-01-16 05:20:16,3840.6858444213867,4856.849431991577,32.47508610194179,33,16,
2025-01-16 05:21:21,40495.46766281128,48640.14530181885,36.34275205471087,296,59678,
2025-01-16 05:23:10,3671.1387634277344,6126.667737960815,26.87811900592617,66,8,
2025-01-16 05:24:16,6324.03826713562,8265.755653381348,29.35545636237433,57,925,
2025-01-16 05:25:24,7969.026803970337,9288.578271865845,36.37599682000506,48,215,
2025-01-16 05:26:33,40368.00146102905,44903.01609039307,29.32735853569932,133,64364,
2025-01-16 05:28:18,8288.450479507446,10750.556945800781,35.74175252156447,88,5213,
2025-01-16 05:29:29,6854.573011398315,12412.694454193115,35.623546919197814,198,12533,
2025-01-16 05:30:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 433943 tokens (433943 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:31:44,8946.515560150146,10455.699920654297,43.73223161281129,66,550,
2025-01-16 05:32:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150507 tokens (150507 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:33:56,4838.642835617065,5930.200815200806,29.315895809953243,32,4,
2025-01-16 05:35:02,9956.25114440918,11284.960746765137,63.219231539426076,84,102,
2025-01-16 05:36:13,11912.322998046875,12858.318090438843,60.254012371115316,57,132,
2025-01-16 05:37:26,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73640 tokens (73640 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 05:38:27,7232.756853103638,8660.280227661133,62.34573919154794,89,2861,
2025-01-16 05:39:36,428265.9568786621,430132.88283348083,28.924553681746463,54,32318,
2025-01-16 05:47:46,3525.925397872925,4754.731178283691,49.64169356333012,61,64,
2025-01-16 05:48:51,4140.269756317139,5311.293840408325,49.52929729452396,58,2136,
2025-01-16 05:49:56,9331.120252609253,11721.725225448608,53.54293220931963,128,9347,
2025-01-16 05:51:08,2825.2220153808594,3746.1917400360107,53.20478913500398,49,23,
2025-01-16 05:52:11,3082.376480102539,4175.931692123413,46.63687707706385,51,7,
2025-01-16 05:53:16,4401.067733764648,7744.588375091553,51.14369502804601,171,3361,
2025-01-16 05:54:23,2988.954544067383,3680.724859237671,44.812561800037564,31,4,
2025-01-16 05:55:27,4351.522445678711,5359.9443435668945,46.607476591321976,47,5,
2025-01-16 05:56:32,3513.8533115386963,5455.801010131836,50.979745783947415,99,20,
2025-01-16 05:57:38,3399.1189002990723,4745.206594467163,41.60204438582965,56,11,
2025-01-16 05:58:43,3270.4100608825684,4797.813892364502,36.66351939530432,56,105,
2025-01-16 05:59:47,40790.67611694336,44298.351526260376,35.92122568277593,126,51650,
2025-01-16 06:01:32,3629.2741298675537,4599.966049194336,53.57003490465265,52,182,
2025-01-16 06:02:36,4042.4118041992188,5372.9658126831055,42.839298244608074,57,5,
2025-01-16 06:03:42,11692.91090965271,13977.540731430054,38.95598278182407,89,14586,
2025-01-16 06:04:56,3395.2317237854004,4737.734317779541,35.7541208596052,48,105,
2025-01-16 06:06:00,3863.7990951538086,9333.386421203613,52.106307662854356,285,214,
2025-01-16 06:07:10,6654.304504394531,8444.122076034546,48.60830588465039,87,4481,
2025-01-16 06:08:18,3724.6861457824707,4638.528823852539,28.451286664471656,26,4,
2025-01-16 06:09:23,4395.927667617798,6651.870012283325,21.277139512675188,48,84,
2025-01-16 06:10:29,13578.826427459717,21639.780044555664,18.236055804642934,147,7260,
2025-01-16 06:11:51,4111.354351043701,6053.711414337158,33.97933430843582,66,67,
2025-01-16 06:12:57,4071.441650390625,6203.214168548584,30.491058237379747,65,14,
2025-01-16 06:14:03,4129.321336746216,6283.676624298096,26.45803147203841,57,898,
2025-01-16 06:15:10,5206.778287887573,9489.392042160034,42.96441625547861,184,1152,
2025-01-16 06:16:19,3579.3023109436035,5685.395240783691,25.639894249158385,54,2614,
2025-01-16 06:17:25,18183.80355834961,22928.39741706848,34.35488997661288,163,40105,
2025-01-16 06:18:48,3762.2103691101074,5524.108648300171,36.89202763162933,65,16,
2025-01-16 06:19:53,3775.6712436676025,6195.103168487549,36.78549459771362,89,2792,
2025-01-16 06:21:00,4020.9269523620605,6035.058975219727,33.26494948674709,67,676,
2025-01-16 06:22:06,4163.802146911621,6983.068466186523,33.696710151325256,95,5826,
2025-01-16 06:23:13,45068.143367767334,48164.45827484131,19.054909390904026,59,65477,
2025-01-16 06:25:01,4982.309341430664,7345.484733581543,35.54539382857493,84,21,
2025-01-16 06:26:08,3556.156635284424,4540.97580909729,31.47785992019137,31,8,
2025-01-16 06:27:13,5931.709766387939,7647.3548412323,28.560685842580334,49,12496,
2025-01-16 06:28:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 255560 tokens (255560 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:29:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 164885 tokens (164885 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:30:23,10570.36304473877,12617.398738861084,28.333653470985535,58,18427,
2025-01-16 06:31:36,4184.593677520752,5750.844478607178,31.923367550916897,50,361,
2025-01-16 06:32:42,3163.020372390747,4097.676992416382,32.09734929088417,30,4,
2025-01-16 06:33:46,3749.187231063843,5142.138957977295,30.869698618544952,43,9,
2025-01-16 06:34:51,6781.291961669922,9885.825157165527,31.88885212876447,99,627,
2025-01-16 06:36:01,3169.963836669922,4492.883920669556,35.52746728124585,47,5,
2025-01-16 06:37:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 285583 tokens (285583 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:38:07,7938.101768493652,11105.796098709106,21.466717716849562,68,15253,
2025-01-16 06:39:18,3237.4796867370605,4325.325965881348,34.012158435753115,37,19,
2025-01-16 06:40:22,3583.9622020721436,5413.0048751831055,38.27138700976242,70,317,
2025-01-16 06:41:28,3346.49920463562,3848.886251449585,19.90497180096091,10,4,
2025-01-16 06:42:32,3205.4245471954346,4403.624534606934,33.38340879673433,40,24,
2025-01-16 06:43:36,3163.6672019958496,4913.977146148682,23.995750090038158,42,7,
2025-01-16 06:44:41,8871.608018875122,11410.283088684082,30.724688215364893,78,18377,
2025-01-16 06:45:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 116620 tokens (116620 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:46:54,4229.768753051758,5700.2928256988525,35.361542845330405,52,44,
2025-01-16 06:47:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 134437 tokens (134437 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:49:01,3723.4721183776855,6278.0516147613525,39.92829353887558,102,19,
2025-01-16 06:50:07,3878.491163253784,5702.026605606079,35.09665812551595,64,10,
2025-01-16 06:51:13,3333.6360454559326,5664.083003997803,25.31703190400674,59,73,
2025-01-16 06:52:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70249 tokens (70249 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 06:53:19,5057.255983352661,7524.605751037598,32.42345331325374,80,6,
2025-01-16 06:54:27,12660.483121871948,15947.643995285034,28.291891873073233,93,30167,
2025-01-16 06:55:43,4646.820306777954,7122.633457183838,32.716523856705784,81,4842,
2025-01-16 06:56:50,6104.349613189697,8266.560792922974,24.974433813935452,54,13477,
2025-01-16 06:57:58,19931.2424659729,22994.882583618164,26.439136742424303,81,44132,
2025-01-16 06:59:21,5274.48582649231,6675.950288772583,34.24988737987754,48,173,
2025-01-16 07:00:28,5057.8272342681885,11758.753776550293,37.90520585433792,254,5198,
2025-01-16 07:01:39,3229.3198108673096,4733.614444732666,20.607665082434035,31,4,
2025-01-16 07:02:44,3279.0722846984863,4680.784702301025,36.38406805814661,51,40,
2025-01-16 07:03:49,3722.012996673584,5133.827447891235,33.998802008720986,48,10,
2025-01-16 07:04:54,8179.622173309326,10934.196949005127,35.577179049439806,98,510,
2025-01-16 07:06:05,21906.64267539978,25619.053602218628,24.243005899436003,90,42291,
2025-01-16 07:07:31,39656.95071220398,45261.756896972656,32.6505491835402,183,58954,
2025-01-16 07:09:16,3658.0770015716553,4844.451665878296,33.71616168436758,40,10,
2025-01-16 07:10:21,3974.911689758301,5494.658708572388,35.53222959577715,54,6,
2025-01-16 07:11:26,3786.733627319336,5001.608371734619,39.510245990916786,48,67,
2025-01-16 07:12:31,3678.99489402771,4921.5357303619385,37.825718580534094,47,7,
2025-01-16 07:13:36,4598.2465744018555,6024.5866775512695,37.158038172644765,53,9,
2025-01-16 07:14:42,3805.737018585205,5285.344362258911,38.52373418103963,57,6,
2025-01-16 07:15:47,8055.523633956909,10104.798316955566,37.57426988134536,77,14193,
2025-01-16 07:16:58,7851.0987758636475,10140.435934066772,38.87588146686751,89,2867,
2025-01-16 07:18:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 68681 tokens (68681 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:19:09,5845.874309539795,11263.682126998901,42.08344180560645,228,1754,
2025-01-16 07:20:20,4389.033555984497,6799.715995788574,26.13367856328691,63,5478,
2025-01-16 07:21:27,3735.2850437164307,5307.670593261719,36.250651131005114,57,437,
2025-01-16 07:22:32,3624.375343322754,5908.959627151489,36.33046090157824,83,81,
2025-01-16 07:23:38,5847.450494766235,12229.220151901245,39.17408703720488,250,6604,
2025-01-16 07:24:50,3267.221450805664,5430.310964584351,24.039689374279085,52,16,
2025-01-16 07:25:56,3464.6682739257812,5044.271945953369,32.28657979411767,51,2549,
2025-01-16 07:27:01,4026.1924266815186,6206.5722942352295,27.059505033035997,59,166,
2025-01-16 07:28:07,3723.6831188201904,5041.46146774292,43.25461868955195,57,9,
2025-01-16 07:29:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 432362 tokens (432362 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:30:14,3677.3462295532227,8364.797830581665,46.720482394303694,219,550,
2025-01-16 07:31:22,3972.31125831604,5262.104272842407,40.31654646470173,52,126,
2025-01-16 07:32:28,3539.4349098205566,4902.669906616211,36.677462152546894,50,152,
2025-01-16 07:33:33,6475.678443908691,8824.533462524414,37.4650624676963,88,9071,
2025-01-16 07:34:41,4370.30291557312,6408.951759338379,43.16584500029413,88,701,
2025-01-16 07:35:48,2764.60862159729,3862.2148036956787,40.99831135605451,45,13,
2025-01-16 07:36:52,3368.393659591675,6239.397764205933,42.84215400539318,123,608,
2025-01-16 07:37:58,3340.1706218719482,4590.008497238159,42.40549998092404,53,1202,
2025-01-16 07:39:03,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 208678 tokens (208678 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:40:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 323449 tokens (323449 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:41:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 259244 tokens (259244 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:42:07,3567.3556327819824,6205.820083618164,38.658849456044095,102,260,
2025-01-16 07:43:14,3345.1433181762695,4237.612724304199,31.3736244713204,28,4,
2025-01-16 07:44:18,4424.762010574341,5947.947263717651,38.73461870658547,59,409,
2025-01-16 07:45:24,5631.130695343018,8728.04307937622,43.914707016309926,136,5,
2025-01-16 07:46:33,4172.516584396362,5693.833827972412,37.46753035285017,57,370,
2025-01-16 07:47:38,4219.162464141846,6170.211553573608,26.652327858723865,52,238,
2025-01-16 07:48:44,3637.195110321045,4860.500335693359,29.42846907977799,36,4,
2025-01-16 07:49:49,3952.6267051696777,5600.717544555664,30.338133557388154,50,519,
2025-01-16 07:50:55,3316.2989616394043,4954.357624053955,29.302979863521166,48,79,
2025-01-16 07:52:00,3137.474536895752,4496.758222579956,27.22021928879093,37,4,
2025-01-16 07:53:04,2763.810396194458,3546.949625015259,12.769121545676288,10,6,
2025-01-16 07:54:08,5911.344528198242,8541.378498077393,34.60031354811034,91,2326,
2025-01-16 07:55:16,3523.8466262817383,4883.420705795288,33.0986010090019,45,13,
2025-01-16 07:56:21,3820.284843444824,5555.800437927246,27.08128935828818,47,98,
2025-01-16 07:57:27,3852.701425552368,6867.470741271973,30.184730727325626,91,477,
2025-01-16 07:58:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 313591 tokens (313591 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 07:59:35,3942.568302154541,5693.602085113525,34.26547253623457,60,7,
2025-01-16 08:00:41,22873.966455459595,25694.724798202515,29.07019674725584,82,32526,
2025-01-16 08:02:07,12203.190088272095,15409.867525100708,28.690132329302042,92,27148,
2025-01-16 08:03:22,3823.2076168060303,4889.846086502075,34.68841697650725,37,4,
2025-01-16 08:04:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110778 tokens (110778 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 08:05:28,5004.905223846436,6903.2251834869385,29.499768843290823,56,747,
2025-01-16 08:06:35,4393.218040466309,5816.067218780518,32.32949823571658,46,312,
2025-01-16 08:07:41,2856.0588359832764,3775.470733642578,31.541902028709956,29,4,
2025-01-16 08:08:45,3824.662685394287,5326.757192611694,30.623905339494154,46,58,
2025-01-16 08:09:50,3742.6204681396484,5198.303461074829,24.73065919895859,36,7,
2025-01-16 08:10:55,3182.4026107788086,4385.602235794067,28.2579875301813,34,5,
2025-01-16 08:12:00,5452.065467834473,7248.620510101318,29.50090520640325,53,859,
2025-01-16 08:13:07,3681.3504695892334,4815.046548843384,29.990400974455454,34,6,
2025-01-16 08:14:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 105887 tokens (105887 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 08:15:13,3498.0359077453613,4511.861562728882,33.53633815920023,34,4,
2025-01-16 08:16:17,3671.1161136627197,5038.577318191528,35.10154426394828,48,45,
2025-01-16 08:17:23,3666.5396690368652,5899.638414382935,29.107535049878315,65,1037,
2025-01-16 08:18:28,3228.9552688598633,4353.331089019775,30.23899962128714,34,4,
2025-01-16 08:19:33,5131.1094760894775,7491.787910461426,36.430205295148575,86,905,
2025-01-16 08:20:40,4984.460353851318,5883.441925048828,35.595835359977,32,5,
2025-01-16 08:21:46,4577.116250991821,7146.08359336853,34.25500922039196,88,2743,
2025-01-16 08:22:53,3840.460777282715,5341.290235519409,33.31491111504726,50,40,
2025-01-16 08:23:59,4180.433988571167,5759.777069091797,32.29190707771225,51,88,
2025-01-16 08:25:04,3775.9220600128174,5296.057224273682,32.23397573585194,49,431,
2025-01-16 08:26:10,3349.89857673645,4619.989633560181,24.407698828716605,31,4,
2025-01-16 08:27:14,2865.56339263916,3889.299154281616,29.304436871355115,30,4,
2025-01-16 08:28:18,1364389.2889022827,1367580.6577205658,17.86067460252536,57,60394,
2025-01-16 08:52:06,10773.403644561768,12496.596097946167,29.596229892856446,51,11186,
2025-01-16 08:53:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110913 tokens (110913 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 08:54:19,5962.202787399292,8506.800174713135,37.726989927212266,96,1180,
2025-01-16 08:55:28,3829.848051071167,5425.480127334595,31.96225543386498,51,70,
2025-01-16 08:56:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 30 column 1 (char 29), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-16 09:27:35,3559.4491958618164,4612.851619720459,36.07358321884711,38,9,
2025-01-16 09:28:40,6910.909652709961,9490.572214126587,35.27593157378971,91,7381,
2025-01-16 09:29:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 287984 tokens (287984 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 09:30:51,4787.734270095825,10208.895206451416,36.70800449133668,199,474,
2025-01-16 09:32:01,3382.784605026245,4932.5268268585205,34.19923601057832,53,117,
2025-01-16 09:33:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 325863 tokens (325863 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 09:34:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 399407 tokens (399407 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 09:35:10,4350.656509399414,6930.688142776489,36.04606966708765,93,478,
2025-01-16 09:36:17,593717.883348465,598217.8256511688,43.1116638725417,194,37968,
2025-01-16 09:47:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 195335 tokens (195335 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 09:48:17,3978.3542156219482,5197.870969772339,43.45983752959908,53,3096,
2025-01-16 09:49:22,6964.212417602539,10971.437454223633,46.66570963473493,187,4197,
2025-01-16 09:50:33,4732.741117477417,7096.6644287109375,35.534147660724194,84,5213,
2025-01-16 09:51:40,3250.081777572632,4891.674280166626,35.331545379471564,58,85,
2025-01-16 09:52:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 154847 tokens (154847 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 09:53:46,5048.912525177002,6379.269123077393,38.33558617327847,51,3307,
2025-01-16 09:54:53,3735.0025177001953,4789.764642715454,45.50789117433052,48,148,
2025-01-16 09:55:58,3300.2851009368896,4156.50200843811,36.20577884927577,31,5,
2025-01-16 09:57:02,6420.413017272949,8781.834840774536,46.15863159864066,109,2947,
2025-01-16 09:58:11,5161.748647689819,6512.663841247559,36.27170693887506,49,318,
2025-01-16 09:59:17,3173.6321449279785,9850.553512573242,51.07144164560677,341,2152,
2025-01-16 10:00:27,4532.456874847412,8644.091129302979,44.02142525295377,181,455,
2025-01-16 10:01:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 503943 tokens (503943 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:02:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 189523 tokens (189523 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:03:40,3254.3046474456787,5417.98734664917,43.444447762420936,94,685,
2025-01-16 10:04:45,26772.642850875854,29538.866758346558,29.643298135969296,82,49914,
2025-01-16 10:06:15,3798.6605167388916,4987.388134002686,40.37930918984293,48,423,
2025-01-16 10:07:20,16595.564603805542,18087.15558052063,34.191679083710106,51,12769,
2025-01-16 10:08:38,6544.719934463501,11495.816230773926,43.22275051678425,214,1997,
2025-01-16 10:09:49,2058.868646621704,2471.625566482544,24.227334585623616,10,4,
2025-01-16 10:10:52,3725.3143787384033,5213.1242752075195,39.655603945113775,59,221,
2025-01-16 10:11:57,3528.3560752868652,4857.051134109497,42.146615679915364,56,878,
2025-01-16 10:13:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 226180 tokens (226180 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:14:04,2990.0665283203125,3836.642026901245,36.61811622467645,31,4,
2025-01-16 10:15:08,6268.661022186279,7675.504684448242,39.80541797370834,56,438,
2025-01-16 10:16:16,741728.1081676483,744600.0626087189,33.426714096556076,96,15623,
2025-01-16 10:29:41,8166.21994972229,9729.186534881592,37.74872768248373,59,1322,
2025-01-16 10:30:50,3580.437421798706,4436.649322509766,36.20599056641864,31,6,
2025-01-16 10:31:55,3684.054374694824,5228.696823120117,37.54914288360319,58,739,
2025-01-16 10:33:00,3941.6706562042236,5283.945798873901,42.46521312063603,57,15,
2025-01-16 10:34:05,3254.7953128814697,4670.604228973389,44.49753019913164,63,169,
2025-01-16 10:35:10,3690.683364868164,5364.201307296753,42.42559831594376,71,22,
2025-01-16 10:36:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 201417 tokens (201417 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:37:17,2930.0198554992676,4461.292743682861,30.040367301588883,46,27,
2025-01-16 10:38:21,3429.797410964966,4802.190065383911,42.99061191418275,59,170,
2025-01-16 10:39:26,3463.0303382873535,4877.519607543945,43.832075186109485,62,40,
2025-01-16 10:40:31,20220.88599205017,21807.1870803833,34.041456818732115,54,14503,
2025-01-16 10:41:53,5764.230966567993,9875.43773651123,44.75571536445501,184,4645,
2025-01-16 10:43:03,3196.8159675598145,4118.703365325928,30.372472894030945,28,4,
2025-01-16 10:44:07,3562.4752044677734,5658.141136169434,41.5142502838498,87,306,
2025-01-16 10:45:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 96626 tokens (96626 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:46:14,3711.4717960357666,5863.899707794189,45.99457173881475,99,395,
2025-01-16 10:47:20,4394.199848175049,5748.978614807129,41.335162152868335,56,673,
2025-01-16 10:48:25,39319.13876533508,45865.42320251465,35.13443422863159,230,59785,
2025-01-16 10:50:11,3374.7451305389404,4940.551996231079,38.957550472251874,61,200,
2025-01-16 10:51:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 506796 tokens (506796 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:52:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 330152 tokens (330152 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 10:53:20,4239.299297332764,5857.656717300415,40.1641807909771,65,12,
2025-01-16 10:54:26,4814.0997886657715,6028.892755508423,42.80564789171636,52,3222,
2025-01-16 10:55:32,3787.0383262634277,7642.508268356323,40.2026218147249,155,677,
2025-01-16 10:56:39,3349.3857383728027,5335.937738418579,46.31139783800275,92,1206,
2025-01-16 10:57:45,9945.251941680908,14445.838212966919,46.660587608288196,210,14469,
2025-01-16 10:58:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 589829 tokens (589829 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:00:01,7111.588478088379,11215.251684188843,41.913771028847236,172,30,
2025-01-16 11:01:13,9582.388401031494,12236.112117767334,33.53777917373881,89,23902,
2025-01-16 11:02:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 518747 tokens (518747 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:03:27,2655.789852142334,3752.209424972534,35.57032450572627,39,11,
2025-01-16 11:04:31,4746.946096420288,6646.445751190186,26.849175714210944,51,6710,
2025-01-16 11:05:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 468892 tokens (468892 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:06:40,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 286263 tokens (286263 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:07:41,5029.376745223999,6273.57029914856,40.99040686967929,51,2568,
2025-01-16 11:08:48,2824.767589569092,3665.372371673584,36.87821037894894,31,4,
2025-01-16 11:09:51,4925.384998321533,6253.915548324585,39.8937006001693,53,39,
2025-01-16 11:10:58,5498.1255531311035,8328.057765960693,45.58413074884773,129,597,
2025-01-16 11:12:06,6012.018442153931,7926.280736923218,37.09000600074879,71,7008,
2025-01-16 11:13:14,3148.6518383026123,4530.739068984985,42.68905658789,59,2340,
2025-01-16 11:14:18,6975.451707839966,9877.98261642456,43.41039043799306,126,11920,
2025-01-16 11:15:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 313578 tokens (313578 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:16:30,7017.024993896484,10790.645360946655,45.57957167653617,172,6888,
2025-01-16 11:17:41,5189.888477325439,7351.1011600494385,44.41950612607053,96,361,
2025-01-16 11:18:48,3922.6319789886475,6101.62878036499,41.762337577788415,91,51,
2025-01-16 11:19:54,3927.8595447540283,5326.383829116821,40.75724721932219,57,1434,
2025-01-16 11:21:00,3150.5370140075684,4397.608280181885,38.4901819983803,48,14,
2025-01-16 11:22:04,16001.691818237305,18480.10230064392,35.50662839133458,88,35169,
2025-01-16 11:23:23,3449.674606323242,4935.511827468872,41.727316503887245,62,13,
2025-01-16 11:24:28,5385.215997695923,6904.584169387817,34.224752741855404,52,16321,
2025-01-16 11:25:34,5585.736989974976,7821.250200271606,38.469913576841996,86,12447,
2025-01-16 11:26:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 204453 tokens (204453 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:27:44,25919.809341430664,29062.293767929077,28.6397600704372,90,52658,
2025-01-16 11:29:13,7346.827983856201,12201.91240310669,44.28347304271817,215,14785,
2025-01-16 11:30:25,3898.120164871216,5848.543167114258,24.610046100152957,48,141,
2025-01-16 11:31:31,8622.112274169922,14075.088262557983,46.58006940446554,254,1218,
2025-01-16 11:32:45,9677.189350128174,11408.556699752808,34.654690706168275,60,2321,
2025-01-16 11:33:56,3155.2278995513916,4246.729373931885,41.22761265672205,45,15,
2025-01-16 11:35:01,3254.213571548462,4074.129343032837,36.58912420441434,30,105,
2025-01-16 11:36:05,2631.6990852355957,3423.6130714416504,39.145665488895474,31,4,
2025-01-16 11:37:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 214776 tokens (214776 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:38:10,6621.068239212036,9518.419027328491,37.96571697537188,110,4892,
2025-01-16 11:39:19,3518.282890319824,4726.797342300415,43.028033231029276,52,8,
2025-01-16 11:40:24,4940.674304962158,6344.82479095459,41.30611396612985,58,2077,
2025-01-16 11:41:30,13818.683624267578,16380.79023361206,37.46916683711366,96,32986,
2025-01-16 11:42:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 222952 tokens (222952 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:43:48,5823.935747146606,7780.236721038818,35.2706464500292,69,294,
2025-01-16 11:44:56,3664.3118858337402,5018.959283828735,42.81556963520207,58,174,
2025-01-16 11:46:01,12384.498357772827,14959.022760391235,36.123176733308405,93,31943,
2025-01-16 11:47:16,2548.900842666626,3018.8064575195312,21.280869357414048,10,4,
2025-01-16 11:48:19,2677.043676376343,2899.184226989746,45.0165445812874,10,4,
2025-01-16 11:49:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 581466 tokens (581466 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 11:50:24,4227.4720668792725,8534.95168685913,45.03793798585799,194,136,
2025-01-16 11:51:33,7330.937623977661,9691.658020019531,38.12395578523385,90,15810,
2025-01-16 11:52:43,6486.310720443726,9485.804796218872,40.34013635073791,121,1737,
2025-01-16 11:53:52,7055.354595184326,8446.895599365234,36.65001595121498,51,5966,
2025-01-16 11:55:00,6163.462400436401,9261.515855789185,41.96183244526895,130,13725,
2025-01-16 11:56:10,3110.2216243743896,3806.3926696777344,35.91071500123461,25,4,
2025-01-16 11:57:14,4520.063400268555,5988.573789596558,40.17676717084628,59,2319,
2025-01-16 11:58:20,9523.794174194336,14386.357069015503,45.24363072697923,220,15991,
2025-01-16 11:59:34,3678.424119949341,4965.548515319824,38.0693584677928,49,64,
2025-01-16 12:00:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 520946 tokens (520946 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:01:42,3319.652318954468,4545.078992843628,39.17003034352229,48,52,
2025-01-16 12:02:46,4825.382232666016,8213.210105895996,46.93272679417689,159,578,
2025-01-16 12:03:54,5354.578971862793,6822.70359992981,34.738195262856095,51,7402,
2025-01-16 12:05:01,5017.37117767334,6985.644340515137,40.13670535747164,79,1060,
2025-01-16 12:06:08,3803.7517070770264,6241.272926330566,43.89705375888682,107,6,
2025-01-16 12:07:14,11857.55968093872,14998.367547988892,29.928605626005474,94,22392,
2025-01-16 12:08:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 139329 tokens (139329 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:09:31,15829.643487930298,18539.026498794556,31.741543980733503,86,37693,
2025-01-16 12:10:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 576341 tokens (576341 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:11:52,3830.028772354126,6045.916318893433,41.51835238375805,92,285,
2025-01-16 12:12:58,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 740535 tokens (740535 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:14:00,8228.079795837402,10008.015871047974,34.270893685203575,61,5178,
2025-01-16 12:15:11,3416.6009426116943,4779.289484024048,37.426013685519976,51,3176,
2025-01-16 12:16:15,16020.591497421265,18119.373559951782,27.158608327000216,57,36116,
2025-01-16 12:17:33,3516.1185264587402,4777.994871139526,40.41600448013894,51,103,
2025-01-16 12:18:38,3548.7725734710693,5225.683689117432,29.220391911537327,49,65,
2025-01-16 12:19:43,3490.2701377868652,5101.779460906982,33.508959101426804,54,328,
2025-01-16 12:20:49,6242.420196533203,9427.027463912964,31.715056683614566,101,6926,
2025-01-16 12:21:58,3751.628875732422,5161.095380783081,39.73134501552929,56,6,
2025-01-16 12:23:03,3268.55731010437,4493.739604949951,39.17784333150995,48,25,
2025-01-16 12:24:08,3606.818437576294,6175.093650817871,39.71536986149532,102,1515,
2025-01-16 12:25:14,5019.444227218628,8549.992084503174,45.31874555102645,160,178,
2025-01-16 12:26:22,2820.8255767822266,3635.469913482666,35.59835709097167,29,4,
2025-01-16 12:27:26,21803.138256072998,24335.862636566162,27.243390765861598,69,45955,
2025-01-16 12:28:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 676790 tokens (676790 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:29:53,6326.673746109009,7924.015045166016,31.928054467825966,51,13203,
2025-01-16 12:31:01,4847.444295883179,10421.002388000488,42.522208629203924,237,7,
2025-01-16 12:32:12,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 177245 tokens (177245 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:33:13,3422.9140281677246,5133.557558059692,28.059615671671434,48,7,
2025-01-16 12:34:18,5700.490951538086,8127.781629562378,41.610179165772415,101,114,
2025-01-16 12:35:26,4643.555641174316,6203.689098358154,37.817277572201796,59,3813,
2025-01-16 12:36:32,7228.026628494263,9328.426837921143,41.42067764492322,87,495,
2025-01-16 12:37:42,6052.165746688843,8250.589847564697,40.48354453744494,89,11167,
2025-01-16 12:38:50,19897.460222244263,22433.302879333496,26.815543862667642,68,43532,
2025-01-16 12:40:12,3550.121068954468,4575.112104415894,38.049113261213215,39,9,
2025-01-16 12:41:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 410137 tokens (410137 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:42:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 385699 tokens (385699 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:43:21,3868.5452938079834,5219.048738479614,40.72555328681373,55,17,
2025-01-16 12:44:27,8833.763360977173,11243.729591369629,38.58975234887637,93,9999,
2025-01-16 12:45:38,3194.1466331481934,4667.8972244262695,41.39099272360539,61,76,
2025-01-16 12:46:42,3579.281806945801,5026.488542556763,39.3862180139291,57,762,
2025-01-16 12:47:47,5154.827356338501,6627.525091171265,36.667402089901444,54,8341,
2025-01-16 12:48:54,7786.027193069458,9208.073377609253,37.9734502205887,54,4512,
2025-01-16 12:50:03,5792.100429534912,7285.563707351685,35.487983392185136,53,8094,
2025-01-16 12:51:11,3838.613510131836,5276.875734329224,42.412293790195754,61,217,
2025-01-16 12:52:16,2575.169324874878,3360.621690750122,12.731516810515702,10,4,
2025-01-16 12:53:19,5512.9570960998535,7826.15065574646,40.63646105532157,94,546,
2025-01-16 12:54:27,2899.559497833252,3766.951084136963,35.73933675342981,31,4,
2025-01-16 12:55:31,4024.43528175354,5425.333499908447,37.83286987815942,53,1861,
2025-01-16 12:56:36,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 647792 tokens (647792 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 12:57:39,3214.244604110718,5382.476329803467,26.288703059073875,57,7,
2025-01-16 12:58:45,3275.7058143615723,4695.820569992065,37.32092761508517,53,2972,
2025-01-16 12:59:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 559636 tokens (559636 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:00:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 343865 tokens (343865 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:01:56,7126.413822174072,8594.214916229248,38.15230839301644,56,8,
2025-01-16 13:03:05,8289.685726165771,12772.2909450531,41.93989673859018,188,4506,
2025-01-16 13:04:17,2641.110897064209,3949.061155319214,38.99230852099941,51,322,
2025-01-16 13:05:21,8473.59299659729,9949.736595153809,35.22692511138443,52,8277,
2025-01-16 13:06:31,3864.8765087127686,10945.13201713562,43.218779270885726,306,4121,
2025-01-16 13:07:42,3393.0819034576416,4847.0964431762695,40.57731087848497,59,108,
2025-01-16 13:08:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 154838 tokens (154838 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:09:48,4707.3469161987305,7010.1318359375,39.951625187139314,92,4121,
2025-01-16 13:10:55,3796.3011264801025,5381.104946136475,37.22858266002462,59,9,
2025-01-16 13:12:01,3370.988607406616,4867.180585861206,38.09671540872385,57,31,
2025-01-16 13:13:06,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 540923 tokens (540923 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:14:08,3112.8053665161133,5024.05309677124,36.10207034270193,69,72,
2025-01-16 13:15:13,7102.232217788696,8706.90631866455,33.02851337294709,53,14501,
2025-01-16 13:16:22,3544.8074340820312,4755.95498085022,35.50351905078839,43,43,
2025-01-16 13:17:27,3749.8626708984375,6178.220987319946,36.238474118464964,88,70,
2025-01-16 13:18:33,9194.129705429077,16221.544027328491,40.27085739317971,283,20159,
2025-01-16 13:19:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 180866 tokens (180866 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:20:50,7804.020166397095,9354.654788970947,32.88975962328676,51,14568,
2025-01-16 13:22:00,8212.371587753296,9708.08482170105,34.097445180311524,51,5981,
2025-01-16 13:23:10,4941.086769104004,6287.048101425171,35.66224292433571,48,51,
2025-01-16 13:24:16,4044.182300567627,5907.631397247314,35.95483242304889,67,1471,
2025-01-16 13:25:22,2986.5872859954834,4229.936599731445,38.60540193308322,48,89,
2025-01-16 13:26:26,3454.6189308166504,5095.3075885772705,33.52250881960117,55,6,
2025-01-16 13:27:31,3529.2224884033203,5028.107166290283,38.02827585131843,57,131,
2025-01-16 13:28:36,9637.881994247437,12275.049209594727,35.64430782126991,94,5047,
2025-01-16 13:29:48,10256.954908370972,12642.858743667603,29.338986326453156,70,25828,
2025-01-16 13:31:01,38248.61145019531,41772.12357521057,26.677927211501487,94,58025,
2025-01-16 13:32:43,21787.931203842163,25849.795818328857,32.00500566571157,130,41917,
2025-01-16 13:34:09,3418.175220489502,4575.2105712890625,34.57111312317148,40,6,
2025-01-16 13:35:13,3736.3765239715576,6077.969551086426,40.14361117047722,94,1015,
2025-01-16 13:36:19,3479.285955429077,4485.864162445068,37.75166175378592,38,6,
2025-01-16 13:37:24,3823.188543319702,5184.977293014526,39.653727505166536,54,777,
2025-01-16 13:38:29,3849.2069244384766,4696.189403533936,36.600520984928366,31,8,
2025-01-16 13:39:34,5709.644794464111,7634.772539138794,45.19180622722867,87,483,
2025-01-16 13:40:41,3171.5030670166016,4373.608350753784,39.92994677701981,48,99,
2025-01-16 13:41:46,2389.326572418213,2937.7033710479736,18.23563656410553,10,4,
2025-01-16 13:42:49,3178.968667984009,4427.492380142212,39.246351128805074,49,5,
2025-01-16 13:43:53,4170.703172683716,5539.416074752808,40.91435093169974,56,3198,
2025-01-16 13:44:59,2992.899179458618,3770.7228660583496,37.283513602901394,29,4,
2025-01-16 13:46:02,4044.6245670318604,5380.78236579895,40.414388218088696,54,9,
2025-01-16 13:47:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 358876 tokens (358876 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:48:10,21186.65909767151,24303.797006607056,28.23102556602977,88,45348,
2025-01-16 13:49:34,4072.0670223236084,5849.24054145813,36.57493165420044,65,202,
2025-01-16 13:50:40,6151.683568954468,13692.949295043945,41.63757271059916,314,1715,
2025-01-16 13:51:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 739915 tokens (739915 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:52:57,5547.333240509033,7770.860195159912,42.72491493808591,95,223,
2025-01-16 13:54:05,4539.045810699463,6576.358079910278,33.377308440959254,68,6502,
2025-01-16 13:55:11,20563.950538635254,23646.8346118927,30.490929196918337,94,42864,
2025-01-16 13:56:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 563650 tokens (563650 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 13:57:38,3716.9835567474365,5097.738027572632,34.763602808624796,48,62,
2025-01-16 13:58:43,3985.6784343719482,5450.50311088562,36.86448000625008,54,2246,
2025-01-16 13:59:48,7604.866027832031,11704.616785049438,39.026762716813444,160,4954,
2025-01-16 14:01:00,3845.0536727905273,5106.053113937378,39.651088151574534,50,6,
2025-01-16 14:02:05,5558.789491653442,8385.207414627075,39.2723238477127,111,1520,
2025-01-16 14:03:14,7977.356195449829,14150.269031524658,40.17552273712256,248,2284,
2025-01-16 14:04:28,6829.352855682373,11504.220962524414,38.93157963828509,182,6,
2025-01-16 14:05:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 190279 tokens (190279 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:06:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151064 tokens (151064 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:07:44,25904.378414154053,29629.273176193237,27.383323963805182,102,48928,
2025-01-16 14:09:13,3193.676710128784,4094.16127204895,35.53642266977256,32,9,
2025-01-16 14:10:17,3782.907724380493,5144.792795181274,35.24526483851993,48,108,
2025-01-16 14:11:23,5472.006797790527,7892.377138137817,28.92119393181622,70,7166,
2025-01-16 14:12:30,3633.5723400115967,5251.624345779419,36.46359930934509,59,14,
2025-01-16 14:13:36,3869.3645000457764,7748.038530349731,38.930830180685945,151,239,
2025-01-16 14:14:43,7132.035493850708,9196.441173553467,24.220045745659448,50,8006,
2025-01-16 14:15:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 647169 tokens (647169 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:16:55,4179.56018447876,6622.533798217773,36.431011575186005,89,478,
2025-01-16 14:18:02,7733.668327331543,12077.527046203613,40.516971520128145,176,5,
2025-01-16 14:19:14,7324.347257614136,8807.050228118896,35.745527630498415,53,56,
2025-01-16 14:20:23,3929.443836212158,5697.272062301636,36.20261236668402,64,315,
2025-01-16 14:21:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250413 tokens (250413 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:22:30,4418.895244598389,5697.846174240112,33.621305558646974,43,15,
2025-01-16 14:23:36,5553.499221801758,6859.969615936279,39.03647585813472,51,4741,
2025-01-16 14:24:43,3197.606325149536,4063.490629196167,39.26621586868375,34,4,
2025-01-16 14:25:47,5438.827991485596,8016.577959060669,35.69004021229636,92,2923,
2025-01-16 14:26:55,7461.315155029297,12143.982410430908,38.65318420633252,181,2622,
2025-01-16 14:28:07,3501.115560531616,5916.077136993408,36.439503161342444,88,157,
2025-01-16 14:29:13,27766.914129257202,30813.369512557983,28.557779141257935,87,45862,
2025-01-16 14:30:44,3058.3839416503906,3987.154483795166,33.37745825617256,31,4,
2025-01-16 14:31:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 735283 tokens (735283 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:32:51,4817.26336479187,6638.17572593689,28.55711296687642,52,6704,
2025-01-16 14:33:58,4538.693189620972,6261.949062347412,27.27395318586043,47,5,
2025-01-16 14:35:04,5685.76455116272,6882.622003555298,25.90116303159532,31,22,
2025-01-16 14:36:11,4386.489152908325,7668.8995361328125,41.7376208350333,137,260,
2025-01-16 14:37:19,6419.219255447388,10332.975387573242,45.2250968186559,177,5,
2025-01-16 14:38:29,4630.6891441345215,5921.366214752197,40.2889314327979,52,108,
2025-01-16 14:39:35,6656.286954879761,7973.426342010498,41.757159900754516,55,924,
2025-01-16 14:40:43,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 611984 tokens (611984 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:41:46,39269.66428756714,42145.62249183655,30.94620772578613,89,44705,
2025-01-16 14:43:28,329474.05433654785,331332.49139785767,23.675808514596177,44,38424,
2025-01-16 14:49:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 215381 tokens (215381 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 14:51:01,3862.549066543579,5056.89549446106,44.3757345114795,53,560,
2025-01-16 14:52:06,9972.959518432617,14696.391105651855,45.09433365698361,213,11190,
2025-01-16 14:53:21,3194.310426712036,4405.011892318726,42.950307303000166,52,63,
2025-01-16 14:54:25,3168.2541370391846,4289.009809494019,27.659909079112236,31,5,
2025-01-16 14:55:29,4000.303030014038,6267.308712005615,40.5821655987988,92,145,
2025-01-16 14:56:36,3419.497489929199,4709.8228931427,39.52491353962857,51,86,
2025-01-16 14:57:40,3555.534601211548,4800.559282302856,38.55345257728247,48,62,
2025-01-16 14:58:45,6241.702556610107,8300.639390945435,45.168942751962845,93,37,
2025-01-16 14:59:53,4688.485383987427,6581.580638885498,35.39177430541256,67,970,
2025-01-16 15:01:00,3813.8883113861084,4908.598184585571,38.36632977215082,42,105,
2025-01-16 15:02:05,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 98720 tokens (98720 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 15:03:06,22461.345911026,27358.59704017639,42.06441421266022,206,39386,
2025-01-16 15:04:34,7038.07806968689,9291.36872291565,42.60435725965524,96,6155,
2025-01-16 15:05:43,7469.820499420166,9508.56065750122,44.144909611586634,90,1059,
2025-01-16 15:06:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 214565 tokens (214565 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 15:07:54,4363.530158996582,8640.80286026001,48.86291209308802,209,104,
2025-01-16 15:09:02,3312.6842975616455,5297.080993652344,43.8420403397122,87,36,
2025-01-16 15:10:08,4069.316625595093,5307.961463928223,42.788698067254856,53,12,
2025-01-16 15:11:13,3490.664005279541,4859.594821929932,41.6383350471079,57,2970,
2025-01-16 15:12:18,3664.7608280181885,5776.254653930664,43.571048549120185,92,294,
2025-01-16 15:13:24,5208.9080810546875,6557.213306427002,38.566935009571715,52,3850,
2025-01-16 15:14:30,5459.452867507935,7086.359262466431,44.25577293390428,72,24,
2025-01-16 15:15:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 608596 tokens (608596 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 15:16:40,6458.942413330078,9036.40627861023,43.453567481081414,112,5765,
2025-01-16 15:17:49,319208.1322669983,322191.83683395386,33.51538255747464,100,58460,
2025-01-16 15:24:11,3609.989643096924,4820.702791213989,37.99413599459168,46,36,
2025-01-16 15:25:16,18351.885557174683,21250.26798248291,34.84702333207778,101,32357,
2025-01-16 15:26:37,3792.797565460205,5073.850631713867,39.81099717371229,51,75,
2025-01-16 15:27:42,5313.896417617798,9590.69561958313,49.80360076342125,213,3186,
2025-01-16 15:28:52,3079.1468620300293,4266.446352005005,39.585631423955725,47,7,
2025-01-16 15:29:56,3563.0064010620117,5003.085374832153,40.27556894894132,58,44,
2025-01-16 15:31:01,7745.159864425659,10957.398414611816,42.02676665846514,135,1816,
2025-01-16 15:32:12,3727.081537246704,5093.491077423096,27.810110280036927,38,8,
2025-01-16 15:33:17,4480.537176132202,6256.724119186401,39.41026606108994,70,885,
2025-01-16 15:34:24,9673.96330833435,12018.656969070435,39.237535180232406,92,2536,
2025-01-16 15:35:36,8195.544719696045,9418.242454528809,37.621726686433846,46,143,
2025-01-16 15:36:45,4143.27597618103,5434.957027435303,40.25761618900113,52,59,
2025-01-16 15:37:51,6497.902631759644,11771.37041091919,43.80419292839889,231,5,
2025-01-16 15:39:02,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 575529 tokens (575529 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 15:40:05,2766.742706298828,3157.63783454895,25.58230910875231,10,4,
2025-01-16 15:41:08,4682.031869888306,6322.778701782227,32.91184169934832,54,3491,
2025-01-16 15:42:14,8101.323843002319,9577.522993087769,34.54818409633137,51,3635,
2025-01-16 15:43:24,4112.685203552246,5386.200904846191,39.261392654364535,50,159,
2025-01-16 15:44:29,6156.536340713501,12325.006008148193,41.339264639044245,255,5481,
2025-01-16 15:45:41,4331.862449645996,5734.806060791016,44.19279542489836,62,8,
2025-01-16 15:46:47,3857.7470779418945,5398.317813873291,39.5957151315877,61,1206,
2025-01-16 15:47:53,4747.887849807739,6758.316516876221,47.253603948318556,95,259,
2025-01-16 15:48:59,4738.046884536743,6018.762588500977,39.040670654098854,50,8522,
2025-01-16 15:50:05,2944.648265838623,4051.093101501465,44.28598554635162,49,275,
2025-01-16 15:51:09,3493.0214881896973,4587.094306945801,43.87276530146611,48,56,
2025-01-16 15:52:14,5495.97954750061,6787.574529647827,41.80877190326917,54,6611,
2025-01-16 15:53:21,2669.761896133423,3519.8206901550293,42.35001184998619,36,4,
2025-01-16 15:54:24,3699.1806030273438,4809.340476989746,45.03855811464263,50,42,
2025-01-16 15:55:29,3416.85152053833,4141.297101974487,41.41097795161829,30,4,
2025-01-16 15:56:33,3498.891592025757,4751.672029495239,43.104121348730295,54,152,
2025-01-16 15:57:38,3806.3199520111084,5196.59948348999,41.71822909476605,58,58,
2025-01-16 15:58:43,5001.54709815979,6956.1779499053955,45.532894316344965,89,242,
2025-01-16 15:59:50,6100.418329238892,8736.312627792358,46.284101781680505,122,11155,
2025-01-16 16:00:59,3311.8772506713867,4913.742780685425,30.58933417436767,49,1592,
2025-01-16 16:02:04,3197.2146034240723,4324.578523635864,42.5772007950924,48,33,
2025-01-16 16:03:08,4749.106168746948,6779.385089874268,45.80651408642978,93,3174,
2025-01-16 16:04:15,3138.7243270874023,4252.358436584473,35.91844005035411,40,24,
2025-01-16 16:05:19,5924.567937850952,10222.616910934448,47.23073219297551,203,5,
2025-01-16 16:06:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 263535 tokens (263535 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:07:33,3182.102680206299,4002.4781227111816,39.006530841895035,32,18,
2025-01-16 16:08:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 72226 tokens (72226 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:09:37,5356.253385543823,6639.078617095947,40.53552948680611,52,1200,
2025-01-16 16:10:44,4646.4221477508545,6093.057870864868,41.47554152115406,60,218,
2025-01-16 16:11:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146750 tokens (146750 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:12:51,3514.0020847320557,5335.5865478515625,30.74246686541159,56,5,
2025-01-16 16:13:57,7969.267129898071,11667.30546951294,43.536595679742824,161,13834,
2025-01-16 16:15:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 633527 tokens (633527 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:16:11,7606.7445278167725,10183.863639831543,42.68332010234595,110,4942,
2025-01-16 16:17:21,13567.885637283325,15317.160844802856,29.15493215748361,51,34178,
2025-01-16 16:18:37,3784.9225997924805,8355.54838180542,44.414049559445054,203,2799,
2025-01-16 16:19:45,4238.487243652344,5597.574234008789,40.46834410913995,55,1515,
2025-01-16 16:20:51,3490.62180519104,6354.789257049561,42.24613331231199,121,32,
2025-01-16 16:21:57,3539.724111557007,6265.107154846191,46.96587524281378,128,225,
2025-01-16 16:23:03,3123.283624649048,3898.346185684204,37.416334445658485,29,5,
2025-01-16 16:24:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 104876 tokens (104876 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:25:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 170608 tokens (170608 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:26:09,3335.022449493408,4371.243715286255,46.32215298464622,48,121,
2025-01-16 16:27:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 163880 tokens (163880 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:28:15,9076.300621032715,17430.08065223694,43.573088905900754,364,20063,
2025-01-16 16:29:32,2990.5571937561035,5939.437627792358,34.58939834342092,102,20,
2025-01-16 16:30:38,4421.907663345337,6595.637798309326,40.94344489615055,89,127,
2025-01-16 16:31:45,3513.0162239074707,4954.174041748047,41.633191908089366,60,20,
2025-01-16 16:32:50,4330.774784088135,5792.8197383880615,40.35443631639293,59,9,
2025-01-16 16:33:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 327679 tokens (327679 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:34:58,3524.6455669403076,5038.962125778198,42.26329008058564,64,7,
2025-01-16 16:36:03,25559.032201766968,28471.757650375366,31.9288589470155,93,50633,
2025-01-16 16:37:31,3375.4427433013916,5261.622190475464,38.17240194609927,72,143,
2025-01-16 16:38:36,7991.360187530518,9338.212966918945,40.093468882708954,54,1871,
2025-01-16 16:39:46,3745.1705932617188,4872.322797775269,42.585198172695385,48,97,
2025-01-16 16:40:51,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 713640 tokens (713640 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:41:55,16139.886379241943,21459.16438102722,39.66703750568842,211,30219,
2025-01-16 16:43:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 510792 tokens (510792 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:44:19,4259.530067443848,5669.915437698364,35.45130363269229,50,733,
2025-01-16 16:45:25,3150.3732204437256,3868.1671619415283,36.22209452722107,26,10,
2025-01-16 16:46:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150868 tokens (150868 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:47:30,4302.359580993652,5203.507900238037,35.510247665813864,32,87,
2025-01-16 16:48:35,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 414562 tokens (414562 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:49:38,9690.626859664917,14509.153366088867,46.48724038383275,224,9623,
2025-01-16 16:50:52,4649.894714355469,6007.835388183594,39.7660965907814,54,2608,
2025-01-16 16:51:58,3493.527412414551,4729.589462280273,47.73223157074465,59,942,
2025-01-16 16:53:03,4139.317989349365,5738.4278774261475,44.39970043921765,71,212,
2025-01-16 16:54:09,5646.061897277832,6905.9906005859375,41.27217664258881,52,2484,
2025-01-16 16:55:16,2958.132266998291,3698.8141536712646,40.50321810184298,30,5,
2025-01-16 16:56:19,2881.913185119629,3709.777593612671,42.277454666411316,35,11,
2025-01-16 16:57:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247832 tokens (247832 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 16:58:24,4553.982257843018,5827.027797698975,40.061410533491646,51,106,
2025-01-16 16:59:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 376977 tokens (376977 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:00:32,6971.167087554932,9215.988397598267,37.86492921272167,85,13521,
2025-01-16 17:01:41,6406.399726867676,10583.27031135559,43.0944642308251,180,5,
2025-01-16 17:02:52,5473.0212688446045,7641.794919967651,42.42028666862493,92,8816,
2025-01-16 17:04:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71849 tokens (71849 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:05:00,3418.659448623657,4333.423852920532,32.795329441201,30,11,
2025-01-16 17:06:05,9582.198143005371,12202.21209526062,33.58760739585054,88,25883,
2025-01-16 17:07:17,2782.733678817749,3594.078779220581,38.20815579536813,31,8,
2025-01-16 17:08:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 180343 tokens (180343 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:09:22,3670.1714992523193,5357.753753662109,38.516641088249,65,6,
2025-01-16 17:10:27,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88211 tokens (88211 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:11:28,3967.1413898468018,5348.18696975708,40.548987531344274,56,15,
2025-01-16 17:12:33,4174.912214279175,6148.179531097412,29.899648920924506,59,201,
2025-01-16 17:13:40,4586.89546585083,6429.26287651062,27.681774929863657,51,8772,
2025-01-16 17:14:46,7576.769828796387,10007.049798965454,37.855720793189036,92,19546,
2025-01-16 17:15:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199973 tokens (199973 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:16:57,3241.6954040527344,4393.103837966919,27.792049334932166,32,4,
2025-01-16 17:18:02,4002.559185028076,6320.4522132873535,38.82836649609728,90,1911,
2025-01-16 17:19:08,3086.1363410949707,4258.443355560303,40.94490556459899,48,83,
2025-01-16 17:20:12,4296.472787857056,5499.841690063477,39.88801764113253,48,9,
2025-01-16 17:21:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80266 tokens (80266 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:22:19,3657.6991081237793,4700.187683105469,40.288211312759664,42,8,
2025-01-16 17:23:24,3582.7786922454834,4687.267541885376,41.6482248915395,46,6,
2025-01-16 17:24:28,3307.9257011413574,5820.815324783325,31.83585910313674,80,68,
2025-01-16 17:25:34,3607.3334217071533,5914.419412612915,41.61093274304458,96,2529,
2025-01-16 17:26:40,3940.715551376343,6048.662900924683,31.310079928771238,66,17,
2025-01-16 17:27:46,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 249587 tokens (249587 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:28:49,3629.855155944824,4629.128217697144,30.02182401213956,30,4,
2025-01-16 17:29:53,26316.04838371277,29425.700664520264,34.08741249117235,106,31248,
2025-01-16 17:31:23,7771.863698959351,10651.854991912842,37.84733664531959,109,7446,
2025-01-16 17:32:34,3133.282423019409,4253.381013870239,27.676135166327022,31,4,
2025-01-16 17:33:38,7496.889352798462,11875.254392623901,41.33963211235952,181,158,
2025-01-16 17:34:50,3047.553777694702,3964.4906520843506,32.71762848447911,30,4,
2025-01-16 17:35:54,4362.255573272705,5928.328275680542,30.649918056933114,48,343,
2025-01-16 17:37:00,3731.3292026519775,5527.386665344238,28.395528015875303,51,141,
2025-01-16 17:38:05,3184.072494506836,4078.155279159546,35.79086920058506,32,4,
2025-01-16 17:39:09,9622.640609741211,17588.524341583252,42.556483550583934,339,1258,
2025-01-16 17:40:27,3463.7725353240967,4871.018171310425,34.81979175984885,49,42,
2025-01-16 17:41:32,4587.942123413086,6111.719369888306,36.094514554030276,55,289,
2025-01-16 17:42:38,8391.767024993896,10741.505861282349,38.727708200856796,91,8314,
2025-01-16 17:43:49,3643.126964569092,4982.240200042725,35.84461621949604,48,52,
2025-01-16 17:44:54,10108.848810195923,12488.834142684937,31.512799249718153,75,18107,
2025-01-16 17:46:06,28390.11025428772,31849.92504119873,29.192314103661815,101,53751,
2025-01-16 17:47:38,3598.782539367676,5812.065839767456,41.1153872545656,91,167,
2025-01-16 17:48:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 359953 tokens (359953 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:49:46,8328.750848770142,12780.698299407959,39.30864008175305,175,15868,
2025-01-16 17:50:59,15163.236856460571,17880.518198013306,33.48935519058175,91,36570,
2025-01-16 17:52:17,7463.895559310913,12283.828496932983,41.494353259336144,200,14067,
2025-01-16 17:53:29,4585.149765014648,6046.7283725738525,34.89377836828673,51,2102,
2025-01-16 17:54:35,4523.62322807312,7043.292999267578,38.49710827542961,97,12,
2025-01-16 17:55:42,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 301096 tokens (301096 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:56:44,3828.5653591156006,5480.475902557373,38.74301804906175,64,7,
2025-01-16 17:57:50,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 240597 tokens (240597 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 17:58:51,4914.1130447387695,6784.332513809204,23.526650603134595,44,53,
2025-01-16 17:59:58,4518.238067626953,7118.257522583008,35.768963121690135,93,6521,
2025-01-16 18:01:05,6250.557422637939,12879.703521728516,44.50045233434662,295,982,
2025-01-16 18:02:18,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 378575 tokens (378575 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:03:20,3344.8894023895264,4775.618314743042,37.74299906414225,54,1342,
2025-01-16 18:04:25,3345.2513217926025,4795.071125030518,21.381967559531883,31,7,
2025-01-16 18:05:30,3212.8560543060303,4159.753322601318,36.962827090008375,35,5,
2025-01-16 18:06:34,4285.48264503479,5654.9177169799805,36.51140607124832,50,1212,
2025-01-16 18:07:40,3585.017442703247,5015.080213546753,41.95620026148202,60,33,
2025-01-16 18:08:45,5433.505535125732,6780.394077301025,39.349952383143986,53,744,
2025-01-16 18:09:52,3543.8601970672607,4639.863729476929,31.934203645356117,35,6,
2025-01-16 18:10:56,39680.59754371643,43545.961141586304,31.821068545216008,123,60719,
2025-01-16 18:12:40,5190.616130828857,10200.41799545288,44.91195581781473,225,5632,
2025-01-16 18:13:50,3526.150941848755,4955.264568328857,39.8848621578052,57,9,
2025-01-16 18:14:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 498206 tokens (498206 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:15:57,41525.322675704956,48006.24084472656,35.488798655009404,230,64102,
2025-01-16 18:17:45,4693.244218826294,7534.656047821045,34.13795881687349,97,641,
2025-01-16 18:18:53,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 163412 tokens (163412 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:19:54,3827.160120010376,4981.795787811279,33.77688831861452,39,38,
2025-01-16 18:20:59,5859.835624694824,9129.374265670776,38.53754729211251,126,1212,
2025-01-16 18:22:08,3226.112127304077,4105.299234390259,36.39725803766049,32,4,
2025-01-16 18:23:12,6112.557888031006,7493.9124584198,36.920281796762445,51,2772,
2025-01-16 18:24:20,5030.504941940308,6787.8289222717285,38.12615132433588,67,2918,
2025-01-16 18:25:27,4668.106555938721,6809.8204135894775,42.95625191542379,92,922,
2025-01-16 18:26:33,5755.692481994629,11373.63600730896,48.950296271376885,275,583,
2025-01-16 18:27:45,21162.867307662964,24613.50417137146,32.45777646959653,112,45190,
2025-01-16 18:29:09,4051.166296005249,5816.818714141846,30.58359586820014,54,5976,
2025-01-16 18:30:15,5597.616672515869,7909.080266952515,41.53212719034725,96,2311,
2025-01-16 18:31:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 96156 tokens (96156 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:32:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128235 tokens (128235 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:33:27,4291.6765213012695,5801.7871379852295,39.73218871327024,60,317,
2025-01-16 18:34:33,3613.377332687378,4977.175712585449,43.2615266813923,59,8,
2025-01-16 18:35:38,4128.2782554626465,5405.131101608276,38.375604634405434,49,2254,
2025-01-16 18:36:43,4003.192901611328,5206.2530517578125,40.72946809353944,49,396,
2025-01-16 18:37:48,5443.051338195801,6881.090402603149,39.6373098692497,57,129,
2025-01-16 18:38:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 215448 tokens (215448 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:39:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 184148 tokens (184148 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:40:58,3991.7972087860107,5356.914520263672,35.161813271595506,48,160,
2025-01-16 18:42:03,36157.41300582886,40749.890089035034,36.58156523292067,168,60357,
2025-01-16 18:43:44,6448.023557662964,13897.69721031189,38.927879732944135,290,13650,
2025-01-16 18:44:58,4407.681226730347,7417.9651737213135,38.53457083872497,116,16,
2025-01-16 18:46:05,3790.0679111480713,5183.7732791900635,38.74563536758437,54,126,
2025-01-16 18:47:11,3333.2481384277344,4604.402542114258,37.76095166786455,48,82,
2025-01-16 18:48:15,4373.8250732421875,6285.584211349487,35.04625591398091,67,7610,
2025-01-16 18:49:21,11184.28373336792,16211.340188980103,33.419159200499045,168,20932,
2025-01-16 18:50:38,4542.954921722412,5894.767045974731,36.24764057143049,49,209,
2025-01-16 18:51:44,3892.8439617156982,6891.568660736084,40.35048633825169,121,6,
2025-01-16 18:52:50,3806.110382080078,5347.956418991089,42.1572572383578,65,2494,
2025-01-16 18:53:56,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 170277 tokens (170277 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:54:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 661237 tokens (661237 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:56:00,9142.70544052124,11151.809930801392,33.34819086022622,67,21447,
2025-01-16 18:57:11,3245.237112045288,4496.4845180511475,36.76331297807669,46,29,
2025-01-16 18:58:16,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 705780 tokens (705780 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 18:59:20,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 185405 tokens (185405 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:00:21,4992.271661758423,6715.7933712005615,33.071820150411384,57,2109,
2025-01-16 19:01:28,3520.8537578582764,5159.424781799316,30.51439288834826,50,140,
2025-01-16 19:02:33,10479.341745376587,12437.188386917114,32.68897504129421,64,9705,
2025-01-16 19:03:45,3028.532028198242,3835.7951641082764,33.4463433283903,27,6,
2025-01-16 19:04:49,3215.4712677001953,4734.862565994263,38.17318163209232,58,102,
2025-01-16 19:05:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 245482 tokens (245482 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:06:56,3571.8369483947754,4958.660840988159,38.93789275509171,54,167,
2025-01-16 19:08:01,5046.308755874634,8517.086744308472,37.16745940820325,129,4442,
2025-01-16 19:09:09,6611.840009689331,7891.9477462768555,38.278028168646834,49,156,
2025-01-16 19:10:17,3947.399139404297,5011.052370071411,35.725929188563384,38,21,
2025-01-16 19:11:22,10645.885229110718,16512.792348861694,32.214588733428286,189,13403,
2025-01-16 19:12:39,3714.5540714263916,5179.745674133301,34.1252297021264,50,29,
2025-01-16 19:13:44,5495.222330093384,6757.0343017578125,39.625555251347066,50,507,
2025-01-16 19:14:51,3459.0353965759277,4686.175346374512,39.11534296302427,48,57,
2025-01-16 19:15:55,3473.701000213623,5089.559555053711,38.98856110350249,63,195,
2025-01-16 19:17:00,8404.667139053345,11138.426065444946,38.77445043770325,106,3890,
2025-01-16 19:18:11,13072.171926498413,16077.658891677856,30.61068008807921,92,22649,
2025-01-16 19:19:28,29163.487434387207,33388.4437084198,26.745838931995603,113,55268,
2025-01-16 19:21:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 412930 tokens (412930 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:22:04,3676.26690864563,6183.178663253784,35.50184797546304,89,203,
2025-01-16 19:23:10,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109457 tokens (109457 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:24:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 134078 tokens (134078 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:25:12,27998.262405395508,116897.24087715149,46.04102398432377,4093,52596,
2025-01-16 19:28:09,3627.7897357940674,5781.092643737793,25.077753715368704,54,98,
2025-01-16 19:29:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140368 tokens (140368 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:30:16,3427.04701423645,5091.529369354248,30.039369204644665,50,57,
2025-01-16 19:31:21,5010.648012161255,9567.07501411438,48.283446636080505,220,1583,
2025-01-16 19:32:31,13458.6660861969,15712.577104568481,23.95835485955174,54,32604,
2025-01-16 19:33:47,10202.407121658325,12435.780048370361,35.37250723116068,79,6,
2025-01-16 19:34:59,7874.3391036987305,10597.798109054565,34.88211124646164,95,3139,
2025-01-16 19:36:10,3248.5411167144775,4876.1186599731445,35.0213728593705,57,2327,
2025-01-16 19:37:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 216469 tokens (216469 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:38:17,8060.855865478516,11189.908981323242,30.9997933588333,97,19846,
2025-01-16 19:39:28,4444.275617599487,6996.08588218689,18.810175923389444,48,113,
2025-01-16 19:40:35,4482.8596115112305,6244.11940574646,32.93097372110549,58,8,
2025-01-16 19:41:41,3417.956590652466,4452.495336532593,35.76473104303358,37,26,
2025-01-16 19:42:46,9177.410125732422,14290.122985839844,36.57549428584006,187,392,
2025-01-16 19:44:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 126717 tokens (126717 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:45:01,4731.196641921997,6263.589382171631,31.976136869467076,49,8297,
2025-01-16 19:46:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 84705 tokens (84705 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:47:08,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69544 tokens (69544 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:48:09,3781.0513973236084,6398.689031600952,37.8203608870909,99,525,
2025-01-16 19:49:16,4876.832485198975,7404.368162155151,37.58601742643063,95,629,
2025-01-16 19:50:23,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 230334 tokens (230334 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:51:25,7390.128135681152,10199.554920196533,32.03500461234649,90,21374,
2025-01-16 19:52:35,18731.622219085693,22904.70600128174,39.778736460604705,166,393,
2025-01-16 19:53:58,3819.430351257324,5387.21776008606,34.44344539055993,54,2869,
2025-01-16 19:55:03,5478.264808654785,8162.048816680908,35.77038976046599,96,4738,
2025-01-16 19:56:11,12328.134298324585,14276.327848434448,34.90412951842763,68,25625,
2025-01-16 19:57:26,3712.453842163086,5189.988613128662,33.163348141024294,49,174,
2025-01-16 19:58:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 113130 tokens (113130 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 19:59:32,7280.198574066162,10036.673545837402,34.10152494132679,94,16944,
2025-01-16 20:00:42,4110.577583312988,5434.954404830933,38.50867756923288,51,141,
2025-01-16 20:01:47,4911.057472229004,7521.56925201416,36.0082650183391,94,8223,
2025-01-16 20:02:55,6257.296085357666,9426.367044448853,38.81263675940959,123,582,
2025-01-16 20:04:04,4317.313671112061,7422.816038131714,35.09899111898187,109,5290,
2025-01-16 20:05:12,3340.435743331909,4292.409181594849,32.563933775889296,31,4,
2025-01-16 20:06:16,4015.460968017578,6750.398874282837,33.63878930824875,92,1870,
2025-01-16 20:07:23,3893.170118331909,5881.479501724243,25.64993175910424,51,73,
2025-01-16 20:08:29,3771.0869312286377,5239.503860473633,32.688263832997215,48,147,
2025-01-16 20:09:34,3595.301389694214,4677.090406417847,28.656234737794204,31,6,
2025-01-16 20:10:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 149452 tokens (149452 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:11:40,5116.76287651062,10386.074542999268,34.91917192338207,184,3366,
2025-01-16 20:12:50,2940.535068511963,4429.953813552856,34.24154568337982,51,5,
2025-01-16 20:13:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 168833 tokens (168833 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:14:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 152511 tokens (152511 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:15:58,3700.990915298462,5435.20712852478,24.79506284859558,43,7,
2025-01-16 20:17:03,3673.1650829315186,4479.837894439697,38.42945932693765,31,5,
2025-01-16 20:18:08,3936.3949298858643,4958.0371379852295,36.21620143203928,37,13,
2025-01-16 20:19:13,6173.363447189331,9340.776681900024,35.360084618144214,112,10308,
2025-01-16 20:20:22,4762.333631515503,6496.226072311401,32.874011477802874,57,931,
2025-01-16 20:21:29,23717.3593044281,26029.221773147583,23.357790842078103,54,47202,
2025-01-16 20:22:55,3432.469367980957,4955.775499343872,37.418611286624056,57,10,
2025-01-16 20:24:00,9792.777299880981,14918.941497802734,42.91708019988753,220,1770,
2025-01-16 20:25:14,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 75524 tokens (75524 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:26:15,3458.6987495422363,4823.508977890015,38.10053509267038,52,244,
2025-01-16 20:27:20,5711.552381515503,7893.863201141357,33.45078040373527,73,6605,
2025-01-16 20:28:28,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 329633 tokens (329633 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:29:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 170899 tokens (170899 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:30:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74594 tokens (74594 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:31:34,5771.100282669067,7293.771743774414,35.46398640767852,54,1651,
2025-01-16 20:32:41,3918.0941581726074,5581.754446029663,32.45854961745638,54,371,
2025-01-16 20:33:46,3341.20512008667,4782.101392745972,31.230561737069742,45,80,
2025-01-16 20:34:51,3879.0853023529053,5489.715099334717,33.52725753689121,54,5334,
2025-01-16 20:35:57,23423.341274261475,31832.146406173706,34.011966684137114,286,47163,
2025-01-16 20:37:29,5203.317642211914,7738.29984664917,35.89768789726128,91,8557,
2025-01-16 20:38:36,3817.185163497925,6171.4348793029785,25.06106281075816,59,133,
2025-01-16 20:39:42,3242.112874984741,4204.158782958984,33.26244593398004,32,32,
2025-01-16 20:40:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73064 tokens (73064 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:41:48,3352.983236312866,5068.643569946289,36.720555208371984,63,88,
2025-01-16 20:42:53,3936.436891555786,6652.95147895813,29.817620113520512,81,115,
2025-01-16 20:43:59,3035.517454147339,4587.583065032959,32.859435607814945,51,115,
2025-01-16 20:45:04,7539.05725479126,9228.811979293823,34.324508260850855,58,17499,
2025-01-16 20:46:13,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 527706 tokens (527706 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:47:15,4197.211742401123,6126.365661621094,31.101717391352707,60,4,
2025-01-16 20:48:21,3843.968152999878,7347.972869873047,24.828733700345946,87,62,
2025-01-16 20:49:29,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176772 tokens (176772 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:50:30,3402.567148208618,5086.512565612793,32.66139117785852,55,9,
2025-01-16 20:51:35,17191.37406349182,19766.56222343445,33.78393911299215,87,34897,
2025-01-16 20:52:55,12761.943817138672,15377.856492996216,27.14157879017307,71,28344,
2025-01-16 20:54:10,3201.037883758545,4060.4817867279053,36.06983526544974,31,43,
2025-01-16 20:55:14,5211.370468139648,6695.980072021484,35.69962087098179,53,8205,
2025-01-16 20:56:21,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 203126 tokens (203126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 20:57:23,3757.8139305114746,7490.490674972534,35.8991708025195,134,780,
2025-01-16 20:58:30,4320.398330688477,9930.98521232605,42.59803921443644,239,945,
2025-01-16 20:59:40,40266.80612564087,44220.141649246216,23.27148795002814,92,61516,
2025-01-16 21:01:24,37324.04184341431,40626.57570838928,23.0126330591241,76,61454,
2025-01-16 21:03:05,29057.331323623657,31681.22887611389,19.817846908942354,52,53738,
2025-01-16 21:04:37,3364.060163497925,4303.909778594971,31.920000304412845,30,5,
2025-01-16 21:05:41,4087.9061222076416,6680.286169052124,35.102877801720375,91,1310,
2025-01-16 21:06:48,3430.6845664978027,5176.1534214019775,26.926862583623855,47,93,
2025-01-16 21:07:53,3270.3804969787598,4335.733890533447,32.852948337845,35,4,
2025-01-16 21:08:57,3793.2400703430176,5170.224666595459,34.85877774568816,48,71,
2025-01-16 21:10:02,4426.292419433594,6181.124925613403,27.35303787168487,48,66,
2025-01-16 21:11:08,3728.686809539795,5626.9097328186035,26.86723428242426,51,88,
2025-01-16 21:12:14,27884.473085403442,30530.93719482422,21.538172309646555,57,53811,
2025-01-16 21:13:45,3865.4003143310547,5567.4426555633545,34.66423752847609,59,19,
2025-01-16 21:14:50,3790.19832611084,5470.514297485352,35.11244373386366,59,7,
2025-01-16 21:15:56,3315.53316116333,4546.334266662598,20.31197395606733,25,6,
2025-01-16 21:17:00,3948.176622390747,5432.15799331665,35.040871144868575,52,49,
2025-01-16 21:18:06,3484.7588539123535,5126.202821731567,32.897863745993845,54,10,
2025-01-16 21:19:11,2817.6889419555664,4137.043476104736,19.706606016075106,26,6,
2025-01-16 21:20:15,21709.656476974487,30336.23433113098,37.67426730443372,325,43428,
2025-01-16 21:21:45,3075.4966735839844,4069.49520111084,32.19320664349318,32,5,
2025-01-16 21:22:49,19661.861896514893,22955.868005752563,26.108026866988084,86,38389,
2025-01-16 21:24:12,4500.319004058838,5633.85272026062,27.348105801276084,31,4,
2025-01-16 21:25:18,5178.211688995361,7735.474348068237,24.635717327073618,63,4071,
2025-01-16 21:26:26,4396.568298339844,6168.458700180054,28.218449599406448,50,31,
2025-01-16 21:27:32,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80560 tokens (80560 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:28:33,4608.416795730591,6252.794027328491,31.01478117064505,51,75,
2025-01-16 21:29:39,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 695126 tokens (695126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:30:42,4884.606838226318,6327.42714881897,33.268175979781965,48,67,
2025-01-16 21:31:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 616431 tokens (616431 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:32:52,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128243 tokens (128243 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:33:54,4584.089040756226,10928.736209869385,37.984775760775115,241,1827,
2025-01-16 21:35:05,8097.3052978515625,11060.950994491577,38.128714281910256,113,5,
2025-01-16 21:36:16,3895.841598510742,4948.990106582642,29.435544714158777,31,4,
2025-01-16 21:37:21,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 412779 tokens (412779 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:38:24,4092.217206954956,5288.933038711548,33.4248105845531,40,10,
2025-01-16 21:39:29,6795.799016952515,8503.321409225464,29.867836715225693,51,10279,
2025-01-16 21:40:37,4184.9205493927,6093.653678894043,27.24320084158911,52,2369,
2025-01-16 21:41:43,6999.356508255005,8371.196746826172,34.98949706417283,48,6231,
2025-01-16 21:42:52,5195.915222167969,10010.209083557129,37.59637554566987,181,568,
2025-01-16 21:44:02,3578.1612396240234,5538.1762981414795,27.04060857577742,53,4,
2025-01-16 21:45:07,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 85556 tokens (85556 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:46:09,8106.4722537994385,10670.219421386719,28.864000684451558,74,21346,
2025-01-16 21:47:19,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 124659 tokens (124659 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:48:21,9539.746522903442,12119.027853012085,36.056555333606326,93,5230,
2025-01-16 21:49:33,4963.618278503418,6695.42932510376,28.871510028852896,50,8157,
2025-01-16 21:50:39,6332.916021347046,10077.87013053894,37.383635664953424,140,11399,
2025-01-16 21:51:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 98539 tokens (98539 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:52:51,3573.0597972869873,5246.191740036011,36.45857116311732,61,9,
2025-01-16 21:53:56,3359.5616817474365,4993.961811065674,25.697501637814725,42,30,
2025-01-16 21:55:01,5664.778709411621,10495.198488235474,37.263842117636365,180,5866,
2025-01-16 21:56:11,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 190254 tokens (190254 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 21:57:13,8654.372930526733,13602.521419525146,35.56885982530918,176,6102,
2025-01-16 21:58:27,3571.737289428711,4944.7619915008545,34.23099375347615,47,8,
2025-01-16 21:59:31,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 325619 tokens (325619 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:00:37,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120490 tokens (120490 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:01:38,3294.5892810821533,5397.234916687012,22.828383055707846,48,30,
2025-01-16 22:02:43,18302.050828933716,21233.57105255127,31.724154331514782,93,40952,
2025-01-16 22:04:05,3602.6742458343506,6296.719074249268,40.088419784589654,108,93,
2025-01-16 22:05:11,13111.757040023804,16285.355806350708,30.879770007417903,98,32305,
2025-01-16 22:06:27,5921.708583831787,8186.490058898926,37.53121479302113,85,3612,
2025-01-16 22:07:36,3110.2306842803955,4484.515428543091,36.382562062729605,50,55,
2025-01-16 22:08:40,3636.8796825408936,6498.9471435546875,34.240981854874484,98,526,
2025-01-16 22:09:47,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 592678 tokens (592678 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:10:49,4374.992370605469,5547.787427902222,45.191186363082856,53,222,
2025-01-16 22:11:54,7651.262521743774,8835.869789123535,43.05224305503982,51,2196,
2025-01-16 22:13:03,4840.177059173584,7349.7161865234375,35.86315870477884,90,4115,
2025-01-16 22:14:11,5512.391567230225,7884.190082550049,37.94588765389457,90,251,
2025-01-16 22:15:18,3772.796869277954,6086.001873016357,41.50086129195339,96,94,
2025-01-16 22:16:25,8436.875104904175,9170.83740234375,47.686373158536036,35,19,
2025-01-16 22:17:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 139905 tokens (139905 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:18:35,3400.2320766448975,5024.59192276001,35.09074675560567,57,3928,
2025-01-16 22:19:40,3642.383575439453,5174.363136291504,37.859512934847366,58,12,
2025-01-16 22:20:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 660353 tokens (660353 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:21:48,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 259323 tokens (259323 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:22:50,8838.675498962402,11554.708242416382,37.18659145160315,101,21287,
2025-01-16 22:24:01,4288.883686065674,5707.124948501587,38.07532711835771,54,821,
2025-01-16 22:25:07,4958.961725234985,8613.940477371216,41.860708468024846,153,7,
2025-01-16 22:26:15,4049.6413707733154,5934.268474578857,38.20384406793984,72,626,
2025-01-16 22:27:21,4296.644687652588,6851.905345916748,35.61280517730748,91,6008,
2025-01-16 22:28:28,4049.983024597168,7128.891706466675,41.57317193385504,128,79,
2025-01-16 22:29:35,36798.34723472595,40181.19478225708,23.648715727194286,80,61968,
2025-01-16 22:31:16,5799.8411655426025,8429.22830581665,26.622173253918117,70,12216,
2025-01-16 22:32:24,7032.187461853027,9169.575691223145,38.3645791968103,82,1075,
2025-01-16 22:33:33,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 319087 tokens (319087 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:34:36,2839.900493621826,3580.289125442505,36.467334639653636,27,4,
2025-01-16 22:35:39,3457.103967666626,5005.483627319336,32.93765820421519,51,23,
2025-01-16 22:36:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144502 tokens (144502 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:37:45,3066.138744354248,4051.919937133789,32.461564730984314,32,5,
2025-01-16 22:38:49,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67477 tokens (67477 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:39:51,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74293 tokens (74293 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:40:51,3875.7481575012207,5561.009883880615,35.009398882365424,59,3432,
2025-01-16 22:41:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 81332 tokens (81332 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:42:58,3517.446994781494,5373.145818710327,29.099549616393425,54,6890,
2025-01-16 22:44:03,10637.43257522583,13265.13957977295,31.586474388648558,83,24736,
2025-01-16 22:45:16,3074.646234512329,4578.936338424683,37.22686192932823,56,6,
2025-01-16 22:46:21,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 85950 tokens (85950 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:47:22,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 321928 tokens (321928 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:48:25,3376.3201236724854,4087.543725967407,26.714524010019147,19,4,
2025-01-16 22:49:29,4271.154880523682,5627.440452575684,36.12808468194873,49,6,
2025-01-16 22:50:34,3534.4150066375732,8395.200729370117,39.49978685587175,192,1552,
2025-01-16 22:51:43,3421.1227893829346,5748.631954193115,39.09759040945451,91,154,
2025-01-16 22:52:48,3477.7607917785645,6025.169372558594,36.50768891244095,93,2148,
2025-01-16 22:53:54,3692.340850830078,4936.9988441467285,39.36824433949867,49,59,
2025-01-16 22:54:59,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 308459 tokens (308459 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 22:56:02,3822.028160095215,4862.879276275635,33.62632700864889,35,15,
2025-01-16 22:57:07,3246.689796447754,4910.685300827026,36.65875288692868,61,5,
2025-01-16 22:58:11,3707.0510387420654,5196.382284164429,38.943653521182675,58,7,
2025-01-16 22:59:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 104616 tokens (104616 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:00:19,3674.9496459960938,4832.928657531738,37.1336609486349,43,8,
2025-01-16 23:01:23,3969.815969467163,5967.747449874878,38.53986022798276,77,208,
2025-01-16 23:02:29,3329.6635150909424,4279.165267944336,32.648702234451285,31,5,
2025-01-16 23:03:34,3447.5841522216797,8567.686557769775,42.967890595627544,220,3301,
2025-01-16 23:04:42,3078.2487392425537,4553.682565689087,26.432903530433787,39,32,
2025-01-16 23:05:47,3798.628330230713,5383.632659912109,30.283828946794443,48,406,
2025-01-16 23:06:52,5792.185068130493,7346.720218658447,38.59674705948122,60,603,
2025-01-16 23:07:59,3528.2814502716064,4817.8417682647705,44.20111196404088,57,173,
2025-01-16 23:09:04,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 126771 tokens (126771 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:10:06,5089.6289348602295,9668.509244918823,39.74770853918019,182,824,
2025-01-16 23:11:15,3429.4421672821045,4468.936920166016,38.48023271789148,40,9,
2025-01-16 23:12:20,4095.125198364258,5440.460681915283,37.9087600257038,51,56,
2025-01-16 23:13:25,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 126258 tokens (126258 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:14:26,4693.137407302856,7131.599903106689,38.95897524094726,95,3410,
2025-01-16 23:15:34,8805.351972579956,10360.856771469116,23.143612302391382,36,22588,
2025-01-16 23:16:44,3557.0037364959717,4745.77522277832,36.17179625873609,43,16,
2025-01-16 23:17:49,4138.684511184692,5720.379590988159,37.301753513282115,59,5334,
2025-01-16 23:18:54,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199784 tokens (199784 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:19:56,17216.65120124817,20091.980934143066,29.56182695416346,85,40464,
2025-01-16 23:21:16,24867.588758468628,27918.33734512329,29.500956058358987,90,48344,
2025-01-16 23:22:44,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70574 tokens (70574 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:23:45,4408.602952957153,5734.885454177856,40.7153075987195,54,152,
2025-01-16 23:24:51,2659.062623977661,3648.8125324249268,31.321043564058783,31,4,
2025-01-16 23:25:54,8014.958620071411,9495.75400352478,34.4409501608944,51,6649,
2025-01-16 23:27:04,3887.4387741088867,5440.843820571899,41.199814656018546,64,107,
2025-01-16 23:28:09,3853.341817855835,5258.187294006348,39.86203532750691,56,9,
2025-01-16 23:29:15,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 639080 tokens (639080 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:30:18,3542.248249053955,4786.465644836426,35.36359493859111,44,4,
2025-01-16 23:31:22,7483.021020889282,12366.811990737915,46.685044754703455,228,15255,
2025-01-16 23:32:35,2962.221384048462,4738.483905792236,38.8456093372178,69,384,
2025-01-16 23:33:39,3810.4727268218994,5186.765432357788,33.42312272307578,46,7,
2025-01-16 23:34:45,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106859 tokens (106859 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:35:46,3143.704414367676,4046.442747116089,34.33996195289459,31,4,
2025-01-16 23:36:50,3689.1937255859375,5174.527645111084,34.33571355881002,51,83,
2025-01-16 23:37:55,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 363224 tokens (363224 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:38:57,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 188551 tokens (188551 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-16 23:39:58,3301.650285720825,4135.77938079834,33.56794549577244,28,11,
2025-01-16 23:41:03,4100.189208984375,5238.718509674072,33.37639178629869,38,16,
2025-01-16 23:42:08,3197.783946990967,4651.84473991394,38.512832663225865,56,8,
2025-01-16 23:43:13,15441.399335861206,18464.41674232483,30.43316912542133,92,39615,
2025-01-16 23:44:31,2689.9731159210205,3628.6449432373047,34.09072166519561,32,52,
2025-01-16 23:45:35,3432.236671447754,4959.189414978027,37.98414865538378,58,760,
2025-01-16 23:46:40,3572.1471309661865,7451.355695724487,40.472172964946544,157,955,
2025-01-16 23:47:47,24331.296920776367,27569.52452659607,27.484170612359154,89,50529,
2025-01-16 23:49:15,3386.7948055267334,4843.763113021851,30.199696021972297,44,4,
2025-01-16 23:50:19,10197.614908218384,12167.505979537964,34.51967521962954,68,22217,
2025-01-16 23:51:32,3741.9373989105225,4925.896883010864,40.54192786544033,48,156,
2025-01-16 23:52:37,3620.4922199249268,5904.761075973511,42.902130255159264,98,2475,
2025-01-16 23:53:42,2821.7430114746094,3696.373224258423,32.013529364461704,28,5,
2025-01-16 23:54:46,4648.921728134155,6031.254768371582,39.06439217478667,54,11410,
2025-01-16 23:55:52,3509.371519088745,4948.292255401611,41.69791878442575,60,16,
2025-01-16 23:56:57,2244.9426651000977,2997.462749481201,35.879441041371884,27,4,
2025-01-16 23:58:00,20242.765426635742,22574.028968811035,23.592356250155962,55,40134,
2025-01-16 23:59:23,3481.703519821167,4351.200819015503,40.25314400910792,35,12,
2025-01-17 00:00:27,2997.4727630615234,3952.568292617798,40.83361171014895,39,18,
2025-01-17 00:01:31,5507.457256317139,6708.982229232788,29.961924064417516,36,13486,
2025-01-17 00:02:38,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156675 tokens (156675 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:03:39,2982.0034503936768,3542.5055027008057,17.841147875976855,10,5,
2025-01-17 00:04:42,4329.079389572144,6185.443639755249,39.32417896584656,73,2002,
2025-01-17 00:05:48,4550.750255584717,5965.6736850738525,35.337601284935054,50,589,
2025-01-17 00:06:54,7080.384254455566,13233.291387557983,42.58149754779323,262,4421,
2025-01-17 00:08:08,3182.119131088257,4384.682416915894,39.91473926211301,48,8,
2025-01-17 00:09:12,3021.7716693878174,4183.112621307373,38.74831067105699,45,64,
2025-01-17 00:10:16,2901.2796878814697,3778.6190509796143,36.4739134546506,32,4,
2025-01-17 00:11:20,4358.574151992798,5947.328567504883,38.39485788641448,61,1633,
2025-01-17 00:12:26,5466.386795043945,8137.911081314087,38.18045021121918,102,5816,
2025-01-17 00:13:34,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 260636 tokens (260636 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:14:36,3928.7726879119873,6324.127674102783,40.495041678249905,97,5,
2025-01-17 00:15:42,3466.679096221924,5381.574869155884,37.0777360332335,71,18,
2025-01-17 00:16:48,4170.1459884643555,8876.49393081665,42.49579556160851,200,3993,
2025-01-17 00:17:56,4404.158115386963,6791.970729827881,37.27260651097551,89,82,
2025-01-17 00:19:03,4038.339138031006,5755.143642425537,36.6960826574825,63,331,
2025-01-17 00:20:09,37223.82688522339,40493.38388442993,20.492072784251683,67,60471,
2025-01-17 00:21:50,7814.189434051514,10080.830097198486,30.00034416817959,68,18657,
2025-01-17 00:23:00,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78744 tokens (78744 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:24:01,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 177368 tokens (177368 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:25:02,5585.721015930176,6971.732139587402,35.35325161799939,49,6,
2025-01-17 00:26:09,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 508624 tokens (508624 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:27:12,3273.258924484253,4879.467487335205,36.73246511354509,59,196,
2025-01-17 00:28:17,3622.6866245269775,5192.882537841797,37.57492902617859,59,993,
2025-01-17 00:29:22,4041.062116622925,5680.48882484436,37.81809805164265,62,112,
2025-01-17 00:30:28,3446.392774581909,4992.265462875366,36.22549283914211,56,6,
2025-01-17 00:31:33,5024.856328964233,8819.820404052734,40.053080079936,152,4221,
2025-01-17 00:32:42,2738.3718490600586,5889.381885528564,40.62189536643177,128,4,
2025-01-17 00:33:48,3387.310266494751,4936.663866043091,38.080396894033356,59,1479,
2025-01-17 00:34:53,4243.261337280273,5786.419868469238,33.04909960268682,51,252,
2025-01-17 00:35:59,15098.266124725342,18549.628257751465,25.497179550626406,88,36908,
2025-01-17 00:37:17,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87191 tokens (87191 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:38:18,3725.799798965454,6314.073801040649,37.86307010827553,98,19,
2025-01-17 00:39:25,3522.592544555664,5187.608957290649,31.831518052690704,53,1215,
2025-01-17 00:40:30,,,,,,"litellm.BadRequestError: litellm.ContextWindowExceededError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 612024 tokens (612024 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
2025-01-17 00:41:32,3786.550521850586,5214.705467224121,35.71041095030547,51,170,
2025-01-17 00:42:37,4004.086971282959,5339.199781417847,33.70501702807691,45,67,
2025-01-17 00:43:43,3661.054849624634,4877.1820068359375,39.46955687600021,48,138,
2025-01-17 00:44:48,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:47:13,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:49:38,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:52:04,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:54:29,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:56:55,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 00:59:20,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:01:46,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:04:11,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:06:37,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:09:02,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:11:28,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:13:53,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:16:19,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:18:44,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:21:09,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:23:35,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:26:00,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:28:25,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:30:51,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:33:16,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:35:42,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:38:07,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:40:32,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:42:58,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:45:23,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:47:49,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:50:14,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:52:40,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:55:05,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.
2025-01-17 01:56:43,2968.0745601654053,4305.97996711731,38.1192868606393,51,11,
2025-01-17 01:57:47,5459.113597869873,6696.01583480835,73.57089128178715,91,565,
2025-01-17 01:58:54,3660.56227684021,4971.581697463989,38.13826035941568,50,861,
2025-01-28 05:34:03,6162.8124713897705,8888.204336166382,17.612146209269746,48,43,
2025-01-28 05:35:12,58540.15636444092,66988.42334747314,14.440831503671554,122,30055,
2025-01-28 05:37:19,168258.16798210144,170165.23361206055,19.401534702711515,37,10,
2025-01-28 05:41:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 05:47:09,313265.29002189636,313266.50500297546,0.0,0,2293,
2025-01-28 05:53:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 05:59:23,307096.2471961975,307096.65060043335,0.0,0,236,
2025-01-28 06:05:30,209074.68700408936,211789.30974006653,17.682015023248383,48,73,
2025-01-28 06:10:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 06:16:11,278136.6012096405,281946.0458755493,18.112876298608416,69,78,
2025-01-28 06:21:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 06:27:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 06:33:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 5 column 1 (char 4), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>"
2025-01-28 06:39:54,194567.2206878662,199209.12170410156,20.250324095931575,94,1870,
2025-01-28 18:44:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:46:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 18:48:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:50:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:52:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 231803 tokens (231803 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 18:53:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:55:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:57:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 18:59:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:01:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:03:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 708626 tokens (708626 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 19:04:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:06:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 147022 tokens (147022 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 19:07:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:10:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:12:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:14:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:16:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:18:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:20:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:22:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 133256 tokens (133256 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 19:23:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:25:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:27:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:29:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:31:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:33:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 19:35:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:37:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:39:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:41:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:43:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:45:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:47:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:49:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:51:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:53:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:55:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 19:57:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 262679 tokens (262679 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 19:58:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:00:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:02:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:04:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:06:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:09:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:11:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:13:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:15:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:17:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:19:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:21:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:23:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:25:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:27:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:29:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:31:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:33:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:35:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:37:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 92887 tokens (92887 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 20:38:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 121715 tokens (121715 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 20:39:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 190459 tokens (190459 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 20:40:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 395513 tokens (395513 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 20:41:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:43:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 20:45:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:47:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:49:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 20:51:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:53:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:55:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 234419 tokens (234419 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 20:56:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 20:58:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:00:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 570041 tokens (570041 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:01:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:03:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 76741 tokens (76741 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:04:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 21:06:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 21:08:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 21:10:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 523704 tokens (523704 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:11:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:13:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:15:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:17:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73528 tokens (73528 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:18:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:20:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:22:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:24:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 169258 tokens (169258 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:26:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:28:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:30:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:32:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 463918 tokens (463918 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:33:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:35:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:37:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:39:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 68082 tokens (68082 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:40:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:42:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 161244 tokens (161244 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 21:43:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:45:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:47:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:49:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:51:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:53:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:55:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:57:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 21:59:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 22:01:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:03:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74631 tokens (74631 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 22:04:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:06:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 22:08:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:10:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:12:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:14:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:16:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:18:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:20:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:23:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:25:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:27:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:29:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:31:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:33:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:35:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:37:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:39:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:41:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67195 tokens (67195 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 22:42:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 485015 tokens (485015 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 22:43:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 100771 tokens (100771 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-28 22:44:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:46:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 22:48:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:50:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 22:52:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:54:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:56:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 22:58:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 339493 tokens (339493 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 22:59:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:01:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 23:03:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:05:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 617126 tokens (617126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 23:06:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:08:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:10:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156921 tokens (156921 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-28 23:11:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:13:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 23:15:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:17:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:19:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 617869 tokens (617869 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 23:20:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:22:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:24:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:26:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-28 23:28:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 139702 tokens (139702 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 23:29:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:31:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:33:37,97059.09061431885,97059.69834327698,0.0,0,40,,deepseek/deepseek-chat
2025-01-28 23:36:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:38:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:40:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:42:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:44:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:46:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 382834 tokens (382834 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 23:47:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:49:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:51:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:53:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:55:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 209580 tokens (209580 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-28 23:56:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-28 23:58:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:00:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86129 tokens (86129 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 00:01:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:03:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:05:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:07:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:09:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:11:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:13:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:15:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:17:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:19:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:21:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:23:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:25:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:27:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:29:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:31:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:33:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:35:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:37:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:39:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 104710 tokens (104710 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 00:40:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:42:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:44:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:46:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:48:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:50:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:53:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:55:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 00:57:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 164880 tokens (164880 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 00:58:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:00:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:02:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:04:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:06:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:08:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:10:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 01:12:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 01:14:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:16:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120964 tokens (120964 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:17:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 01:19:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:21:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156607 tokens (156607 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:22:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:24:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 326444 tokens (326444 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:25:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 207912 tokens (207912 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:26:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:28:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74965 tokens (74965 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:29:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:31:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:33:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 01:35:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 116274 tokens (116274 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:36:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 561353 tokens (561353 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:37:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 168311 tokens (168311 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:38:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:40:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:42:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:44:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:46:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103133 tokens (103133 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 01:47:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:49:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:51:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:53:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:56:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 01:58:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:00:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:02:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:04:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:06:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:08:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:10:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:12:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 02:14:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 100053 tokens (100053 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 02:15:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:17:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:19:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:21:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:23:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 02:25:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:27:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 02:29:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:31:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:33:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 85520 tokens (85520 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 02:34:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:36:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 247776 tokens (247776 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 02:37:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:39:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 661134 tokens (661134 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 02:40:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:42:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:44:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:46:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:48:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:50:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94407 tokens (94407 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 02:51:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:53:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 02:55:55,10978.777408599854,12255.1589012146,45.44095972528043,58,17,,deepseek/deepseek-chat
2025-01-29 02:57:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460812 tokens (460812 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 02:58:09,5610.725402832031,6502.925634384155,57.16205644923158,51,56,,deepseek/deepseek-chat
2025-01-29 02:59:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:01:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:03:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 03:05:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:07:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:09:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:11:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:13:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:15:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 122915 tokens (122915 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:16:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:18:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:20:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:22:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 03:24:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:26:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:28:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:30:47,6924.3223667144775,8634.525537490845,47.94751957030645,82,15512,,deepseek/deepseek-chat
2025-01-29 03:31:56,4529.838800430298,6281.564712524414,57.65742191896828,101,58,,deepseek/deepseek-chat
2025-01-29 03:33:02,9892.93384552002,10616.05429649353,26.275014037317007,19,4,,deepseek/deepseek-chat
2025-01-29 03:34:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 303463 tokens (303463 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:35:16,7381.9262981414795,10612.729549407959,55.09465794001035,178,5189,,deepseek/deepseek-chat
2025-01-29 03:36:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67149 tokens (67149 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:37:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:39:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 134922 tokens (134922 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:40:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:42:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:44:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:46:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:48:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:50:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 297177 tokens (297177 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:51:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:53:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:55:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 285880 tokens (285880 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 03:56:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 03:58:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:00:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 623543 tokens (623543 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:01:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 234270 tokens (234270 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:02:52,284218.1906700134,284218.7397480011,0.0,0,11,,deepseek/deepseek-reasoner
2025-01-29 04:08:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:10:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:12:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 351204 tokens (351204 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 04:13:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:15:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:17:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:19:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:21:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:23:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:25:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:27:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:29:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:31:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:33:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 697680 tokens (697680 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:34:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 316108 tokens (316108 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:35:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 408208 tokens (408208 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:36:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:38:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:40:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:42:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:44:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:46:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:48:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:50:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 590123 tokens (590123 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 04:51:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:53:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:55:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 04:57:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:00:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 133364 tokens (133364 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:01:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80373 tokens (80373 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:02:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:04:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:06:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:08:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:10:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:12:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:14:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:16:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 201403 tokens (201403 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:17:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 114235 tokens (114235 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:18:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:20:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460998 tokens (460998 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:21:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:23:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:25:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 224545 tokens (224545 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:26:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:28:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:30:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:32:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:34:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 72630 tokens (72630 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:35:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:37:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 729836 tokens (729836 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 05:38:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:40:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:42:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:44:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:46:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 05:52:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 278970 tokens (278970 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:53:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 408713 tokens (408713 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 05:54:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:56:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 05:58:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:00:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:02:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:04:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:06:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:08:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 181821 tokens (181821 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:09:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 229747 tokens (229747 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:10:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 06:12:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:14:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:16:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 06:18:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 06:20:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:22:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:24:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:26:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:28:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:30:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 651452 tokens (651452 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 06:31:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:33:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:35:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:37:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:39:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:41:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:43:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 270839 tokens (270839 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:44:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:47:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 284232 tokens (284232 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:48:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 142458 tokens (142458 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:49:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:51:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 592816 tokens (592816 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 06:52:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:54:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:56:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 06:58:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:00:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 496074 tokens (496074 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:01:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 464438 tokens (464438 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:02:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 711430 tokens (711430 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 07:03:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:05:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:07:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298978 tokens (298978 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:08:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:10:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:12:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:14:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:16:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:18:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:20:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 606819 tokens (606819 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:21:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173010 tokens (173010 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 07:22:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:24:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:26:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:28:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 07:30:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 07:32:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:34:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:36:45,89899.34778213501,89899.85299110413,0.0,0,76,,deepseek/deepseek-chat
2025-01-29 07:39:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:41:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87471 tokens (87471 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 07:42:20,151732.76734352112,151734.44414138794,0.0,0,63306,,deepseek/deepseek-reasoner
2025-01-29 07:45:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:47:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 07:49:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 281287 tokens (281287 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:51:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 223993 tokens (223993 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 07:52:02,413232.9981327057,413233.5157394409,0.0,0,228,,deepseek/deepseek-reasoner
2025-01-29 07:59:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:01:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 08:03:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:05:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:07:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:09:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:11:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:14:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 08:16:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:18:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:20:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:22:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:24:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:26:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:28:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:30:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:32:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:34:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:36:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 08:38:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:40:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:42:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:44:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:46:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:48:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 123213 tokens (123213 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 08:49:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 08:51:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 239161 tokens (239161 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 08:52:24,268632.8709125519,268633.2712173462,0.0,0,338,,deepseek/deepseek-reasoner
2025-01-29 08:57:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 124198 tokens (124198 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 08:58:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:00:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:02:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 79372 tokens (79372 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 09:04:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:06:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:08:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:10:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:12:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:14:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:16:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:18:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:20:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:22:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:24:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:26:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:28:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:30:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:32:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:34:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:36:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:38:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:40:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 393109 tokens (393109 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 09:41:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:43:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:45:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:47:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:49:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:51:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 09:53:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:55:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:57:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 09:59:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:01:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:03:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:05:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:07:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:09:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:11:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:13:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71317 tokens (71317 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 10:14:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:16:30,252758.32843780518,252758.850812912,0.0,0,385,,deepseek/deepseek-reasoner
2025-01-29 10:21:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:23:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:25:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:27:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 613546 tokens (613546 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 10:28:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 10:30:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:32:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:34:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 385692 tokens (385692 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 10:35:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:37:55,75946.1760520935,75946.9051361084,0.0,0,8,,deepseek/deepseek-reasoner
2025-01-29 10:40:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:42:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:44:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:46:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:48:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:50:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:52:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 10:54:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 261912 tokens (261912 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 10:55:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 10:57:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173872 tokens (173872 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 10:58:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:00:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 171008 tokens (171008 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 11:01:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 502819 tokens (502819 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 11:02:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 629804 tokens (629804 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 11:03:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:05:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 692072 tokens (692072 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 11:06:38,948496.0432052612,948496.4981079102,0.0,0,13563,,deepseek/deepseek-reasoner
2025-01-29 11:23:27,833590.6059741974,841032.6149463654,24.052645014193608,179,11008,,deepseek/deepseek-reasoner
2025-01-29 11:38:28,366991.0202026367,366991.4162158966,0.0,0,83,,deepseek/deepseek-reasoner
2025-01-29 11:45:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:47:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:49:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:51:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:53:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:55:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:57:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 11:59:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:01:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:03:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:05:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:07:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:09:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:11:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 273512 tokens (273512 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 12:12:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:14:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:16:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:18:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:20:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:22:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:24:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:26:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:28:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:30:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:32:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88995 tokens (88995 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 12:33:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:35:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:38:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 246272 tokens (246272 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 12:39:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 117326 tokens (117326 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 12:40:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:42:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 12:44:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:46:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 81082 tokens (81082 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 12:47:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:49:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:51:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:53:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:55:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:57:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 12:59:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 399375 tokens (399375 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 13:00:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:02:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:04:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:06:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 123782 tokens (123782 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 13:07:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:09:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:11:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:13:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:15:46,911961.7509841919,924057.7290058136,21.7427643742312,263,24610,,deepseek/deepseek-reasoner
2025-01-29 13:32:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 13:34:10,2022184.4017505646,2022185.5971813202,0.0,0,1325,,deepseek/deepseek-reasoner
2025-01-29 14:08:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:10:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:12:54,204786.6714000702,213886.52515411377,25.275131470964354,230,12,,deepseek/deepseek-reasoner
2025-01-29 14:17:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 325029 tokens (325029 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 14:18:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:20:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:22:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:24:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 491032 tokens (491032 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 14:25:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:27:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 14:29:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 123190 tokens (123190 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 14:30:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:32:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:34:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 270468 tokens (270468 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 14:35:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:37:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 81657 tokens (81657 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 14:38:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:40:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:42:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:44:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:46:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:48:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:50:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:52:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:54:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:56:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 14:58:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:00:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:02:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:04:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:06:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:08:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 129554 tokens (129554 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:09:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:11:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:13:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 148671 tokens (148671 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:14:59,333408.83779525757,344691.57791137695,22.51226186067303,254,789,,deepseek/deepseek-reasoner
2025-01-29 15:21:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:23:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:25:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 15:27:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:29:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:31:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:33:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:35:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 588332 tokens (588332 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:36:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 507360 tokens (507360 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:37:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:39:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:41:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 123073 tokens (123073 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:42:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:44:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 273108 tokens (273108 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:45:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:47:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:50:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 161515 tokens (161515 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:51:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:53:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:55:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 15:57:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 469338 tokens (469338 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 15:58:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:00:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:02:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 159630 tokens (159630 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 16:03:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:05:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:07:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 204766 tokens (204766 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 16:08:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:10:12,,,,,,litellm.APIConnectionError: APIConnectionError: OpenAIException - peer closed connection without sending complete message body (incomplete chunked read),deepseek/deepseek-chat
2025-01-29 16:12:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:14:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:16:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 433349 tokens (433349 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 16:17:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:19:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:21:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:23:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:25:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:27:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:29:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:31:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:33:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298557 tokens (298557 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 16:34:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:36:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:38:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:40:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:42:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 334896 tokens (334896 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 16:43:27,29370.12267112732,34585.77060699463,22.049034255007985,115,6,,deepseek/deepseek-reasoner
2025-01-29 16:45:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:47:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:49:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:51:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:53:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:55:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:57:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 16:59:06,4451182.953834534,4476809.900522232,21.422762793022667,549,5245,,deepseek/deepseek-reasoner
2025-01-29 18:14:43,11434.099674224854,13246.962547302246,29.235526187389347,53,15675,,deepseek/deepseek-chat
2025-01-29 18:15:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:17:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 18:19:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:21:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:23:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:25:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 99577 tokens (99577 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 18:27:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:29:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 758726 tokens (758726 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 18:30:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:32:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:34:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:36:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:38:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:40:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:42:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 18:44:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:46:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120133 tokens (120133 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 18:47:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 18:49:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:51:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:53:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:55:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 516280 tokens (516280 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 18:56:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 18:58:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:00:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 19:02:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 183060 tokens (183060 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:03:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 203333 tokens (203333 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:04:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:06:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:08:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:10:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 442658 tokens (442658 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 19:11:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 19:13:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:15:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:17:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:19:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:21:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:23:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:25:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:27:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 101855 tokens (101855 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:28:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:30:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73825 tokens (73825 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:31:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 155834 tokens (155834 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:32:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:34:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:36:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:38:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 285559 tokens (285559 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 19:39:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:41:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:43:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:45:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:47:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:49:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:51:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:53:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:55:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:57:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 19:59:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:01:53,1216768.15199852,1216769.9258327484,0.0,0,45995,,deepseek/deepseek-reasoner
2025-01-29 20:23:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:25:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:27:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:29:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:31:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:33:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:35:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:37:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:39:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:41:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:43:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:45:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 444112 tokens (444112 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 20:46:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:48:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:50:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:52:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:54:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:56:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 20:58:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:00:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 21:02:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:04:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:06:29,119096.81677818298,119097.92828559875,0.0,0,18,,deepseek/deepseek-reasoner
2025-01-29 21:09:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-29 21:11:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:13:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:15:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:17:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:19:31,359198.46081733704,370817.5542354584,27.885135986141915,324,293,,deepseek/deepseek-reasoner
2025-01-29 21:26:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 89090 tokens (89090 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 21:27:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 220313 tokens (220313 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 21:28:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:30:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 532403 tokens (532403 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 21:31:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:33:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:35:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:37:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:39:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:41:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:43:51,122866.16539955139,122866.76597595215,0.0,0,10,,deepseek/deepseek-chat
2025-01-29 21:46:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:48:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:50:55,119463.97233009338,119465.15536308289,0.0,0,9,,deepseek/deepseek-chat
2025-01-29 21:53:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 21:55:55,155066.30635261536,181822.06964492798,24.704957686253575,661,8,,deepseek/deepseek-reasoner
2025-01-29 21:59:57,70953.47046852112,72783.4939956665,29.507817358082512,54,7,,deepseek/deepseek-reasoner
2025-01-29 22:02:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:04:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:06:11,306881.3545703888,316541.40853881836,28.985345311224965,280,207,,deepseek/deepseek-reasoner
2025-01-29 22:12:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:14:28,115634.73653793335,115635.22028923035,0.0,0,981,,deepseek/deepseek-chat
2025-01-29 22:17:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173069 tokens (173069 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 22:18:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 404074 tokens (404074 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 22:19:28,114648.59175682068,116556.76794052124,26.203031159855517,50,86,,deepseek/deepseek-chat
2025-01-29 22:22:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 323272 tokens (323272 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 22:23:26,98217.14782714844,98218.19353103638,0.0,0,47,,deepseek/deepseek-chat
2025-01-29 22:26:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:28:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:30:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:32:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:34:06,115471.8849658966,115472.66149520874,0.0,0,728,,deepseek/deepseek-chat
2025-01-29 22:37:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109802 tokens (109802 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 22:38:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 217581 tokens (217581 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 22:39:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 22:41:05,1697638.831615448,1716353.6331653595,31.151813095381552,583,11622,,deepseek/deepseek-reasoner
2025-01-29 23:10:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 206199 tokens (206199 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 23:11:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 382039 tokens (382039 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-29 23:12:45,457470.4842567444,469408.6625576019,32.08194670475715,383,22,,deepseek/deepseek-reasoner
2025-01-29 23:21:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 469268 tokens (469268 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 23:22:38,108846.90546989441,108848.61183166504,0.0,0,57758,,deepseek/deepseek-chat
2025-01-29 23:25:26,12856.945514678955,15509.827613830566,34.30231597141151,91,1439,,deepseek/deepseek-chat
2025-01-29 23:26:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 23:28:42,65012.94279098511,65013.46397399902,0.0,0,5398,,deepseek/deepseek-chat
2025-01-29 23:30:47,22020.621061325073,24553.84397506714,31.97507789803905,81,18,,deepseek/deepseek-chat
2025-01-29 23:32:12,5954.450368881226,7831.6473960876465,29.299002290584852,55,308,,deepseek/deepseek-chat
2025-01-29 23:33:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 133872 tokens (133872 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-29 23:34:21,141053.63774299622,144561.62929534912,31.642037428952552,111,177,,deepseek/deepseek-reasoner
2025-01-29 23:37:45,5403.797149658203,7699.574708938599,26.570518451761618,61,11,,deepseek/deepseek-chat
2025-01-29 23:38:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 23:40:53,10069.694757461548,14329.871654510498,29.106772558176054,124,9112,,deepseek/deepseek-chat
2025-01-29 23:42:08,63911.45658493042,63912.031412124634,0.0,0,8,,deepseek/deepseek-chat
2025-01-29 23:44:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 23:46:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 23:48:13,317329.11491394043,324387.788772583,32.159015212476945,227,342,,deepseek/deepseek-reasoner
2025-01-29 23:54:37,38299.35073852539,40758.938789367676,20.735179609664744,51,23628,,deepseek/deepseek-chat
2025-01-29 23:56:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-29 23:58:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 00:00:19,66896.28839492798,66897.36366271973,0.0,0,10101,,deepseek/deepseek-chat
2025-01-30 00:02:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 00:04:27,82217.39053726196,84765.58232307434,23.546108395005152,60,740,,deepseek/deepseek-chat
2025-01-30 00:06:51,35854.944944381714,38432.07883834839,25.221817210262696,65,531,,deepseek/deepseek-chat
2025-01-30 00:08:30,15499.502182006836,16561.142683029175,26.374276389264118,28,5,,deepseek/deepseek-chat
2025-01-30 00:09:46,11770.373344421387,13645.67518234253,27.728869533687636,52,701,,deepseek/deepseek-chat
2025-01-30 00:11:00,5159.853935241699,6887.407064437866,30.10037672427225,52,14,,deepseek/deepseek-chat
2025-01-30 00:12:07,6570.934772491455,11880.85126876831,33.14553065446615,176,334,,deepseek/deepseek-chat
2025-01-30 00:13:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 573557 tokens (573557 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 00:14:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 511722 tokens (511722 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 00:15:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95244 tokens (95244 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 00:16:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 395391 tokens (395391 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 00:17:27,5989.87603187561,7342.166900634766,24.40303396434258,33,5,,deepseek/deepseek-chat
2025-01-30 00:18:34,2232132.1737766266,2266333.9071273804,27.074651173478948,926,20215,,deepseek/deepseek-reasoner
2025-01-30 00:57:21,4364.282608032227,5881.255388259888,22.41305872007632,34,4,,deepseek/deepseek-chat
2025-01-30 00:58:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 661453 tokens (661453 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 00:59:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 76993 tokens (76993 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 01:00:31,6680.86576461792,9776.665210723877,28.74863231594295,89,262,,deepseek/deepseek-chat
2025-01-30 01:01:40,593445.916891098,597499.3088245392,25.657523799261202,104,318,,deepseek/deepseek-reasoner
2025-01-30 01:12:38,5369.387149810791,7335.036277770996,23.401938497403734,46,73,,deepseek/deepseek-chat
2025-01-30 01:13:45,5085.26349067688,6871.610164642334,27.43028590930027,49,12,,deepseek/deepseek-chat
2025-01-30 01:14:52,4595.873117446899,6434.618949890137,26.104749853447608,48,192,,deepseek/deepseek-chat
2025-01-30 01:15:59,4780.333757400513,8191.608667373657,15.243567690183426,52,5,,deepseek/deepseek-chat
2025-01-30 01:17:07,4320.469617843628,6167.037725448608,27.618802572165926,51,27,,deepseek/deepseek-chat
2025-01-30 01:18:13,5196.3050365448,6608.582019805908,21.24229195517057,30,4,,deepseek/deepseek-chat
2025-01-30 01:19:20,732982.1512699127,737480.420589447,24.676157009534425,111,554,,deepseek/deepseek-reasoner
2025-01-30 01:32:37,5816.657543182373,8152.405500411987,20.978290850404065,49,54,,deepseek/deepseek-chat
2025-01-30 01:33:45,4917.355537414551,7008.326768875122,24.39057947458025,51,31,,deepseek/deepseek-chat
2025-01-30 01:34:52,6077.247619628906,8137.301206588745,23.30036475936379,48,613,,deepseek/deepseek-chat
2025-01-30 01:36:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 381013 tokens (381013 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 01:37:04,152591.8664932251,161826.96771621704,30.427387119527747,281,21,,deepseek/deepseek-reasoner
2025-01-30 01:40:46,4491.460561752319,6910.590887069702,17.3616111378744,42,5,,deepseek/deepseek-chat
2025-01-30 01:41:53,11710.306167602539,20229.33554649353,27.82007074506095,237,5596,,deepseek/deepseek-chat
2025-01-30 01:43:13,5007.848739624023,8635.122776031494,25.91477761440368,94,103,,deepseek/deepseek-chat
2025-01-30 01:44:22,4416.941404342651,5698.355197906494,21.850865146477744,28,5,,deepseek/deepseek-chat
2025-01-30 01:45:27,4965.500831604004,7711.503505706787,24.03493653609225,66,396,,deepseek/deepseek-chat
2025-01-30 01:46:35,6544.067144393921,13761.93380355835,27.57047329869032,199,5002,,deepseek/deepseek-chat
2025-01-30 01:47:49,4829.381465911865,7322.9687213897705,25.665835378090332,64,236,,deepseek/deepseek-chat
2025-01-30 01:48:56,4419.178247451782,5179.003477096558,13.160921235367617,10,4,,deepseek/deepseek-chat
2025-01-30 01:50:01,10340.90805053711,19475.186347961426,23.86614387055315,218,2077,,deepseek/deepseek-chat
2025-01-30 01:51:21,12075.22702217102,14447.437286376953,24.028224167169274,57,6096,,deepseek/deepseek-chat
2025-01-30 01:52:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146466 tokens (146466 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 01:53:37,5294.553756713867,8000.126838684082,17.74115817453584,48,95,,deepseek/deepseek-chat
2025-01-30 01:54:45,5035.910606384277,6541.134357452393,19.9305917002119,30,4,,deepseek/deepseek-chat
2025-01-30 01:55:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 98794 tokens (98794 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 01:56:53,5292.866468429565,6841.427326202393,21.310108565872763,33,32,,deepseek/deepseek-chat
2025-01-30 01:58:00,315139.7635936737,328024.165391922,21.731703526823235,280,39,,deepseek/deepseek-reasoner
2025-01-30 02:04:28,2181757.967233658,2197686.25831604,19.964476939508604,318,3545,,deepseek/deepseek-reasoner
2025-01-30 02:42:05,4765.803575515747,7653.548002243042,22.162626099336546,64,13,,deepseek/deepseek-chat
2025-01-30 02:43:13,13591.540813446045,17583.998203277588,22.292035032527988,89,30661,,deepseek/deepseek-chat
2025-01-30 02:44:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 02:46:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 02:48:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 02:50:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 02:52:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176147 tokens (176147 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 02:53:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 02:55:35,113301.7086982727,118668.97678375244,17.14093623325634,92,64359,,deepseek/deepseek-chat
2025-01-30 02:58:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:00:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:02:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:04:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:06:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:08:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:10:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 194090 tokens (194090 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 03:11:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:13:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:15:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:17:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:19:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:21:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:23:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:25:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70713 tokens (70713 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 03:26:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 429126 tokens (429126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 03:27:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:29:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:31:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:33:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:35:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:37:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 429487 tokens (429487 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 03:38:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:40:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:42:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 03:44:55,24767.31586456299,26620.371341705322,15.11017902344718,28,7,,deepseek/deepseek-chat
2025-01-30 03:46:22,12644.747734069824,22429.28647994995,26.470333117036756,259,5,,deepseek/deepseek-chat
2025-01-30 03:47:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 205010 tokens (205010 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 03:48:46,77181.08177185059,77182.63840675354,0.0,0,36195,,deepseek/deepseek-chat
2025-01-30 03:51:03,6161.32378578186,10823.039770126343,24.45451425673466,114,522,,deepseek/deepseek-chat
2025-01-30 03:52:14,65129.20331954956,65129.645586013794,0.0,0,71,,deepseek/deepseek-chat
2025-01-30 03:54:19,64448.760747909546,64449.38635826111,0.0,0,129,,deepseek/deepseek-chat
2025-01-30 03:56:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 03:58:24,6997.915983200073,9256.925106048584,16.378862584381526,37,4,,deepseek/deepseek-chat
2025-01-30 03:59:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87255 tokens (87255 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:00:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:02:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:04:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 547931 tokens (547931 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:05:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 215394 tokens (215394 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:06:40,3660.1438522338867,4329.466104507446,46.31550780614111,31,4,,deepseek/deepseek-chat
2025-01-30 04:07:44,6457.826137542725,8286.319732666016,49.22084509348883,90,6893,,deepseek/deepseek-chat
2025-01-30 04:08:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:10:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:12:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:14:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80976 tokens (80976 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:15:55,69264.27602767944,69265.46049118042,0.0,0,19430,,deepseek/deepseek-chat
2025-01-30 04:18:04,6979.145526885986,9220.31283378601,21.417410405827063,48,28,,deepseek/deepseek-chat
2025-01-30 04:19:14,97048.94781112671,97049.4453907013,0.0,0,52323,,deepseek/deepseek-chat
2025-01-30 04:21:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:23:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:25:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:27:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:29:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 89727 tokens (89727 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:30:54,63936.20586395264,63936.74850463867,0.0,0,7,,deepseek/deepseek-chat
2025-01-30 04:32:58,89388.32640647888,89388.8373374939,0.0,0,48702,,deepseek/deepseek-chat
2025-01-30 04:35:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:37:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 112345 tokens (112345 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:38:28,525777.8141498566,525778.2819271088,0.0,0,62,,deepseek/deepseek-reasoner
2025-01-30 04:48:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:50:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:52:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 477191 tokens (477191 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 04:53:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:55:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 04:57:21,307928.0688762665,325961.55405044556,19.796506141317824,357,46,,deepseek/deepseek-reasoner
2025-01-30 05:03:47,6486.421823501587,8532.89794921875,23.454952343105873,48,47,,deepseek/deepseek-chat
2025-01-30 05:04:55,66311.851978302,66312.38865852356,0.0,0,4,,deepseek/deepseek-chat
2025-01-30 05:07:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 05:09:02,56230.658292770386,60230.27062416077,23.252253542200304,93,7438,,deepseek/deepseek-chat
2025-01-30 05:11:02,98428.95460128784,98429.44192886353,0.0,0,1959,,deepseek/deepseek-chat
2025-01-30 05:13:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 244959 tokens (244959 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:14:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 102091 tokens (102091 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:15:43,6850.326776504517,8905.78556060791,23.352450738114875,48,24,,deepseek/deepseek-chat
2025-01-30 05:16:52,82492.9838180542,82494.46225166321,0.0,0,35840,,deepseek/deepseek-chat
2025-01-30 05:19:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 05:21:15,5553.6253452301025,6941.843271255493,22.330787853140723,31,4,,deepseek/deepseek-chat
2025-01-30 05:22:22,7583.2579135894775,13823.058366775513,25.321322562378636,158,333,,deepseek/deepseek-chat
2025-01-30 05:23:36,5219.02871131897,7533.998966217041,21.16657866178824,49,1031,,deepseek/deepseek-chat
2025-01-30 05:24:44,255426.3517856598,271229.7215461731,20.24884596445753,320,323,,deepseek/deepseek-reasoner
2025-01-30 05:30:15,65956.95066452026,65957.50784873962,0.0,0,14791,,deepseek/deepseek-chat
2025-01-30 05:32:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71357 tokens (71357 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:33:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 05:35:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 84629 tokens (84629 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:36:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 05:38:24,,,,,,"litellm.APIError: APIError: DeepseekException - <!DOCTYPE html>
<!--[if lt IE 7]> <html class=""no-js ie6 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 7]>    <html class=""no-js ie7 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 8]>    <html class=""no-js ie8 oldie"" lang=""en-US""> <![endif]-->
<!--[if gt IE 8]><!--> <html class=""no-js"" lang=""en-US""> <!--<![endif]-->
<head>


<title>api.deepseek.com | 522: Connection timed out</title>
<meta charset=""UTF-8"" />
<meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" />
<meta http-equiv=""X-UA-Compatible"" content=""IE=Edge"" />
<meta name=""robots"" content=""noindex, nofollow"" />
<meta name=""viewport"" content=""width=device-width,initial-scale=1"" />
<link rel=""stylesheet"" id=""cf_styles-css"" href=""/cdn-cgi/styles/main.css"" />


</head>
<body>
<div id=""cf-wrapper"">
    <div id=""cf-error-details"" class=""p-0"">
        <header class=""mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-8"">
            <h1 class=""inline-block sm:block sm:mb-2 font-light text-60 lg:text-4xl text-black-dark leading-tight mr-2"">
              <span class=""inline-block"">Connection timed out</span>
              <span class=""code-label"">Error code 522</span>
            </h1>
            <div>
               Visit <a href=""https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_522&utm_campaign=api.deepseek.com"" target=""_blank"" rel=""noopener noreferrer"">cloudflare.com</a> for more information.
            </div>
            <div class=""mt-3"">2025-01-30 04:44:53 UTC</div>
        </header>
        <div class=""my-8 bg-gradient-gray"">
            <div class=""w-240 lg:w-full mx-auto"">
                <div class=""clearfix md:px-8"">
                  
<div id=""cf-browser-status"" class="" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center"">
  <div class=""relative mb-10 md:m-0"">
    
    <span class=""cf-icon-browser block md:hidden h-20 bg-center bg-no-repeat""></span>
    <span class=""cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4""></span>
    
  </div>
  <span class=""md:block w-full truncate"">You</span>
  <h3 class=""md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3"">
    
    Browser
    
  </h3>
  <span class=""leading-1.3 text-2xl text-green-success"">Working</span>
</div>

<div id=""cf-cloudflare-status"" class="" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center"">
  <div class=""relative mb-10 md:m-0"">
    <a href=""https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_522&utm_campaign=api.deepseek.com"" target=""_blank"" rel=""noopener noreferrer"">
    <span class=""cf-icon-cloud block md:hidden h-20 bg-center bg-no-repeat""></span>
    <span class=""cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4""></span>
    </a>
  </div>
  <span class=""md:block w-full truncate"">Frankfurt</span>
  <h3 class=""md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3"">
    <a href=""https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_522&utm_campaign=api.deepseek.com"" target=""_blank"" rel=""noopener noreferrer"">
    Cloudflare
    </a>
  </h3>
  <span class=""leading-1.3 text-2xl text-green-success"">Working</span>
</div>

<div id=""cf-host-status"" class=""cf-error-source relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center"">
  <div class=""relative mb-10 md:m-0"">
    
    <span class=""cf-icon-server block md:hidden h-20 bg-center bg-no-repeat""></span>
    <span class=""cf-icon-error w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4""></span>
    
  </div>
  <span class=""md:block w-full truncate"">api.deepseek.com</span>
  <h3 class=""md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3"">
    
    Host
    
  </h3>
  <span class=""leading-1.3 text-2xl text-red-error"">Error</span>
</div>

                </div>
            </div>
        </div>

        <div class=""w-240 lg:w-full mx-auto mb-8 lg:px-8"">
            <div class=""clearfix"">
                <div class=""w-1/2 md:w-full float-left pr-6 md:pb-10 md:pr-0 leading-relaxed"">
                    <h2 class=""text-3xl font-normal leading-1.3 mb-4"">What happened?</h2>
                    <p>The initial connection between Cloudflare's network and the origin web server timed out. As a result, the web page can not be displayed.</p>
                </div>
                <div class=""w-1/2 md:w-full float-left leading-relaxed"">
                    <h2 class=""text-3xl font-normal leading-1.3 mb-4"">What can I do?</h2>
                          <h3 class=""text-15 font-semibold mb-2"">If you're a visitor of this website:</h3>
      <p class=""mb-6"">Please try again in a few minutes.</p>

      <h3 class=""text-15 font-semibold mb-2"">If you're the owner of this website:</h3>
      <p><span>Contact your hosting provider letting them know your web server is not completing requests. An Error 522 means that the request was able to connect to your web server, but that the request didn't finish. The most likely cause is that something on your server is hogging resources.</span> <a rel=""noopener noreferrer"" href=""https://support.cloudflare.com/hc/en-us/articles/200171906-Error-522"">Additional troubleshooting information here.</a></p>
                </div>
            </div>
        </div>

        <div class=""cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300"">
  <p class=""text-13"">
    <span class=""cf-footer-item sm:block sm:mb-1"">Cloudflare Ray ID: <strong class=""font-semibold"">909ece42dd8ccb25</strong></span>
    <span class=""cf-footer-separator sm:hidden"">&bull;</span>
    <span id=""cf-footer-item-ip"" class=""cf-footer-item hidden sm:block sm:mb-1"">
      Your IP:
      <button type=""button"" id=""cf-footer-ip-reveal"" class=""cf-footer-ip-reveal-btn"">Click to reveal</button>
      <span class=""hidden"" id=""cf-footer-ip"">141.98.102.235</span>
      <span class=""cf-footer-separator sm:hidden"">&bull;</span>
    </span>
    <span class=""cf-footer-item sm:block sm:mb-1""><span>Performance &amp; security by</span> <a rel=""noopener noreferrer"" href=""https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_522&utm_campaign=api.deepseek.com"" id=""brand_link"" target=""_blank"">Cloudflare</a></span>
    
  </p>
  <script>(function(){function d(){var b=a.getElementById(""cf-footer-item-ip""),c=a.getElementById(""cf-footer-ip-reveal"");b&&""classList""in b&&(b.classList.remove(""hidden""),c.addEventListener(""click"",function(){c.classList.add(""hidden"");a.getElementById(""cf-footer-ip"").classList.remove(""hidden"")}))}var a=document;document.addEventListener&&a.addEventListener(""DOMContentLoaded"",d)})();</script>
</div><!-- /.error-footer -->


    </div>
</div>
</body>
</html>",deepseek/deepseek-chat
2025-01-30 05:45:53,241077.6765346527,251249.591588974,21.431559233026434,218,2240,,deepseek/deepseek-chat
2025-01-30 05:51:05,18455.831050872803,22280.680894851685,25.360472686973164,97,1032,,deepseek/deepseek-chat
2025-01-30 05:52:27,17787.320137023926,19904.21485900879,23.147112367523004,49,5,,deepseek/deepseek-chat
2025-01-30 05:53:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71834 tokens (71834 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:54:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 131012 tokens (131012 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 05:55:49,9000.341176986694,13128.789901733398,21.799955867326858,90,10891,,deepseek/deepseek-chat
2025-01-30 05:57:02,52549.73816871643,55547.693490982056,23.68280790333581,71,121,,deepseek/deepseek-chat
2025-01-30 05:58:58,6090.413570404053,8056.422710418701,24.41494244510092,48,24,,deepseek/deepseek-chat
2025-01-30 06:00:06,22566.555500030518,26668.357372283936,24.135734265880597,99,354,,deepseek/deepseek-chat
2025-01-30 06:01:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 251542 tokens (251542 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 06:02:34,31016.477346420288,33428.28464508057,24.462982607596217,59,1157,,deepseek/deepseek-chat
2025-01-30 06:04:07,20843.867301940918,25729.660034179688,20.058157472245945,98,32265,,deepseek/deepseek-chat
2025-01-30 06:05:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:07:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:09:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:11:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:13:35,682150.7394313812,682151.2868404388,0.0,0,52335,,deepseek/deepseek-reasoner
2025-01-30 06:25:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 261729 tokens (261729 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 06:26:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:29:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:31:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 611794 tokens (611794 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 06:32:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:34:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:36:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 734743 tokens (734743 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 06:37:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:39:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 439719 tokens (439719 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 06:40:12,16112.191915512085,17575.15287399292,20.506357210757383,30,26,,deepseek/deepseek-chat
2025-01-30 06:41:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 06:43:30,13670.906066894531,15307.63554573059,21.384108035305765,35,20,,deepseek/deepseek-chat
2025-01-30 06:44:45,82259.55390930176,82260.04648208618,0.0,0,38164,,deepseek/deepseek-chat
2025-01-30 06:47:07,5345.98183631897,7003.831148147583,19.905307294545903,33,5,,deepseek/deepseek-chat
2025-01-30 06:48:14,84461.04264259338,84462.58687973022,0.0,0,43802,,deepseek/deepseek-chat
2025-01-30 06:50:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 06:52:39,267896.0921764374,267896.8904018402,0.0,0,675,,deepseek/deepseek-reasoner
2025-01-30 06:58:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:00:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:02:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:04:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 311992 tokens (311992 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 07:05:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:07:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:09:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:11:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:13:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:15:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:17:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:19:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:21:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:23:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:25:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:27:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:29:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:31:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 83311 tokens (83311 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 07:32:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 07:34:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 07:36:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88109 tokens (88109 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 07:37:21,36054.12745475769,38660.63451766968,40.66745166674396,106,7603,,deepseek/deepseek-chat
2025-01-30 07:39:00,54851.57108306885,61471.24409675598,33.083205098980834,219,11,,deepseek/deepseek-reasoner
2025-01-30 07:41:01,3529.860019683838,4256.371736526489,41.29320877353094,30,28,,deepseek/deepseek-chat
2025-01-30 07:42:05,32024.829149246216,33811.233043670654,27.42940728741935,49,3043,,deepseek/deepseek-chat
2025-01-30 07:43:39,58598.002672195435,60066.625118255615,34.72642007945325,51,67,,deepseek/deepseek-chat
2025-01-30 07:45:39,28983.52313041687,36986.55939102173,34.736816246661505,278,8,,deepseek/deepseek-chat
2025-01-30 07:47:16,33672.37710952759,37743.75820159912,27.263475830390224,111,635,,deepseek/deepseek-chat
2025-01-30 07:48:54,24078.431129455566,26545.477390289307,24.320581641514476,60,241,,deepseek/deepseek-chat
2025-01-30 07:50:21,4639.492034912109,6468.798875808716,26.786102202506065,49,135,,deepseek/deepseek-chat
2025-01-30 07:51:27,19958.45675468445,22015.097618103027,26.256407212604145,54,1955,,deepseek/deepseek-chat
2025-01-30 07:52:49,17848.73676300049,19550.24766921997,21.15766632374219,36,15856,,deepseek/deepseek-chat
2025-01-30 07:54:09,35121.721267700195,37882.896900177,32.23264719316894,89,23202,,deepseek/deepseek-chat
2025-01-30 07:55:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 260742 tokens (260742 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 07:56:48,27762.54391670227,29380.80382347107,26.571751435069952,43,15,,deepseek/deepseek-chat
2025-01-30 07:58:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:00:18,4010.542631149292,4944.588899612427,41.753820251506376,39,22,,deepseek/deepseek-chat
2025-01-30 08:01:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:03:25,57663.60807418823,59675.62508583069,27.335752969157166,55,9,,deepseek/deepseek-chat
2025-01-30 08:05:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 08:07:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:09:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298153 tokens (298153 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 08:10:27,63187.94870376587,63188.724517822266,0.0,0,6,,deepseek/deepseek-chat
2025-01-30 08:12:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:14:31,4838.509798049927,6879.140377044678,30.382765326658117,62,14,,deepseek/deepseek-chat
2025-01-30 08:15:38,5139.180660247803,8358.513593673706,30.75171224824132,99,165,,deepseek/deepseek-chat
2025-01-30 08:16:46,67512.30144500732,67513.72361183167,0.0,0,6355,,deepseek/deepseek-chat
2025-01-30 08:18:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:20:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:22:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:24:57,729614.419221878,729615.4725551605,0.0,0,6663,,deepseek/deepseek-reasoner
2025-01-30 08:38:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:40:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 446493 tokens (446493 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 08:41:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 555769 tokens (555769 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 08:42:12,122127.82144546509,122128.61943244934,0.0,0,65,,deepseek/deepseek-chat
2025-01-30 08:45:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:47:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:49:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 08:51:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 08:53:17,86261.17825508118,86261.62028312683,0.0,0,53,,deepseek/deepseek-chat
2025-01-30 08:55:43,78892.9591178894,78893.29242706299,0.0,0,4953,,deepseek/deepseek-chat
2025-01-30 08:58:02,136707.7763080597,136708.2769870758,0.0,0,14,,deepseek/deepseek-reasoner
2025-01-30 09:01:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:03:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:05:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:07:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 380192 tokens (380192 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 09:08:22,55372.25651741028,57596.04024887085,26.081672952030214,58,13223,,deepseek/deepseek-chat
2025-01-30 09:10:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:12:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 340567 tokens (340567 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 09:13:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:15:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:17:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 09:19:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:21:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:23:25,63661.74602508545,63662.232875823975,0.0,0,3861,,deepseek/deepseek-chat
2025-01-30 09:25:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:27:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 149496 tokens (149496 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 09:28:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:30:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 131830 tokens (131830 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 09:31:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:33:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:35:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:37:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 141893 tokens (141893 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 09:38:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 393004 tokens (393004 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 09:39:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 09:41:39,75863.37494850159,75864.03584480286,0.0,0,1407,,deepseek/deepseek-chat
2025-01-30 09:43:55,32968.666076660156,36111.974000930786,26.405303584522105,83,138,,deepseek/deepseek-chat
2025-01-30 09:45:31,65457.13448524475,65458.16516876221,0.0,0,303,,deepseek/deepseek-chat
2025-01-30 09:47:36,93253.09610366821,93894.82951164246,15.582794780104955,10,4,,deepseek/deepseek-chat
2025-01-30 09:50:10,50795.08900642395,53228.047132492065,25.89440374044325,63,175,,deepseek/deepseek-chat
2025-01-30 09:52:03,70652.02736854553,74874.94134902954,19.417871256426015,82,49880,,deepseek/deepseek-chat
2025-01-30 09:54:18,808775.3765583038,808776.6582965851,0.0,0,14313,,deepseek/deepseek-reasoner
2025-01-30 10:08:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:10:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 604470 tokens (604470 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 10:11:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:13:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:15:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:17:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 343244 tokens (343244 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 10:18:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:20:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:22:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 393675 tokens (393675 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 10:23:56,360349.85184669495,360350.6772518158,0.0,0,61,,deepseek/deepseek-reasoner
2025-01-30 10:30:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 248652 tokens (248652 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 10:31:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:33:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:35:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 10:38:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:40:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:42:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:44:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 10:46:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:48:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:50:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:52:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:54:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 10:56:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 10:58:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:00:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:02:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:04:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:06:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 273105 tokens (273105 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 11:07:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:09:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:11:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:13:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:15:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:17:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:19:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:21:15,120842.050075531,120842.86832809448,0.0,0,796,,deepseek/deepseek-chat
2025-01-30 11:24:16,538322.7224349976,538323.6193656921,0.0,0,52,,deepseek/deepseek-reasoner
2025-01-30 11:34:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 114121 tokens (114121 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 11:35:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:37:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:39:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:41:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66637 tokens (66637 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 11:42:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:44:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:46:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:48:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:50:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:52:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 144680 tokens (144680 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 11:53:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:55:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:57:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 11:59:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:01:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:03:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 367214 tokens (367214 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:04:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:06:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 751977 tokens (751977 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:07:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140027 tokens (140027 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 12:08:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460273 tokens (460273 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:09:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:11:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:13:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:15:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:17:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:19:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:21:41,145552.08015441895,145553.0960559845,0.0,0,10,,deepseek/deepseek-reasoner
2025-01-30 12:25:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:27:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:29:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:31:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:33:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 73061 tokens (73061 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:34:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:36:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 12:38:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 12:40:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:42:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 362306 tokens (362306 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:43:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:45:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:47:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:49:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 142079 tokens (142079 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:50:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:52:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:54:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 121431 tokens (121431 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:55:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80272 tokens (80272 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 12:56:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 12:58:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 13:00:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:02:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:04:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:06:23,417917.15717315674,417918.09368133545,0.0,0,20,,deepseek/deepseek-reasoner
2025-01-30 13:14:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:16:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:18:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:20:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:22:24,315314.5959377289,315315.34719467163,0.0,0,1755,,deepseek/deepseek-reasoner
2025-01-30 13:28:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:30:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:32:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:34:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 13:36:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 615665 tokens (615665 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 13:37:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:39:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:41:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:43:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:45:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 13:47:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 256798 tokens (256798 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 13:48:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 750079 tokens (750079 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 13:49:51,7569.690465927124,10181.834697723389,66.22911472274828,173,11779,,deepseek/deepseek-chat
2025-01-30 13:51:01,885614.9024963379,887891.4535045624,48.31870650058067,110,3470,,deepseek/deepseek-reasoner
2025-01-30 14:06:49,16818.121433258057,20724.2169380188,25.34500241464628,99,32028,,deepseek/deepseek-chat
2025-01-30 14:08:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 229690 tokens (229690 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 14:09:11,102432.45959281921,102434.14878845215,0.0,0,58435,,deepseek/deepseek-chat
2025-01-30 14:11:53,77166.38016700745,77167.33598709106,0.0,0,480,,deepseek/deepseek-chat
2025-01-30 14:14:11,35918.32947731018,38045.161962509155,27.270600954063312,58,2269,,deepseek/deepseek-chat
2025-01-30 14:15:49,12983.444690704346,14617.295742034912,31.214595699202142,51,13760,,deepseek/deepseek-chat
2025-01-30 14:17:03,20326.253414154053,22166.58043861389,36.94995459840021,68,293,,deepseek/deepseek-chat
2025-01-30 14:18:25,19717.337369918823,22027.019739151,37.66751703998241,87,568,,deepseek/deepseek-chat
2025-01-30 14:19:47,36964.95795249939,39910.23945808411,33.273559696815596,98,930,,deepseek/deepseek-chat
2025-01-30 14:21:27,19528.84006500244,21631.14333152771,35.675157430526944,75,17,,deepseek/deepseek-chat
2025-01-30 14:22:49,17802.233934402466,19746.6778755188,24.171434828312215,47,43,,deepseek/deepseek-chat
2025-01-30 14:24:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 244159 tokens (244159 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 14:25:10,22426.1257648468,23543.967485427856,47.41279469552324,53,146,,deepseek/deepseek-chat
2025-01-30 14:26:34,13946.330308914185,15490.71979522705,41.440323549984804,64,24,,deepseek/deepseek-chat
2025-01-30 14:27:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 14:29:50,46819.663524627686,48035.635471343994,41.94175707566592,51,1748,,deepseek/deepseek-chat
2025-01-30 14:31:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 129637 tokens (129637 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 14:32:39,10629.528522491455,12295.963048934937,32.40451343458855,54,4749,,deepseek/deepseek-chat
2025-01-30 14:33:51,7562.869071960449,8319.798231124878,25.10142431423045,19,4,,deepseek/deepseek-chat
2025-01-30 14:34:59,62811.62238121033,62812.17837333679,0.0,0,8,,deepseek/deepseek-chat
2025-01-30 14:37:02,19084.582567214966,19928.497314453125,36.73356829164589,31,6,,deepseek/deepseek-chat
2025-01-30 14:38:22,14825.734376907349,15958.159685134888,67.99565449532354,77,6,,deepseek/deepseek-chat
2025-01-30 14:39:38,3914.75772857666,5140.452861785889,39.161451081495244,48,135,,deepseek/deepseek-chat
2025-01-30 14:40:43,2895.322799682617,3711.9178771972656,58.78066292793561,48,33,,deepseek/deepseek-chat
2025-01-30 14:41:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 14:43:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 14:45:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 14:47:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 14:49:49,58585.41750907898,59657.50598907471,36.37759450615067,39,10,,deepseek/deepseek-chat
2025-01-30 14:51:49,29364.71915245056,30472.036600112915,27.995585245625616,31,23,,deepseek/deepseek-chat
2025-01-30 14:53:19,4668.082714080811,7495.184659957886,53.41158645524331,151,139,,deepseek/deepseek-chat
2025-01-30 14:54:27,52721.41146659851,54291.43929481506,18.471010181355876,29,9,,deepseek/deepseek-chat
2025-01-30 14:56:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 275900 tokens (275900 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 14:57:23,8872.672080993652,13004.694938659668,32.67164888730918,135,24482,,deepseek/deepseek-chat
2025-01-30 14:58:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:00:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:02:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 15:04:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:06:38,686132.476568222,686132.9772472382,0.0,0,8036,,deepseek/deepseek-reasoner
2025-01-30 15:19:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:21:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:23:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:25:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 495124 tokens (495124 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 15:26:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 516170 tokens (516170 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 15:27:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:29:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 671320 tokens (671320 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 15:30:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:32:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:34:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 231638 tokens (231638 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 15:35:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:37:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:39:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:41:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:43:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:45:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:47:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 15:49:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 15:51:24,684699.8074054718,684700.3560066223,0.0,0,94,,deepseek/deepseek-reasoner
2025-01-30 16:03:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:05:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:07:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:09:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:11:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:13:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 16:15:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:17:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:19:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:21:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 16:23:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:25:56,2687.8678798675537,3253.757953643799,63.616595639800046,36,21,,deepseek/deepseek-chat
2025-01-30 16:26:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:29:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:31:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:33:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 16:35:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 16:37:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:39:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:41:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:43:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:45:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 16:47:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:49:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 16:51:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 716918 tokens (716918 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 16:52:11,9120.73826789856,9964.343547821045,130.3927353443157,110,15849,,deepseek/deepseek-chat
2025-01-30 16:53:21,3086.925983428955,3892.8310871124268,59.560362356077775,48,491,,deepseek/deepseek-chat
2025-01-30 16:54:25,10562.55316734314,11960.339307785034,39.347936289175365,55,36655,,deepseek/deepseek-chat
2025-01-30 16:55:37,25344.711780548096,27736.2699508667,45.57698046101844,109,50136,,deepseek/deepseek-chat
2025-01-30 16:57:04,10082.69214630127,12016.631841659546,54.81039571937776,106,1176,,deepseek/deepseek-chat
2025-01-30 16:58:16,3973.428964614868,5569.17405128479,57.653318671336194,92,1027,,deepseek/deepseek-chat
2025-01-30 16:59:22,12752.112865447998,14744.880437850952,44.661505552642275,89,24917,,deepseek/deepseek-chat
2025-01-30 17:00:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 241150 tokens (241150 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:01:39,4458.92596244812,6119.143962860107,54.209748344895836,90,14367,,deepseek/deepseek-chat
2025-01-30 17:02:45,534483.6013317108,536318.9702033997,70.28559870981067,129,3153,,deepseek/deepseek-reasoner
2025-01-30 17:12:41,6271.580696105957,7869.561672210693,54.44370195950178,87,17865,,deepseek/deepseek-chat
2025-01-30 17:13:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 245983 tokens (245983 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:14:52,7248.640775680542,7331.958770751953,612.1126649325676,51,2314,,deepseek/deepseek-chat
2025-01-30 17:15:59,3936.234712600708,4971.940755844116,57.93149551595211,60,1524,,deepseek/deepseek-chat
2025-01-30 17:17:04,4840.250730514526,5361.900568008423,99.68372701852594,52,2112,,deepseek/deepseek-chat
2025-01-30 17:18:09,7247.440814971924,8849.696159362793,55.546701910883755,89,18544,,deepseek/deepseek-chat
2025-01-30 17:19:18,4349.264621734619,7120.417356491089,67.84180375266163,188,7,,deepseek/deepseek-chat
2025-01-30 17:20:25,13905.915021896362,15790.329694747925,47.22951974541892,89,36628,,deepseek/deepseek-chat
2025-01-30 17:21:41,5309.364557266235,6911.068439483643,64.93085342093437,104,430,,deepseek/deepseek-chat
2025-01-30 17:22:48,8155.163526535034,10030.688047409058,48.519760198918966,91,28310,,deepseek/deepseek-chat
2025-01-30 17:23:58,4029.913902282715,5221.743583679199,48.66467155948033,58,10281,,deepseek/deepseek-chat
2025-01-30 17:25:03,5932.46865272522,7871.32453918457,51.576809136967576,100,16852,,deepseek/deepseek-chat
2025-01-30 17:26:11,10581.112384796143,12268.076658248901,50.386366408357915,85,32719,,deepseek/deepseek-chat
2025-01-30 17:27:24,3968.4126377105713,5389.017343521118,62.649377153245275,89,4196,,deepseek/deepseek-chat
2025-01-30 17:28:29,4326.486587524414,5689.945459365845,60.14116134596288,82,7657,,deepseek/deepseek-chat
2025-01-30 17:29:35,4070.6913471221924,5047.231912612915,58.36931102944695,57,9179,,deepseek/deepseek-chat
2025-01-30 17:30:40,6647.923231124878,6773.354530334473,310.9271788282389,39,65,,deepseek/deepseek-chat
2025-01-30 17:31:46,4623.908519744873,7728.495121002197,60.23346229873794,187,6031,,deepseek/deepseek-chat
2025-01-30 17:32:54,3034.4974994659424,3718.7042236328125,58.46186333334611,40,11,,deepseek/deepseek-chat
2025-01-30 17:33:58,3695.591688156128,5435.298681259155,53.45727778798003,93,202,,deepseek/deepseek-chat
2025-01-30 17:35:03,2910.0701808929443,3726.048707962036,58.82507738581173,48,180,,deepseek/deepseek-chat
2025-01-30 17:36:07,5096.6010093688965,5933.573007583618,58.54437197960983,49,221,,deepseek/deepseek-chat
2025-01-30 17:37:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 378063 tokens (378063 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:38:15,5261.208534240723,6667.384386062622,64.00337474391435,90,14540,,deepseek/deepseek-chat
2025-01-30 17:39:22,3734.5900535583496,4675.049304962158,32.96261901164436,31,4,,deepseek/deepseek-chat
2025-01-30 17:40:26,2905.1811695098877,3745.436668395996,57.12548155130385,48,649,,deepseek/deepseek-chat
2025-01-30 17:41:30,57739.25304412842,61331.00342750549,57.632067350229434,207,14,,deepseek/deepseek-reasoner
2025-01-30 17:43:31,6739.250659942627,7486.139297485352,64.26660895246815,48,35,,deepseek/deepseek-chat
2025-01-30 17:44:39,3111.5920543670654,3977.459669113159,58.90045906723881,51,83,,deepseek/deepseek-chat
2025-01-30 17:45:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 431970 tokens (431970 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:46:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 328616 tokens (328616 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:47:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 432012 tokens (432012 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:48:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103464 tokens (103464 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:49:53,8164.323806762695,9026.740312576294,37.10504122345315,32,4,,deepseek/deepseek-chat
2025-01-30 17:51:02,38990.67783355713,42902.448654174805,71.83447417699935,281,21,,deepseek/deepseek-reasoner
2025-01-30 17:52:45,3259.4361305236816,4746.150493621826,60.53617442186419,90,460,,deepseek/deepseek-chat
2025-01-30 17:53:50,3825.4780769348145,4765.489339828491,59.57375428419103,56,27,,deepseek/deepseek-chat
2025-01-30 17:54:55,2991.8389320373535,3601.099729537964,50.88133050275395,31,6,,deepseek/deepseek-chat
2025-01-30 17:55:58,6271.198511123657,8129.087448120117,49.51856818133112,92,18437,,deepseek/deepseek-chat
2025-01-30 17:57:06,2380.8960914611816,2855.6721210479736,63.18768878477218,30,4,,deepseek/deepseek-chat
2025-01-30 17:58:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 705125 tokens (705125 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 17:59:12,6242.673873901367,8639.543056488037,73.8463330772954,177,17902,,deepseek/deepseek-chat
2025-01-30 18:00:20,5187.439680099487,8523.97632598877,60.24210770999272,201,5,,deepseek/deepseek-chat
2025-01-30 18:01:29,3564.9797916412354,4415.710926055908,56.422056344541055,48,5883,,deepseek/deepseek-chat
2025-01-30 18:02:33,3085.362195968628,4182.034969329834,63.82943180531065,70,5,,deepseek/deepseek-chat
2025-01-30 18:03:37,4316.589593887329,5647.025346755981,70.6535432450004,94,4583,,deepseek/deepseek-chat
2025-01-30 18:04:43,9819.869995117188,9923.993825912476,557.0290639232289,58,1814,,deepseek/deepseek-chat
2025-01-30 18:05:53,3295.6697940826416,3870.1884746551514,55.69879811064089,32,29,,deepseek/deepseek-chat
2025-01-30 18:06:57,4801.8739223480225,4859.017133712769,524.9967456065689,30,9,,deepseek/deepseek-chat
2025-01-30 18:08:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 276521 tokens (276521 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 18:09:05,3343.2536125183105,4747.930288314819,64.07168393321996,90,7601,,deepseek/deepseek-chat
2025-01-30 18:10:10,3413.5775566101074,4781.206369400024,65.80732956071832,90,908,,deepseek/deepseek-chat
2025-01-30 18:11:15,7409.771680831909,7840.907335281372,125.25060138891807,54,15929,,deepseek/deepseek-chat
2025-01-30 18:12:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 165332 tokens (165332 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:13:24,6829.651355743408,10928.403854370117,55.870658225088405,229,17848,,deepseek/deepseek-chat
2025-01-30 18:14:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67179 tokens (67179 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 18:15:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 206138 tokens (206138 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:16:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 259919 tokens (259919 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:17:40,4618.48783493042,6342.263221740723,22.62475743557641,39,28,,deepseek/deepseek-chat
2025-01-30 18:18:46,7272.116661071777,8819.953203201294,27.780711224739864,43,35,,deepseek/deepseek-chat
2025-01-30 18:19:55,18888.338327407837,24826.90382003784,24.585061860678607,146,36513,,deepseek/deepseek-chat
2025-01-30 18:21:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 99041 tokens (99041 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:22:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94523 tokens (94523 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:23:22,4156.062126159668,4980.2258014678955,38.82723900447611,32,4,,deepseek/deepseek-chat
2025-01-30 18:24:27,5327.387094497681,6479.126214981079,42.54435672848737,49,175,,deepseek/deepseek-chat
2025-01-30 18:25:33,3414.308786392212,4599.273920059204,31.224547413897174,37,9,,deepseek/deepseek-chat
2025-01-30 18:26:38,119011.19875907898,119011.60144805908,0.0,0,5386,,deepseek/deepseek-chat
2025-01-30 18:29:37,10323.829650878906,12697.51262664795,24.434602510981374,58,184,,deepseek/deepseek-chat
2025-01-30 18:30:50,18667.470693588257,26952.17204093933,25.106517577305947,208,9360,,deepseek/deepseek-chat
2025-01-30 18:32:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:34:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:36:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:38:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74230 tokens (74230 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:39:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:41:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:43:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:45:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:47:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:49:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:51:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 175295 tokens (175295 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:52:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:54:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 318901 tokens (318901 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 18:55:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:57:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 18:59:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:01:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:03:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:05:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:07:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:09:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:11:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140940 tokens (140940 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:12:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 169258 tokens (169258 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:13:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:15:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:17:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 19:19:35,81818.84813308716,85509.38439369202,25.741516487479256,95,2811,,deepseek/deepseek-chat
2025-01-30 19:22:01,220361.00101470947,220361.6063594818,0.0,0,20,,deepseek/deepseek-reasoner
2025-01-30 19:26:41,66067.48795509338,66067.91114807129,0.0,0,3172,,deepseek/deepseek-chat
2025-01-30 19:28:47,8300.269365310669,10585.224390029907,22.75755953069118,52,2115,,deepseek/deepseek-chat
2025-01-30 19:29:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176683 tokens (176683 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:30:59,5712.446212768555,7701.549053192139,24.13148230675609,48,317,,deepseek/deepseek-chat
2025-01-30 19:32:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 19:34:08,99494.27819252014,99495.4206943512,0.0,0,1068,,deepseek/deepseek-chat
2025-01-30 19:36:47,342007.0445537567,342007.5182914734,0.0,0,55520,,deepseek/deepseek-reasoner
2025-01-30 19:43:29,7438.953161239624,11216.708421707153,27.794283313897196,105,2542,,deepseek/deepseek-chat
2025-01-30 19:44:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 102273 tokens (102273 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:45:42,40122.05791473389,42462.45765686035,24.782091262453207,58,2939,,deepseek/deepseek-chat
2025-01-30 19:47:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 159991 tokens (159991 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:48:26,70983.66355895996,70984.19237136841,0.0,0,3740,,deepseek/deepseek-chat
2025-01-30 19:50:37,68262.44115829468,68262.9189491272,0.0,0,142,,deepseek/deepseek-chat
2025-01-30 19:52:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 546509 tokens (546509 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:53:48,9895.532131195068,11651.559352874756,29.04282995750893,51,44,,deepseek/deepseek-chat
2025-01-30 19:54:59,68633.42046737671,68633.83316993713,0.0,0,15717,,deepseek/deepseek-chat
2025-01-30 19:57:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136069 tokens (136069 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 19:58:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:00:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:02:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 375454 tokens (375454 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 20:03:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:05:13,65568.61162185669,65568.95637512207,0.0,0,12981,,deepseek/deepseek-chat
2025-01-30 20:07:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 205337 tokens (205337 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:08:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:10:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:12:21,28317.838430404663,31240.4203414917,24.97791412554391,73,619,,deepseek/deepseek-chat
2025-01-30 20:13:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:15:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 157462 tokens (157462 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:16:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:18:56,19835.23964881897,23466.994762420654,27.810245140630165,101,293,,deepseek/deepseek-chat
2025-01-30 20:20:19,37552.650451660156,38830.833196640015,24.253182983227095,31,4,,deepseek/deepseek-chat
2025-01-30 20:21:58,6817.364931106567,8568.591594696045,18.272905880961073,32,7,,deepseek/deepseek-chat
2025-01-30 20:23:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 650640 tokens (650640 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:24:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 20:26:11,11036.29732131958,15422.829627990723,24.164873888833025,106,11605,,deepseek/deepseek-chat
2025-01-30 20:27:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 553515 tokens (553515 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:28:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:30:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:32:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:34:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 448011 tokens (448011 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:35:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 135015 tokens (135015 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:36:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:38:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 267379 tokens (267379 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 20:39:40,10613.993644714355,18735.25357246399,30.78340087918771,250,826,,deepseek/deepseek-chat
2025-01-30 20:40:58,7402.977705001831,9945.781469345093,27.921934439300866,71,13,,deepseek/deepseek-chat
2025-01-30 20:42:08,5528.606653213501,7135.659456253052,27.379311940951283,44,116,,deepseek/deepseek-chat
2025-01-30 20:43:16,80542.8421497345,80544.50011253357,0.0,0,44829,,deepseek/deepseek-chat
2025-01-30 20:45:36,14124.665021896362,18129.49252128601,20.974686178818423,84,3142,,deepseek/deepseek-chat
2025-01-30 20:46:54,102488.5630607605,102490.1270866394,0.0,0,5011,,deepseek/deepseek-chat
2025-01-30 20:49:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:51:37,73915.109872818,73915.54188728333,0.0,0,3509,,deepseek/deepseek-chat
2025-01-30 20:53:51,116104.11167144775,116105.22532463074,0.0,0,5,,deepseek/deepseek-chat
2025-01-30 20:56:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 20:58:48,231326.92575454712,231327.44121551514,0.0,0,17,,deepseek/deepseek-reasoner
2025-01-30 21:03:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87043 tokens (87043 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 21:04:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 21:06:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 110229 tokens (110229 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 21:07:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:09:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 155354 tokens (155354 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 21:10:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 447536 tokens (447536 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 21:11:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:13:48,65981.23979568481,65981.76693916321,0.0,0,6022,,deepseek/deepseek-chat
2025-01-30 21:15:54,63786.29016876221,63787.057638168335,0.0,0,22,,deepseek/deepseek-chat
2025-01-30 21:17:57,357902.76169776917,357903.4814834595,0.0,0,268,,deepseek/deepseek-reasoner
2025-01-30 21:24:55,71111.66262626648,71112.25008964539,0.0,0,14655,,deepseek/deepseek-chat
2025-01-30 21:27:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:29:07,70040.44508934021,70041.05353355408,0.0,0,20576,,deepseek/deepseek-chat
2025-01-30 21:31:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:33:17,51829.86235618591,53876.64175033569,25.89433924901119,53,7,,deepseek/deepseek-chat
2025-01-30 21:35:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94908 tokens (94908 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 21:36:13,117500.75697898865,117501.8801689148,0.0,0,464,,deepseek/deepseek-reasoner
2025-01-30 21:39:10,65425.90403556824,65426.347494125366,0.0,0,14755,,deepseek/deepseek-chat
2025-01-30 21:41:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:43:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:45:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:47:17,29726.30262374878,33773.86808395386,21.49440221668261,87,29403,,deepseek/deepseek-chat
2025-01-30 21:48:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:50:52,211619.7292804718,211620.12124061584,0.0,0,207,,deepseek/deepseek-reasoner
2025-01-30 21:55:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:57:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 21:59:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118985 tokens (118985 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 22:00:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:02:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:04:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:06:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:08:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:10:30,93113.2447719574,93113.7592792511,0.0,0,809,,deepseek/deepseek-chat
2025-01-30 22:13:03,7387.606620788574,9853.515625,15.81566875882017,39,6,,deepseek/deepseek-chat
2025-01-30 22:14:13,78850.94237327576,78852.60081291199,0.0,0,35855,,deepseek/deepseek-chat
2025-01-30 22:16:32,1180722.541809082,1180724.2941856384,0.0,0,34341,,deepseek/deepseek-reasoner
2025-01-30 22:37:13,99100.426197052,99100.91137886047,0.0,0,1130,,deepseek/deepseek-chat
2025-01-30 22:39:52,22267.547369003296,24236.13405227661,29.46276152978902,58,10,,deepseek/deepseek-chat
2025-01-30 22:41:16,6671.234846115112,8899.92070198059,24.229525151731117,54,1023,,deepseek/deepseek-chat
2025-01-30 22:42:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 22:44:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 137183 tokens (137183 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 22:45:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 450190 tokens (450190 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 22:46:31,27439.722299575806,31179.18610572815,27.54405586986549,103,5166,,deepseek/deepseek-chat
2025-01-30 22:48:02,6051.1369705200195,9290.28606414795,28.711244006319458,93,440,,deepseek/deepseek-chat
2025-01-30 22:49:11,30117.16055870056,34346.118450164795,21.04537389214454,89,39973,,deepseek/deepseek-chat
2025-01-30 22:50:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:52:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:54:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 258863 tokens (258863 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-30 22:55:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:57:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 22:59:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70544 tokens (70544 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 23:00:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:02:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:04:52,61077.99434661865,63167.51670837402,24.40749184285149,51,51,,deepseek/deepseek-chat
2025-01-30 23:06:55,11348.318576812744,15123.481273651123,24.899589116708285,94,362,,deepseek/deepseek-chat
2025-01-30 23:08:10,4991.669178009033,7061.712026596069,25.120252962635053,52,3060,,deepseek/deepseek-chat
2025-01-30 23:09:17,21006.829500198364,24287.692546844482,28.041402122544415,92,650,,deepseek/deepseek-chat
2025-01-30 23:10:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-30 23:12:43,5673.2728481292725,7350.6269454956055,25.635612699498374,43,7,,deepseek/deepseek-chat
2025-01-30 23:13:50,9433.29119682312,13306.910753250122,23.492239925579508,91,11789,,deepseek/deepseek-chat
2025-01-30 23:15:03,61499.87864494324,63484.21597480774,28.221008170936287,56,10,,deepseek/deepseek-chat
2025-01-30 23:17:07,104441.74003601074,110235.78453063965,45.39143602431801,263,659,,deepseek/deepseek-reasoner
2025-01-30 23:19:57,11641.838788986206,15055.462837219238,26.95082958758193,92,2215,,deepseek/deepseek-chat
2025-01-30 23:21:12,10815.67120552063,20606.826305389404,27.473775796239327,269,23,,deepseek/deepseek-chat
2025-01-30 23:22:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 351555 tokens (351555 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 23:23:34,5763.885021209717,8272.4928855896,21.12725577901406,53,22,,deepseek/deepseek-chat
2025-01-30 23:24:43,8541.31031036377,12221.555233001709,23.911452050023627,88,552,,deepseek/deepseek-chat
2025-01-30 23:25:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 635300 tokens (635300 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 23:26:58,5969.795703887939,8257.157325744629,24.919540249054347,57,3409,,deepseek/deepseek-chat
2025-01-30 23:28:06,10335.854768753052,14246.603727340698,26.3376660303959,103,604,,deepseek/deepseek-chat
2025-01-30 23:29:20,4934.002876281738,6685.338973999023,17.12977882378054,30,4,,deepseek/deepseek-chat
2025-01-30 23:30:27,6430.719614028931,8479.200839996338,22.45566101211215,46,12437,,deepseek/deepseek-chat
2025-01-30 23:31:36,83062.8490447998,83063.23909759521,0.0,0,45211,,deepseek/deepseek-chat
2025-01-30 23:33:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:35:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:38:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:40:00,64145.8945274353,64146.36325836182,0.0,0,5,,deepseek/deepseek-chat
2025-01-30 23:42:04,29270.24221420288,30885.27822494507,47.057774250540895,76,6,,deepseek/deepseek-reasoner
2025-01-30 23:43:35,182999.51338768005,190628.85284423828,42.729780455602665,326,20,,deepseek/deepseek-reasoner
2025-01-30 23:47:46,5864.201068878174,7082.822322845459,26.259184218084435,32,4,,deepseek/deepseek-chat
2025-01-30 23:48:53,13148.61536026001,21697.535037994385,27.020958051768638,231,10027,,deepseek/deepseek-chat
2025-01-30 23:50:15,16407.606601715088,25011.735439300537,26.382682580064937,227,17484,,deepseek/deepseek-chat
2025-01-30 23:51:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-30 23:53:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 79205 tokens (79205 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-30 23:54:41,207986.8085384369,212074.66292381287,47.70228624033169,195,174,,deepseek/deepseek-reasoner
2025-01-30 23:59:13,16628.453254699707,20038.992404937744,25.80237203664942,88,2503,,deepseek/deepseek-chat
2025-01-31 00:00:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 439970 tokens (439970 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:01:36,5844.09236907959,13696.65789604187,29.92652518379027,235,2907,,deepseek/deepseek-chat
2025-01-31 00:02:49,6402.3118019104,9105.661392211914,25.153979434977824,68,440,,deepseek/deepseek-chat
2025-01-31 00:03:58,5611.146926879883,7367.81907081604,23.339585671422853,41,84,,deepseek/deepseek-chat
2025-01-31 00:05:06,5896.568536758423,7599.436283111572,28.187743941240587,48,122,,deepseek/deepseek-chat
2025-01-31 00:06:13,5395.537376403809,7412.028789520264,26.28327582019769,53,5,,deepseek/deepseek-chat
2025-01-31 00:07:21,8831.515312194824,12191.66350364685,26.78453296463337,90,324,,deepseek/deepseek-chat
2025-01-31 00:08:33,362239.590883255,364639.6882534027,46.66477343504917,112,13,,deepseek/deepseek-reasoner
2025-01-31 00:15:38,4339.678764343262,6376.744031906128,25.5269189593579,52,11,,deepseek/deepseek-chat
2025-01-31 00:16:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 319997 tokens (319997 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:17:46,218026.522397995,222852.56147384644,42.063480384104736,203,83,,deepseek/deepseek-reasoner
2025-01-31 00:22:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82774 tokens (82774 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:23:29,7518.880367279053,9569.384336471558,24.87193429822229,51,233,,deepseek/deepseek-chat
2025-01-31 00:24:39,4325.67286491394,6814.801454544067,17.275121976076615,43,13,,deepseek/deepseek-chat
2025-01-31 00:25:46,5471.118211746216,7619.734525680542,24.667037877484894,53,29,,deepseek/deepseek-chat
2025-01-31 00:26:53,11529.455184936523,15746.607065200806,21.341417751916453,90,20869,,deepseek/deepseek-chat
2025-01-31 00:28:09,7713.404893875122,10487.744092941284,18.743201991127513,52,2474,,deepseek/deepseek-chat
2025-01-31 00:29:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66250 tokens (66250 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:30:21,10808.172941207886,14558.1693649292,24.266689809185483,91,1029,,deepseek/deepseek-chat
2025-01-31 00:31:35,7796.445369720459,10853.999853134155,17.007055894534076,52,2370,,deepseek/deepseek-chat
2025-01-31 00:32:46,8378.697395324707,10693.883419036865,21.59653673091471,50,12228,,deepseek/deepseek-chat
2025-01-31 00:33:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 174093 tokens (174093 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:34:58,4981.736898422241,7154.460191726685,26.23435767253641,57,9,,deepseek/deepseek-chat
2025-01-31 00:36:05,7128.112554550171,9312.905073165894,24.258596433486794,53,730,,deepseek/deepseek-chat
2025-01-31 00:37:15,5279.550075531006,7961.3587856292725,23.491608392043577,63,91,,deepseek/deepseek-chat
2025-01-31 00:38:22,12132.647514343262,22686.224937438965,24.9204596182186,263,22099,,deepseek/deepseek-chat
2025-01-31 00:39:45,69214.62535858154,69215.13080596924,0.0,0,22952,,deepseek/deepseek-chat
2025-01-31 00:41:54,32805.75227737427,35169.74687576294,23.265704599108933,55,11,,deepseek/deepseek-chat
2025-01-31 00:43:30,18085.8211517334,21675.246715545654,24.516457699303004,88,222,,deepseek/deepseek-chat
2025-01-31 00:44:51,6269.671201705933,8393.635988235474,23.07006232436824,49,1209,,deepseek/deepseek-chat
2025-01-31 00:46:00,8426.902294158936,10574.35417175293,22.817740649396818,49,430,,deepseek/deepseek-chat
2025-01-31 00:47:10,300409.3999862671,303883.03899765015,29.36401844456137,102,127,,deepseek/deepseek-reasoner
2025-01-31 00:53:14,6300.4889488220215,8461.720943450928,23.134952714127866,50,32,,deepseek/deepseek-chat
2025-01-31 00:54:23,7341.44401550293,8741.302728652954,22.859449814040286,32,18,,deepseek/deepseek-chat
2025-01-31 00:55:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 155992 tokens (155992 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:56:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 91426 tokens (91426 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:57:35,5623.960256576538,11444.202661514282,27.833892255512136,162,2996,,deepseek/deepseek-chat
2025-01-31 00:58:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 535417 tokens (535417 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 00:59:48,6979.694366455078,10617.549896240234,25.289624408320943,92,1764,,deepseek/deepseek-chat
2025-01-31 01:00:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 353995 tokens (353995 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 01:02:01,7667.275905609131,12621.986865997314,25.22851504342984,125,4,,deepseek/deepseek-chat
2025-01-31 01:03:13,5055.954217910767,6684.928894042969,6.138830852634098,10,4,,deepseek/deepseek-chat
2025-01-31 01:04:20,12592.530012130737,14986.921310424805,24.640918149859527,59,767,,deepseek/deepseek-chat
2025-01-31 01:05:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 117601 tokens (117601 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 01:06:37,4525.490999221802,6129.5928955078125,21.819062792105516,35,25,,deepseek/deepseek-chat
2025-01-31 01:07:43,7803.878307342529,10203.51529121399,22.503403791051333,54,5080,,deepseek/deepseek-chat
2025-01-31 01:08:53,7037.367820739746,15243.841171264648,26.076974951281862,214,4290,,deepseek/deepseek-chat
2025-01-31 01:10:08,6282.03558921814,8924.298286437988,21.950883256621978,58,3746,,deepseek/deepseek-chat
2025-01-31 01:11:17,6428.683519363403,9243.3922290802,23.44825230836786,66,205,,deepseek/deepseek-chat
2025-01-31 01:12:26,7482.012510299683,9970.026016235352,21.704062245310112,54,2639,,deepseek/deepseek-chat
2025-01-31 01:13:36,91701.9407749176,91702.43573188782,0.0,0,46815,,deepseek/deepseek-chat
2025-01-31 01:16:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:18:09,66938.95435333252,66939.36920166016,0.0,0,2877,,deepseek/deepseek-chat
2025-01-31 01:20:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:22:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 667306 tokens (667306 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 01:23:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:25:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:27:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:29:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 196428 tokens (196428 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 01:30:23,4353.262424468994,5760.006427764893,22.03670314383375,31,9,,deepseek/deepseek-chat
2025-01-31 01:31:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:33:30,65183.82215499878,65184.2737197876,0.0,0,7,,deepseek/deepseek-chat
2025-01-31 01:35:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:37:36,4362.181663513184,7009.4780921936035,23.04237611592528,61,149,,deepseek/deepseek-chat
2025-01-31 01:38:43,5930.728197097778,8696.523666381836,22.41669736195246,62,458,,deepseek/deepseek-chat
2025-01-31 01:39:51,67993.54934692383,67994.56095695496,0.0,0,263,,deepseek/deepseek-chat
2025-01-31 01:41:59,264460.6854915619,271317.2941207886,28.14802629645894,193,10,,deepseek/deepseek-reasoner
2025-01-31 01:47:31,64991.2531375885,64991.74118041992,0.0,0,8777,,deepseek/deepseek-chat
2025-01-31 01:49:36,4697.491884231567,6977.6763916015625,22.36661105062252,51,105,,deepseek/deepseek-chat
2025-01-31 01:50:43,66816.80059432983,66817.36612319946,0.0,0,57,,deepseek/deepseek-chat
2025-01-31 01:52:49,38467.96417236328,40501.14464759827,23.60833215971759,48,110,,deepseek/deepseek-chat
2025-01-31 01:54:30,6355.252981185913,10127.385139465332,25.18469555513461,95,3549,,deepseek/deepseek-chat
2025-01-31 01:55:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 01:57:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 365367 tokens (365367 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 01:58:43,4764.664173126221,6631.139039993286,25.181158789933733,47,31,,deepseek/deepseek-chat
2025-01-31 01:59:49,67210.41083335876,67211.3676071167,0.0,0,17136,,deepseek/deepseek-chat
2025-01-31 02:01:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 696665 tokens (696665 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 02:02:59,70793.97177696228,70794.60000991821,0.0,0,9248,,deepseek/deepseek-chat
2025-01-31 02:05:10,5055.076837539673,6532.673597335815,22.333562781061364,33,5,,deepseek/deepseek-chat
2025-01-31 02:06:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:08:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:10:17,86576.93362236023,86577.44026184082,0.0,0,4019,,deepseek/deepseek-chat
2025-01-31 02:12:44,76172.14345932007,76173.42400550842,0.0,0,36224,,deepseek/deepseek-chat
2025-01-31 02:15:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:17:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 262995 tokens (262995 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 02:18:02,8435.85228919983,10717.203617095947,23.670181501504754,54,2644,,deepseek/deepseek-chat
2025-01-31 02:19:13,70859.17377471924,70859.62152481079,0.0,0,28727,,deepseek/deepseek-chat
2025-01-31 02:21:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:23:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:25:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:27:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:29:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:31:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70406 tokens (70406 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 02:32:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:34:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 171160 tokens (171160 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 02:35:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:37:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:39:31,213711.37809753418,213711.85541152954,0.0,0,10,,deepseek/deepseek-reasoner
2025-01-31 02:44:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:46:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:48:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:50:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 02:52:07,66184.50140953064,66184.95059013367,0.0,0,7,,deepseek/deepseek-reasoner
2025-01-31 02:54:13,8398.391008377075,10150.284051895142,22.832444108388003,40,10,,deepseek/deepseek-chat
2025-01-31 02:55:23,18466.662406921387,21459.676504135132,22.385460884521592,67,96,,deepseek/deepseek-chat
2025-01-31 02:56:44,59763.869285583496,61734.61580276489,24.356252608605708,48,70,,deepseek/deepseek-chat
2025-01-31 02:58:46,69248.47984313965,69249.2606639862,0.0,0,17940,,deepseek/deepseek-chat
2025-01-31 03:00:55,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:03:21,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-reasoner
2025-01-31 03:05:46,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:08:12,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-reasoner
2025-01-31 03:10:37,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:13:02,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:15:28,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:17:53,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:20:19,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-reasoner
2025-01-31 03:22:44,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-reasoner
2025-01-31 03:25:10,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:27:35,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:30:00,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:32:26,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-reasoner
2025-01-31 03:34:51,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:37:16,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:39:42,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:42:07,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:44:32,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:46:58,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:49:23,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:51:49,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:54:14,,,,,,litellm.APIError: APIError: DeepseekException - Connection error.,deepseek/deepseek-chat
2025-01-31 03:56:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 576595 tokens (576595 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 03:57:42,4930.011034011841,6351.241827011108,21.812080172130056,31,5,,deepseek/deepseek-chat
2025-01-31 03:58:49,7026.443243026733,15459.640741348267,27.273166559395424,230,537,,deepseek/deepseek-chat
2025-01-31 04:00:04,6350.56471824646,8399.698972702026,23.424526672974437,48,158,,deepseek/deepseek-chat
2025-01-31 04:01:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 405261 tokens (405261 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 04:02:15,5048.433065414429,7096.142292022705,21.487425767416894,44,81,,deepseek/deepseek-chat
2025-01-31 04:03:22,4964.6594524383545,6444.3678855896,22.301690833593415,33,34,,deepseek/deepseek-chat
2025-01-31 04:04:28,16134.023427963257,23866.613149642944,23.278100413802903,180,23683,,deepseek/deepseek-chat
2025-01-31 04:05:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:07:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:09:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:11:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69008 tokens (69008 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 04:12:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:14:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:16:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:18:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:20:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:22:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:24:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:26:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:29:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:31:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 04:33:14,1176567.8896903992,1176569.1502094269,0.0,0,6931,,deepseek/deepseek-reasoner
2025-01-31 04:53:51,10920.51386833191,15309.561967849731,23.239663290818264,102,17,,deepseek/deepseek-chat
2025-01-31 04:55:06,5665.3382778167725,7297.320604324341,22.05906241462784,36,13,,deepseek/deepseek-chat
2025-01-31 04:56:13,15435.97936630249,19788.989543914795,23.661787084655323,103,3678,,deepseek/deepseek-chat
2025-01-31 04:57:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 04:59:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86775 tokens (86775 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:00:34,5183.080434799194,7169.682025909424,22.651748695545624,45,17,,deepseek/deepseek-chat
2025-01-31 05:01:42,7297.854900360107,9451.523303985596,24.609173775674904,53,69,,deepseek/deepseek-chat
2025-01-31 05:02:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 05:04:52,5246.275901794434,7199.790716171265,24.571095978768884,48,30,,deepseek/deepseek-chat
2025-01-31 05:05:59,8465.608835220337,10470.094203948975,22.449652515319503,45,30,,deepseek/deepseek-chat
2025-01-31 05:07:09,5410.0401401519775,6889.254331588745,20.957073140225592,31,4,,deepseek/deepseek-chat
2025-01-31 05:08:16,11039.856672286987,13599.367618560791,21.097780448493285,54,18118,,deepseek/deepseek-chat
2025-01-31 05:09:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:11:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:13:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:15:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:17:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:19:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 187214 tokens (187214 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:20:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:22:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:24:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 165941 tokens (165941 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:25:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:27:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:29:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:31:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80219 tokens (80219 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:32:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 270954 tokens (270954 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:33:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:35:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:37:49,7983.283519744873,11434.025764465332,24.052796214202242,83,353,,deepseek/deepseek-chat
2025-01-31 05:39:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 448782 tokens (448782 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 05:40:03,9590.996026992798,12695.462942123413,18.360640186626636,57,6,,deepseek/deepseek-chat
2025-01-31 05:41:16,5163.280487060547,6447.441101074219,22.58284492105693,29,8,,deepseek/deepseek-chat
2025-01-31 05:42:22,39017.75407791138,41149.441957473755,23.924703278075576,51,242,,deepseek/deepseek-chat
2025-01-31 05:44:04,21355.79776763916,22862.37668991089,22.567685965453666,34,4,,deepseek/deepseek-chat
2025-01-31 05:45:26,9297.687768936157,13721.133470535278,21.70253835495144,96,320,,deepseek/deepseek-chat
2025-01-31 05:46:40,15292.10090637207,17762.7911567688,23.879966333507866,59,997,,deepseek/deepseek-chat
2025-01-31 05:47:58,33852.519512176514,42010.18500328064,22.923224812775434,187,21662,,deepseek/deepseek-chat
2025-01-31 05:49:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:51:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:53:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:55:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:57:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 05:59:43,150145.7371711731,150146.30651474,0.0,0,883,,deepseek/deepseek-reasoner
2025-01-31 06:03:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:05:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:07:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:09:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:11:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:13:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:15:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 06:17:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 391619 tokens (391619 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 06:18:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:20:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:22:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:24:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:26:21,83378.94868850708,83380.06591796875,0.0,0,44,,deepseek/deepseek-chat
2025-01-31 06:28:45,65176.35464668274,65177.36268043518,0.0,0,7471,,deepseek/deepseek-chat
2025-01-31 06:30:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:32:51,5931.1676025390625,7775.934219360352,22.767107566359833,42,511,,deepseek/deepseek-chat
2025-01-31 06:33:58,10835.96158027649,13374.360084533691,21.667204699245737,55,242,,deepseek/deepseek-chat
2025-01-31 06:35:12,67621.9596862793,67623.21090698242,0.0,0,10206,,deepseek/deepseek-chat
2025-01-31 06:37:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:39:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 233131 tokens (233131 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 06:40:23,38118.574380874634,47094.64240074158,25.623691742412777,230,7,,deepseek/deepseek-reasoner
2025-01-31 06:42:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:44:11,8371.710062026978,10639.050245285034,25.139588854326128,57,9,,deepseek/deepseek-chat
2025-01-31 06:45:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:47:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 164618 tokens (164618 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 06:48:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:50:24,65443.69149208069,65444.21195983887,0.0,0,28,,deepseek/deepseek-chat
2025-01-31 06:52:29,66017.13752746582,66017.59672164917,0.0,0,1028,,deepseek/deepseek-chat
2025-01-31 06:54:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 06:56:36,67036.39221191406,67036.90433502197,0.0,0,17542,,deepseek/deepseek-chat
2025-01-31 06:58:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:00:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88997 tokens (88997 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:01:46,114529.28686141968,114530.78365325928,0.0,0,34662,,deepseek/deepseek-chat
2025-01-31 07:04:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:06:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:08:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 374113 tokens (374113 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:09:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 104757 tokens (104757 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:10:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:12:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:14:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:16:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:18:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:20:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:22:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:24:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:26:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176610 tokens (176610 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:27:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:29:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:31:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 274550 tokens (274550 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:32:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:34:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 722981 tokens (722981 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 07:36:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:38:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:40:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:42:03,77126.87635421753,77128.01694869995,0.0,0,4,,deepseek/deepseek-chat
2025-01-31 07:44:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 745293 tokens (745293 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 07:45:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:47:23,304554.25691604614,304554.88634109497,0.0,0,2269,,deepseek/deepseek-reasoner
2025-01-31 07:53:28,67323.35758209229,67323.8513469696,0.0,0,7174,,deepseek/deepseek-chat
2025-01-31 07:55:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:57:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 07:59:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 608149 tokens (608149 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 08:00:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:02:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 08:04:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:06:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 310282 tokens (310282 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 08:07:44,14390.463590621948,15500.15640258789,19.825306393599675,22,4,,deepseek/deepseek-chat
2025-01-31 08:08:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:11:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:13:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:15:01,20952.46458053589,23780.45654296875,23.69172221492501,67,2993,,deepseek/deepseek-chat
2025-01-31 08:16:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:18:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:20:26,67129.52494621277,67130.20157814026,0.0,0,1862,,deepseek/deepseek-chat
2025-01-31 08:22:33,880398.6208438873,880399.6405601501,0.0,0,9825,,deepseek/deepseek-reasoner
2025-01-31 08:38:13,70875.50044059753,70876.06716156006,0.0,0,113,,deepseek/deepseek-chat
2025-01-31 08:40:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250944 tokens (250944 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 08:41:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 187730 tokens (187730 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 08:42:31,14721.639633178711,16582.72409439087,22.56748733082465,42,20,,deepseek/deepseek-chat
2025-01-31 08:43:47,52236.17172241211,57510.11610031128,18.581917627094402,98,55610,,deepseek/deepseek-chat
2025-01-31 08:45:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:47:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67966 tokens (67966 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 08:48:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:50:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:52:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 693838 tokens (693838 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 08:53:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:55:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:57:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 08:59:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:01:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 304002 tokens (304002 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:02:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:04:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:06:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:08:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94710 tokens (94710 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:09:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:11:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 09:13:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:16:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:18:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:20:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88591 tokens (88591 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:21:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:23:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:25:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 489070 tokens (489070 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:26:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:28:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:30:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 09:32:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:34:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:36:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:38:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:40:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:42:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 09:44:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 656063 tokens (656063 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:45:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:47:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 425978 tokens (425978 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:48:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 113158 tokens (113158 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 09:49:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:51:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:53:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 09:55:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:57:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 09:59:35,1712053.5306930542,1718327.4290561676,23.111627190601187,145,1348,,deepseek/deepseek-reasoner
2025-01-31 10:29:13,10080.224752426147,18251.92952156067,18.478396401488286,151,849,,deepseek/deepseek-chat
2025-01-31 10:30:31,8453.176021575928,10105.544090270996,18.15576115779823,30,6,,deepseek/deepseek-chat
2025-01-31 10:31:41,13278.813123703003,17891.56937599182,19.51111116164931,90,6304,,deepseek/deepseek-chat
2025-01-31 10:32:59,10081.245183944702,12570.087671279907,18.884280640163265,47,40,,deepseek/deepseek-chat
2025-01-31 10:34:12,35209.78093147278,40415.86709022522,19.208287560105127,100,1008,,deepseek/deepseek-chat
2025-01-31 10:35:52,80619.66609954834,80620.24116516113,0.0,0,34913,,deepseek/deepseek-chat
2025-01-31 10:38:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 10:40:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 10:42:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 10:44:15,66552.77824401855,66553.17449569702,0.0,0,354,,deepseek/deepseek-chat
2025-01-31 10:46:22,73619.63939666748,73620.17679214478,0.0,0,428,,deepseek/deepseek-chat
2025-01-31 10:48:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 10:50:36,14540.3311252594,17369.94957923889,19.08384500534197,54,1339,,deepseek/deepseek-chat
2025-01-31 10:51:53,29420.54295539856,31442.802906036377,17.30736940567955,35,18,,deepseek/deepseek-chat
2025-01-31 10:53:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 10:55:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 558934 tokens (558934 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 10:56:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 243062 tokens (243062 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 10:57:32,78244.85039710999,78246.31786346436,0.0,0,39663,,deepseek/deepseek-chat
2025-01-31 10:59:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:01:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 147204 tokens (147204 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 11:02:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118566 tokens (118566 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 11:04:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 11:06:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 11:08:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:10:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 90205 tokens (90205 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 11:11:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:13:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:15:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:17:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:19:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 11:21:19,105588.17672729492,105588.55676651001,0.0,0,13,,deepseek/deepseek-chat
2025-01-31 11:24:05,916530.0633907318,916531.2905311584,0.0,0,5438,,deepseek/deepseek-reasoner
2025-01-31 11:40:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:42:22,10119.640588760376,11729.768991470337,19.253122886239872,31,4,,deepseek/deepseek-chat
2025-01-31 11:43:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 130356 tokens (130356 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 11:44:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 135116 tokens (135116 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 11:45:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:47:37,68737.79344558716,68738.35921287537,0.0,0,1350,,deepseek/deepseek-chat
2025-01-31 11:49:46,9886.67893409729,12056.060552597046,14.289786423763546,31,5,,deepseek/deepseek-chat
2025-01-31 11:50:58,19508.882522583008,24215.19160270691,19.548227376000973,92,3726,,deepseek/deepseek-chat
2025-01-31 11:52:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:54:22,77206.40873908997,77207.65995979309,0.0,0,8988,,deepseek/deepseek-chat
2025-01-31 11:56:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 11:58:41,63586.52901649475,66525.34317970276,17.353938414509486,51,39,,deepseek/deepseek-chat
2025-01-31 12:00:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:02:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:04:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:06:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 320575 tokens (320575 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 12:07:56,36754.796743392944,41752.382040023804,16.007730784291425,80,15665,,deepseek/deepseek-chat
2025-01-31 12:09:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150335 tokens (150335 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 12:10:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:12:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460887 tokens (460887 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 12:13:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:15:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:17:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:19:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:21:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:23:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:25:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:27:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:29:56,197396.27814292908,197396.82245254517,0.0,0,10,,deepseek/deepseek-reasoner
2025-01-31 12:34:13,91414.6237373352,91415.14086723328,0.0,0,22,,deepseek/deepseek-chat
2025-01-31 12:36:45,97935.58669090271,97936.96141242981,0.0,0,15528,,deepseek/deepseek-chat
2025-01-31 12:39:23,25420.459270477295,28447.312116622925,18.50106458637722,56,243,,deepseek/deepseek-chat
2025-01-31 12:40:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:42:52,118779.53600883484,118780.78031539917,0.0,0,1925,,deepseek/deepseek-chat
2025-01-31 12:45:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:47:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 342429 tokens (342429 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 12:48:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:50:54,107189.82148170471,107190.35863876343,0.0,0,10324,,deepseek/deepseek-chat
2025-01-31 12:53:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:55:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 67216 tokens (67216 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 12:56:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 12:58:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 13:00:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:02:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:04:45,103524.64485168457,103525.64120292664,0.0,0,6,,deepseek/deepseek-reasoner
2025-01-31 13:07:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:09:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 479221 tokens (479221 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:10:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:12:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:14:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:16:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 223371 tokens (223371 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:17:35,106602.49161720276,106603.6856174469,0.0,0,11,,deepseek/deepseek-chat
2025-01-31 13:20:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 239100 tokens (239100 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:21:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:23:23,79646.04640007019,83669.22235488892,12.6765521997404,51,41028,,deepseek/deepseek-chat
2025-01-31 13:25:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:27:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69941 tokens (69941 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:28:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:30:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:32:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106248 tokens (106248 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:33:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 13:35:52,473209.6438407898,480329.5075893402,33.006249599628276,235,31,,deepseek/deepseek-reasoner
2025-01-31 13:44:52,10211.747884750366,14808.963537216187,45.027254679464704,207,7988,,deepseek/deepseek-chat
2025-01-31 13:46:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 342044 tokens (342044 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 13:47:09,11232.14602470398,13260.488271713257,46.343263883893286,94,5439,,deepseek/deepseek-chat
2025-01-31 13:48:23,29419.925928115845,32066.18595123291,26.452426968059644,70,3096,,deepseek/deepseek-chat
2025-01-31 13:49:55,51402.61006355286,59093.334913253784,32.50668888638265,250,5085,,deepseek/deepseek-chat
2025-01-31 13:51:54,51510.159969329834,53616.94431304932,23.25819448301555,49,136,,deepseek/deepseek-chat
2025-01-31 13:53:48,49380.3551197052,50717.448472976685,21.688837154896706,29,4,,deepseek/deepseek-chat
2025-01-31 13:55:38,6518.932580947876,9151.903867721558,32.66271851577247,86,128,,deepseek/deepseek-chat
2025-01-31 13:56:47,13900.561809539795,17432.164907455444,25.48417744143388,90,8451,,deepseek/deepseek-chat
2025-01-31 13:58:05,44609.06720161438,47593.27530860901,31.16404642894008,93,1703,,deepseek/deepseek-chat
2025-01-31 13:59:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 158102 tokens (158102 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:00:54,577346.0032939911,586660.4197025299,30.275648804092732,282,480,,deepseek/deepseek-reasoner
2025-01-31 14:11:40,7663.379430770874,8932.477712631226,46.48970126530533,59,5493,,deepseek/deepseek-chat
2025-01-31 14:12:49,4962.996959686279,5763.394832611084,58.720795731569275,47,111,,deepseek/deepseek-chat
2025-01-31 14:13:55,18496.841430664062,19504.59384918213,30.761523793300874,31,4,,deepseek/deepseek-chat
2025-01-31 14:15:14,27694.165468215942,30190.489530563354,23.634751949840798,59,110,,deepseek/deepseek-chat
2025-01-31 14:16:45,8814.423561096191,10862.984895706177,23.919225249523148,49,434,,deepseek/deepseek-chat
2025-01-31 14:17:55,28361.279487609863,30705.66701889038,20.047880042395693,47,15,,deepseek/deepseek-chat
2025-01-31 14:19:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 516834 tokens (516834 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:20:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 81984 tokens (81984 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 14:21:30,9022.93348312378,11602.758884429932,20.5440259535263,53,77,,deepseek/deepseek-chat
2025-01-31 14:22:41,16416.125059127808,19279.492139816284,22.351308161513,64,14,,deepseek/deepseek-chat
2025-01-31 14:24:01,6381.9451332092285,8792.261600494385,29.04182954815225,70,5,,deepseek/deepseek-chat
2025-01-31 14:25:09,45880.65004348755,54570.0261592865,23.01661216348567,200,17663,,deepseek/deepseek-chat
2025-01-31 14:27:04,44765.97356796265,46191.26319885254,21.749965289963463,31,4,,deepseek/deepseek-chat
2025-01-31 14:28:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106867 tokens (106867 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:29:52,19903.01752090454,24606.085062026978,38.91077438286696,183,47074,,deepseek/deepseek-chat
2025-01-31 14:31:16,25504.497289657593,29103.551626205444,14.170389005273615,51,330,,deepseek/deepseek-chat
2025-01-31 14:32:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93244 tokens (93244 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:33:47,6706.869125366211,8125.458717346191,37.3610523435646,53,9,,deepseek/deepseek-chat
2025-01-31 14:34:55,6558.275461196899,9231.558799743652,39.65161435436305,106,6,,deepseek/deepseek-chat
2025-01-31 14:36:04,4644.593238830566,5883.258581161499,47.63191314368514,59,211,,deepseek/deepseek-chat
2025-01-31 14:37:10,22216.36939048767,29292.330503463745,31.091182736513137,220,8022,,deepseek/deepseek-chat
2025-01-31 14:38:39,7075.517416000366,9259.791851043701,47.155246770974784,103,35,,deepseek/deepseek-chat
2025-01-31 14:39:49,6192.706108093262,6852.523326873779,15.155712393323297,10,4,,deepseek/deepseek-chat
2025-01-31 14:40:55,36420.64166069031,39475.02279281616,17.679522516699123,54,24910,,deepseek/deepseek-chat
2025-01-31 14:42:35,48970.290660858154,54577.92901992798,19.972757305016042,112,46195,,deepseek/deepseek-chat
2025-01-31 14:44:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 176488 tokens (176488 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:45:32,37286.81230545044,41088.09781074524,17.625616362326873,67,20917,,deepseek/deepseek-chat
2025-01-31 14:47:13,104714.30611610413,104714.86926078796,0.0,0,65123,,deepseek/deepseek-reasoner
2025-01-31 14:49:58,6955.983400344849,10494.94743347168,8.75962561637286,31,4,,deepseek/deepseek-chat
2025-01-31 14:51:08,7304.628133773804,8489.550352096558,41.352925316362985,49,23,,deepseek/deepseek-chat
2025-01-31 14:52:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 236822 tokens (236822 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:53:18,59175.07362365723,62060.842514038086,21.138213875453246,61,17,,deepseek/deepseek-chat
2025-01-31 14:55:20,15484.381675720215,18252.78067588806,24.562933304006105,68,14198,,deepseek/deepseek-chat
2025-01-31 14:56:39,7044.961452484131,7878.478288650513,61.18652651884727,51,2110,,deepseek/deepseek-chat
2025-01-31 14:57:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173210 tokens (173210 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 14:58:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 15:00:48,11953.89437675476,14075.75798034668,43.829395934106394,93,3675,,deepseek/deepseek-chat
2025-01-31 15:02:02,9951.370000839233,12790.691137313843,21.13181183671879,60,19340,,deepseek/deepseek-chat
2025-01-31 15:03:15,301882.2338581085,301882.6017379761,0.0,0,4139,,deepseek/deepseek-reasoner
2025-01-31 15:09:17,614561.4411830902,626757.1492195129,25.17279022121047,307,683,,deepseek/deepseek-reasoner
2025-01-31 15:20:43,48845.73173522949,53231.30464553833,22.802037965196632,100,9544,,deepseek/deepseek-chat
2025-01-31 15:22:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88049 tokens (88049 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:23:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 339030 tokens (339030 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:24:40,16549.607276916504,20049.949169158936,25.711774098256186,90,25813,,deepseek/deepseek-chat
2025-01-31 15:26:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 241718 tokens (241718 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:27:04,7585.6993198394775,11518.54395866394,22.884199165035216,90,5172,,deepseek/deepseek-chat
2025-01-31 15:28:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 236856 tokens (236856 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:29:17,4947.497129440308,9003.096103668213,26.38327918513452,107,284,,deepseek/deepseek-chat
2025-01-31 15:30:26,6450.896501541138,8368.517875671387,28.15988637198628,54,142,,deepseek/deepseek-chat
2025-01-31 15:31:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 414749 tokens (414749 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 15:32:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 264840 tokens (264840 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:33:40,11645.601034164429,13095.317125320435,35.17930187236352,51,262,,deepseek/deepseek-chat
2025-01-31 15:34:53,14017.359972000122,15969.706058502197,25.610213448161026,50,1399,,deepseek/deepseek-chat
2025-01-31 15:36:09,9001.981973648071,9569.248914718628,17.628384938363965,10,4,,deepseek/deepseek-chat
2025-01-31 15:37:19,6780.640363693237,9275.06160736084,26.85993801972614,67,8,,deepseek/deepseek-chat
2025-01-31 15:38:28,28761.45029067993,31228.562116622925,25.53593207146976,63,13,,deepseek/deepseek-chat
2025-01-31 15:39:59,38136.79027557373,40656.75663948059,21.42885745358936,54,2333,,deepseek/deepseek-chat
2025-01-31 15:41:40,10280.353546142578,13930.418729782104,29.31454497842938,107,15,,deepseek/deepseek-chat
2025-01-31 15:42:54,7613.683223724365,9903.70798110962,22.27050158978705,51,1604,,deepseek/deepseek-chat
2025-01-31 15:44:04,6687.663316726685,8385.054588317871,20.619877447107367,35,4,,deepseek/deepseek-chat
2025-01-31 15:45:12,8267.938137054443,9865.444660186768,25.664996922584645,41,7,,deepseek/deepseek-chat
2025-01-31 15:46:22,6843.845367431641,9367.751598358154,25.35751891880148,64,1669,,deepseek/deepseek-chat
2025-01-31 15:47:32,11325.191259384155,14768.512964248657,21.200458817678967,73,14403,,deepseek/deepseek-chat
2025-01-31 15:48:46,7390.968799591064,8719.129085540771,26.352241043687805,35,18,,deepseek/deepseek-chat
2025-01-31 15:49:55,54380.59091567993,58034.26718711853,16.969210021332326,62,40221,,deepseek/deepseek-chat
2025-01-31 15:51:53,9545.787334442139,13894.265413284302,22.07668942085623,96,2498,,deepseek/deepseek-chat
2025-01-31 15:53:07,19411.957263946533,25826.2882232666,29.4652709999913,189,39357,,deepseek/deepseek-chat
2025-01-31 15:54:33,28975.948810577393,31663.034915924072,21.58471955349455,58,52,,deepseek/deepseek-chat
2025-01-31 15:56:05,8425.258159637451,10138.354778289795,28.01943537648248,48,75,,deepseek/deepseek-chat
2025-01-31 15:57:15,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 208156 tokens (208156 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 15:58:16,26792.60015487671,28375.603199005127,20.846453910749993,33,4,,deepseek/deepseek-chat
2025-01-31 15:59:44,18290.647506713867,20074.762105941772,30.26711402023675,54,69,,deepseek/deepseek-chat
2025-01-31 16:01:05,25815.786123275757,27459.131002426147,31.03426471646475,51,2655,,deepseek/deepseek-chat
2025-01-31 16:02:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 88820 tokens (88820 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:03:33,52101.309299468994,61876.0142326355,24.45086594778429,239,5175,,deepseek/deepseek-chat
2025-01-31 16:05:35,33473.36268424988,35524.0535736084,26.820229360459404,55,989,,deepseek/deepseek-chat
2025-01-31 16:07:11,10417.420387268066,14412.19425201416,22.028781347700452,88,858,,deepseek/deepseek-chat
2025-01-31 16:08:25,43670.42589187622,45457.58605003357,32.453722591824615,58,678,,deepseek/deepseek-chat
2025-01-31 16:10:11,8205.372333526611,10804.141759872437,21.933457975203552,57,8,,deepseek/deepseek-chat
2025-01-31 16:11:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 321509 tokens (321509 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:12:23,23285.61520576477,27872.103691101074,20.93104568057382,96,39028,,deepseek/deepseek-chat
2025-01-31 16:13:51,8497.360229492188,9703.747987747192,28.18330985816678,34,6,,deepseek/deepseek-chat
2025-01-31 16:15:01,22190.999507904053,24446.20704650879,24.387999356381936,55,2551,,deepseek/deepseek-chat
2025-01-31 16:16:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 102465 tokens (102465 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:17:26,14572.120189666748,16027.928352355957,24.041629176846374,35,19,,deepseek/deepseek-chat
2025-01-31 16:18:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 99078 tokens (99078 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 16:19:44,9991.604328155518,11946.690082550049,27.108785320986353,53,16391,,deepseek/deepseek-chat
2025-01-31 16:20:56,13249.168872833252,19662.357091903687,25.57230419533377,164,1795,,deepseek/deepseek-chat
2025-01-31 16:22:15,50866.97864532471,55003.87144088745,26.106550335517866,108,2309,,deepseek/deepseek-chat
2025-01-31 16:24:10,12575.680017471313,16160.77446937561,26.21967182763343,94,8917,,deepseek/deepseek-chat
2025-01-31 16:25:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 174288 tokens (174288 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:26:28,6080.047845840454,9473.776578903198,55.98561786891269,190,103,,deepseek/deepseek-chat
2025-01-31 16:27:37,5065.637826919556,6754.4567584991455,17.763893712359284,30,4,,deepseek/deepseek-chat
2025-01-31 16:28:44,17567.498922348022,20473.54006767273,21.67897729230557,63,1852,,deepseek/deepseek-chat
2025-01-31 16:30:04,10858.969449996948,12901.235580444336,24.4826074597079,50,25,,deepseek/deepseek-chat
2025-01-31 16:31:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 101935 tokens (101935 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 16:32:18,13609.675884246826,16036.157369613647,20.193848704595602,49,1591,,deepseek/deepseek-chat
2025-01-31 16:33:34,6001.537799835205,7877.212047576904,21.858806266260657,41,8,,deepseek/deepseek-chat
2025-01-31 16:34:42,11378.852367401123,19407.61971473694,33.13086411555651,266,64,,deepseek/deepseek-chat
2025-01-31 16:36:02,7749.35245513916,16682.343006134033,25.411422826896292,227,1090,,deepseek/deepseek-chat
2025-01-31 16:37:18,656247.4248409271,660693.911075592,28.33698191117761,126,1138,,deepseek/deepseek-reasoner
2025-01-31 16:49:19,36155.02953529358,40186.4755153656,23.316695911257305,94,208,,deepseek/deepseek-chat
2025-01-31 16:50:59,15926.750659942627,17395.850658416748,23.14342116623379,34,4,,deepseek/deepseek-chat
2025-01-31 16:52:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 623089 tokens (623089 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:53:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 83853 tokens (83853 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:54:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 487042 tokens (487042 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 16:55:22,39105.41248321533,47143.43619346619,30.106902980564552,242,51644,,deepseek/deepseek-chat
2025-01-31 16:57:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 16:59:10,34345.58725357056,38403.86652946472,27.105083835257638,110,2172,,deepseek/deepseek-chat
2025-01-31 17:00:48,11350.406169891357,16987.79582977295,55.34476394639594,312,9039,,deepseek/deepseek-chat
2025-01-31 17:02:05,7199.890613555908,9820.014953613281,25.95296679641967,68,1806,,deepseek/deepseek-chat
2025-01-31 17:03:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 17:05:16,25523.96869659424,28931.682109832764,26.410671639921848,90,355,,deepseek/deepseek-chat
2025-01-31 17:06:45,4150.347471237183,5588.655471801758,63.2687852422986,91,460,,deepseek/deepseek-chat
2025-01-31 17:07:50,4646.187305450439,6114.435911178589,61.297521175146116,90,1412,,deepseek/deepseek-chat
2025-01-31 17:08:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 758084 tokens (758084 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 17:10:02,169000.422000885,169000.91361999512,0.0,0,342,,deepseek/deepseek-reasoner
2025-01-31 17:13:51,8747.208833694458,13221.74859046936,41.792007706907334,187,2757,,deepseek/deepseek-chat
2025-01-31 17:15:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 17:17:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:19:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:21:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 719841 tokens (719841 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 17:22:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 17:24:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:26:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:28:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80291 tokens (80291 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 17:29:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 17:31:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:33:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:35:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:37:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:39:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:41:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:43:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-01-31 17:45:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:47:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:49:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 109922 tokens (109922 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 17:50:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:52:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:54:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:56:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 272853 tokens (272853 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 17:57:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 17:59:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 525913 tokens (525913 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 18:00:31,16289.766788482666,19979.55322265625,17.345177327135556,64,45,,deepseek/deepseek-chat
2025-01-31 18:01:51,17473.453760147095,29789.958953857422,23.30206462678734,287,602,,deepseek/deepseek-chat
2025-01-31 18:03:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 451785 tokens (451785 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-01-31 18:04:23,12028.593063354492,13796.797752380371,17.53190690670445,31,4,,deepseek/deepseek-chat
2025-01-31 18:05:37,10505.686044692993,17306.309700012207,19.851120550452404,135,4653,,deepseek/deepseek-chat
2025-01-31 18:06:54,76471.31586074829,76471.65155410767,0.0,0,4,,deepseek/deepseek-chat
2025-01-31 18:09:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 18:11:12,66293.76006126404,68734.89427566528,16.385825803441573,40,6,,deepseek/deepseek-chat
2025-01-31 18:13:21,21740.767002105713,25709.753274917603,20.15628034493623,80,2211,,deepseek/deepseek-chat
2025-01-31 18:14:46,42760.61034202576,47204.12874221802,21.379454622240242,95,991,,deepseek/deepseek-chat
2025-01-31 18:16:34,78548.21753501892,78548.6626625061,0.0,0,41147,,deepseek/deepseek-chat
2025-01-31 18:18:52,299557.8463077545,309563.78173828125,26.384339758437008,264,125,,deepseek/deepseek-reasoner
2025-01-31 18:25:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 18:27:02,13528.162240982056,18600.727558135986,24.83950272141492,126,5,,deepseek/deepseek-chat
2025-01-31 18:28:21,68495.67103385925,68496.86765670776,0.0,0,7016,,deepseek/deepseek-chat
2025-01-31 18:30:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 18:32:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 18:34:31,65715.40594100952,65715.91997146606,0.0,0,134,,deepseek/deepseek-chat
2025-01-31 18:36:36,70890.56944847107,70891.80302619934,0.0,0,8198,,deepseek/deepseek-chat
2025-01-31 18:38:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-01-31 18:40:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 399813 tokens (399813 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 18:41:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250582 tokens (250582 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-01-31 18:42:52,10721.53902053833,15713.098764419556,21.03550901673801,105,86,,deepseek/deepseek-chat
2025-01-31 18:44:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:33:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 77199 tokens (77199 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 09:34:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:36:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:38:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:40:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:42:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 252310 tokens (252310 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 09:43:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:45:53,131562.0036125183,131563.31276893616,0.0,0,539,,deepseek/deepseek-reasoner
2025-02-03 09:49:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:51:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 330507 tokens (330507 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 09:52:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 278792 tokens (278792 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 09:53:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:55:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 09:57:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 668864 tokens (668864 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 09:58:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:00:37,641664.2196178436,648984.4913482666,16.52944104480784,121,136,,deepseek/deepseek-reasoner
2025-02-03 10:12:26,834406.578540802,850008.3587169647,17.8184794851003,278,202,,deepseek/deepseek-reasoner
2025-02-03 10:27:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:29:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:31:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:33:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:35:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:37:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:39:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:41:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:43:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:45:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 678682 tokens (678682 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 10:46:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 717698 tokens (717698 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 10:48:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 10:50:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:52:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:54:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:56:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 10:58:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 602737 tokens (602737 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 10:59:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 11:01:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:03:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:05:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:07:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 312295 tokens (312295 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:08:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70285 tokens (70285 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:09:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:11:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:13:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 210179 tokens (210179 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:14:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:16:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:18:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 96631 tokens (96631 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:19:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:21:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:23:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 385983 tokens (385983 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:24:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:26:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:28:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:30:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:32:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:34:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 77615 tokens (77615 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-03 11:35:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 556947 tokens (556947 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:36:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:38:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:40:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:42:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:44:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:46:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:48:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 540600 tokens (540600 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 11:49:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:51:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 11:53:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:55:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 11:57:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 11:59:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:01:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:03:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:05:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:07:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:09:48,82174.09086227417,82174.99923706055,0.0,0,6,,deepseek/deepseek-reasoner
2025-02-03 12:12:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:14:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:16:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:18:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:20:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 573617 tokens (573617 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 12:21:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:23:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:25:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:27:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 174487 tokens (174487 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 12:28:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:30:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 12:32:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:34:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:36:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:38:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:40:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:42:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:44:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:46:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 594013 tokens (594013 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 12:47:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:49:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 741473 tokens (741473 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 12:50:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:52:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:54:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:56:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 101525 tokens (101525 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 12:57:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 12:59:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:01:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:03:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:05:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:07:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:09:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:11:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:13:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:15:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:17:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:19:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:21:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:23:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:25:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:27:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:29:45,1031488.63697052,1051231.892824173,19.04447791119677,376,49181,,deepseek/deepseek-reasoner
2025-02-03 13:48:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 13:50:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 13:52:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:54:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 234867 tokens (234867 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 13:55:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 13:57:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 13:59:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:01:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:03:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:05:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:07:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:09:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:11:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:13:25,139950.4508972168,139950.98495483398,0.0,0,2500,,deepseek/deepseek-reasoner
2025-02-03 14:16:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:18:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:20:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:22:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 14:24:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:26:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:28:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:30:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:32:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:34:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:36:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:38:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151556 tokens (151556 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 14:40:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460193 tokens (460193 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 14:41:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:43:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:45:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:47:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 447075 tokens (447075 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 14:48:10,113571.75350189209,113572.31998443604,0.0,0,20863,,deepseek/deepseek-chat
2025-02-03 14:51:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:53:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 162560 tokens (162560 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 14:54:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:56:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 14:58:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:00:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:02:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:04:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:06:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:08:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:10:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:12:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:14:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:16:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:18:28,125099.0686416626,125099.96604919434,0.0,0,6,,deepseek/deepseek-reasoner
2025-02-03 15:21:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:23:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:25:34,100256.78753852844,100257.40790367126,0.0,0,83,,deepseek/deepseek-chat
2025-02-03 15:28:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:30:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:32:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 115573 tokens (115573 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 15:33:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:35:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74926 tokens (74926 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 15:36:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:38:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:40:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:42:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:44:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:46:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:48:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:50:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:52:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:54:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 689633 tokens (689633 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 15:55:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 15:57:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 15:59:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:01:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:03:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:05:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:07:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:09:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:11:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:13:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:15:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:17:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 309136 tokens (309136 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 16:18:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:20:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:22:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:24:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:26:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:28:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:30:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:32:53,100153.84244918823,100154.2809009552,0.0,0,240,,deepseek/deepseek-chat
2025-02-03 16:35:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:37:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 145375 tokens (145375 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 16:38:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:40:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:42:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:44:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 580973 tokens (580973 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 16:45:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:47:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:49:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:51:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:53:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:55:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 16:57:54,324635.2355480194,324636.13080978394,0.0,0,134,,deepseek/deepseek-reasoner
2025-02-03 17:04:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 132185 tokens (132185 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 17:05:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 17:07:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:09:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:11:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:13:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:15:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 114759 tokens (114759 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 17:16:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:18:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:20:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:22:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:24:28,240401.50690078735,245833.03904533386,28.16884737644765,153,299,,deepseek/deepseek-reasoner
2025-02-03 17:29:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:31:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:33:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:35:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:37:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:39:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:41:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:43:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:45:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 17:47:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:49:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 119816 tokens (119816 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 17:50:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:52:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:54:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:56:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 17:58:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:00:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:02:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:04:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:06:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 18:08:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:10:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:12:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 18:14:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 108043 tokens (108043 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-03 18:15:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-03 18:17:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:19:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:21:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95443 tokens (95443 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:22:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:24:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:26:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 196808 tokens (196808 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:28:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:30:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:32:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:34:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 178484 tokens (178484 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:35:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146321 tokens (146321 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:36:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:38:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:40:09,251988.09146881104,263121.4334964752,34.76045189650874,387,1231,,deepseek/deepseek-reasoner
2025-02-03 18:45:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:47:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80594 tokens (80594 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:48:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:50:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 163706 tokens (163706 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:51:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128086 tokens (128086 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:52:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 290943 tokens (290943 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 18:53:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:55:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:57:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 18:59:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:01:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:03:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:05:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 201894 tokens (201894 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:06:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:08:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:10:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:12:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:14:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:16:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:18:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 256158 tokens (256158 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:19:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:21:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 277975 tokens (277975 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:22:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:24:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:26:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 267532 tokens (267532 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:28:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 625649 tokens (625649 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-03 19:29:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:31:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:33:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66255 tokens (66255 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:34:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:36:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:38:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:40:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:42:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69414 tokens (69414 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:43:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 94031 tokens (94031 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:44:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 449756 tokens (449756 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:45:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:47:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:49:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:51:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 80068 tokens (80068 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 19:52:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:54:19,87843.88470649719,94839.92981910706,37.02091622210544,259,47,,deepseek/deepseek-reasoner
2025-02-03 19:56:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 19:58:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:00:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:02:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:04:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:06:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:08:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:10:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 301355 tokens (301355 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 20:12:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:14:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 271826 tokens (271826 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 20:15:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:17:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:19:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:21:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:23:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:25:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:27:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:29:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:31:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 85628 tokens (85628 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 20:32:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:34:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:36:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:38:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:40:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:42:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:44:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:46:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:48:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:50:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 174339 tokens (174339 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 20:51:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:53:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 20:55:21,695100.4614830017,698423.0573177338,46.34930267178254,154,1451,,deepseek/deepseek-reasoner
2025-02-03 21:07:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:10:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:12:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:14:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:16:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:18:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:20:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:22:04,159678.23672294617,159678.69067192078,0.0,0,1434,,deepseek/deepseek-reasoner
2025-02-03 21:25:44,108487.92004585266,108488.86346817017,0.0,0,9,,deepseek/deepseek-reasoner
2025-02-03 21:28:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:30:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 347492 tokens (347492 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 21:31:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:33:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 68794 tokens (68794 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-03 21:34:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:36:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:38:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 251905 tokens (251905 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 21:39:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:41:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:43:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:45:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:47:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 672711 tokens (672711 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 21:48:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:50:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 733002 tokens (733002 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 21:51:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:53:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:55:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:57:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 21:59:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:01:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:03:58,93639.89734649658,99907.0827960968,51.21916410188187,321,41,,deepseek/deepseek-reasoner
2025-02-03 22:06:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:08:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:10:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:12:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:14:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:16:41,20291.066884994507,21645.8957195282,54.61944572907582,74,6,,deepseek/deepseek-reasoner
2025-02-03 22:18:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:20:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:22:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:24:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:26:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:28:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:30:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:32:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:34:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:36:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:38:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:40:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:42:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199232 tokens (199232 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 22:43:12,96613.16704750061,96613.74497413635,0.0,0,200,,deepseek/deepseek-chat
2025-02-03 22:45:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:47:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:49:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:51:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69256 tokens (69256 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 22:52:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:54:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:56:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 22:58:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:00:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:02:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:04:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:06:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69490 tokens (69490 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 23:07:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:09:58,60584.99455451965,62172.49941825867,52.913223712686296,84,9,,deepseek/deepseek-reasoner
2025-02-03 23:12:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:14:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:16:02,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 107175 tokens (107175 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-03 23:17:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:19:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:21:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:23:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:25:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:27:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:29:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:31:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:33:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:35:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:37:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:39:11,965278.2969474792,969426.0349273682,49.665625215196705,206,6180,,deepseek/deepseek-reasoner
2025-02-03 23:56:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-03 23:58:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 741415 tokens (741415 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-03 23:59:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:01:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:03:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:05:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:07:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 95020 tokens (95020 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 00:08:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:10:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:12:30,300732.0864200592,300733.10470581055,0.0,0,702,,deepseek/deepseek-reasoner
2025-02-04 00:18:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74293 tokens (74293 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 00:19:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:21:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:23:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:25:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:27:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:29:34,156983.00743103027,156983.5238456726,0.0,0,3377,,deepseek/deepseek-reasoner
2025-02-04 00:33:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:35:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:37:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:39:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 00:41:14,56898.23842048645,58336.56668663025,24.333805309851865,35,15,,deepseek/deepseek-chat
2025-02-04 00:43:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:45:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:47:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:49:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:51:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:53:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:55:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 00:57:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 184797 tokens (184797 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 00:58:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:00:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:02:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 177063 tokens (177063 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 01:03:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:05:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:07:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:09:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:11:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:13:24,599688.2112026215,599689.4519329071,0.0,0,9046,,deepseek/deepseek-reasoner
2025-02-04 01:24:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:26:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:28:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:30:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:32:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:34:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:36:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 392433 tokens (392433 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 01:37:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 01:39:32,130923.16579818726,130923.58183860779,0.0,0,26,,deepseek/deepseek-reasoner
2025-02-04 01:42:43,361495.9239959717,369110.09430885315,29.5501664336757,225,10,,deepseek/deepseek-reasoner
2025-02-04 01:49:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:51:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:53:53,136315.40369987488,136315.88554382324,0.0,0,11,,deepseek/deepseek-reasoner
2025-02-04 01:57:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 01:59:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:01:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:03:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:05:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:07:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:09:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:11:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 84409 tokens (84409 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 02:12:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:14:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:16:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:18:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:20:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:22:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:24:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:26:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:28:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:30:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:32:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:34:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:36:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 523054 tokens (523054 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 02:37:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:39:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:41:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:43:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:45:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:47:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:49:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:51:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:53:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 348388 tokens (348388 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 02:54:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 135417 tokens (135417 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 02:55:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:57:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 02:59:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82601 tokens (82601 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:00:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:02:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:04:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:06:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:08:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 287310 tokens (287310 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:09:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:11:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66315 tokens (66315 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:12:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 113454 tokens (113454 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:13:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:15:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173573 tokens (173573 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:16:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 266284 tokens (266284 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:17:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:19:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:21:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93313 tokens (93313 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:22:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:24:58,92033.85710716248,92034.37304496765,0.0,0,8,,deepseek/deepseek-reasoner
2025-02-04 03:27:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:29:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 373218 tokens (373218 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:30:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:32:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 133346 tokens (133346 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:33:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:35:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:37:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150358 tokens (150358 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:38:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:40:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:42:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:44:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:46:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:48:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:50:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 90101 tokens (90101 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 03:51:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:53:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:55:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:57:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 03:59:47,95116.3399219513,95116.71781539917,0.0,0,884,,deepseek/deepseek-reasoner
2025-02-04 04:02:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:04:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:06:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:08:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:10:24,382102.4763584137,382102.98442840576,0.0,0,104,,deepseek/deepseek-reasoner
2025-02-04 04:17:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:19:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:21:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:23:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:25:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:27:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:29:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:31:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:33:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 04:35:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:37:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:39:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:41:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:43:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 583371 tokens (583371 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 04:44:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:46:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 402701 tokens (402701 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 04:48:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:50:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:52:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:54:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 04:56:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 04:58:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:00:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 381781 tokens (381781 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:01:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:03:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:05:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:07:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:09:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:11:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:13:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:15:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:17:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:19:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:21:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:23:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:25:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:27:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:29:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:31:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:33:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:35:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250786 tokens (250786 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:36:23,189457.18383789062,189457.7341079712,0.0,0,30,,deepseek/deepseek-reasoner
2025-02-04 05:40:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:42:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:44:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 90617 tokens (90617 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:45:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:47:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:49:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 358502 tokens (358502 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:50:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:52:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103264 tokens (103264 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:53:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:55:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 05:57:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 127162 tokens (127162 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 05:58:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:00:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:02:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:04:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:06:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:11:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:13:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:15:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:17:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:19:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:21:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:23:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:25:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 207106 tokens (207106 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 06:26:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:28:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:30:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:32:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:34:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:36:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:38:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 76886 tokens (76886 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 06:40:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:42:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:44:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 06:46:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118026 tokens (118026 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 06:47:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 127546 tokens (127546 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 06:48:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:50:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:52:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:54:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:56:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 06:58:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:00:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:02:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:04:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:06:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:08:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 07:10:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:12:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:14:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:16:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69386 tokens (69386 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 07:17:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:19:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:21:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:23:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:25:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:27:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:29:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 106522 tokens (106522 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 07:30:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 215486 tokens (215486 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 07:31:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:33:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:35:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:37:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:39:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 07:41:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:43:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:45:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:47:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:49:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:51:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:53:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:55:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:57:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 07:59:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:01:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:03:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:05:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:07:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 211046 tokens (211046 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 08:08:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:10:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 758005 tokens (758005 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 08:11:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 307555 tokens (307555 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 08:12:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:14:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:16:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:18:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:20:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:22:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:24:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:26:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:28:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:30:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:32:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:34:50,1125427.1667003632,1125427.587032318,0.0,0,43024,,deepseek/deepseek-reasoner
2025-02-04 08:54:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:56:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 77399 tokens (77399 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 08:57:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 08:59:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:01:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 09:03:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 486771 tokens (486771 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 09:04:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:06:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 139709 tokens (139709 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 09:07:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:09:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:11:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:13:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 151186 tokens (151186 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 09:14:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:16:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:18:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:20:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:22:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:24:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 138905 tokens (138905 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 09:25:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:27:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 09:29:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:31:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:33:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:35:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:37:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:39:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:41:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:43:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:45:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:47:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:49:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:51:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:53:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:56:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 09:58:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:00:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:02:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:04:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:06:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:08:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:10:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 10:12:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 10:14:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:16:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:18:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:20:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:22:07,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 454340 tokens (454340 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 10:23:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 10:25:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:27:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:29:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:31:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:33:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:35:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:37:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:39:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:41:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:43:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:45:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:47:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:49:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118453 tokens (118453 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-04 10:50:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:52:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 593562 tokens (593562 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 10:53:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:55:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 10:57:25,1585430.0181865692,1585431.191444397,0.0,0,3878,,deepseek/deepseek-reasoner
2025-02-04 11:24:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:26:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:28:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 243933 tokens (243933 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 11:29:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:31:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:33:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:35:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:37:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:39:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:41:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 540319 tokens (540319 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 11:43:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:45:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:47:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:49:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:51:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 117528 tokens (117528 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 11:52:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:54:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 11:56:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 11:58:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:00:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:02:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:04:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:06:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:08:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:10:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:12:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150036 tokens (150036 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 12:13:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:15:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 242666 tokens (242666 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 12:16:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 12:18:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:20:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:22:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:24:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:26:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:28:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:30:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:32:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 147980 tokens (147980 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-04 12:33:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 12:35:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:37:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 150211 tokens (150211 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 12:38:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:40:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:42:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 12:44:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 524106 tokens (524106 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 12:45:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 12:47:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:49:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:51:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:53:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:55:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:57:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 12:59:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:01:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:03:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:05:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:07:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:09:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:11:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:13:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 213107 tokens (213107 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:14:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 222533 tokens (222533 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:15:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:17:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:19:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 162875 tokens (162875 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:20:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:22:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:24:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66669 tokens (66669 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:25:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 76938 tokens (76938 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:26:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 597765 tokens (597765 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:27:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:29:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:31:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 207264 tokens (207264 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:32:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:34:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 206104 tokens (206104 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 13:35:58,378225.01826286316,378225.9695529938,0.0,0,1164,,deepseek/deepseek-reasoner
2025-02-04 13:43:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:45:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:47:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:49:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:51:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:53:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:55:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:57:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 13:59:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 440944 tokens (440944 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:00:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:02:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:04:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:06:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 14:08:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 14:10:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:12:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:14:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 14:16:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 200097 tokens (200097 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:17:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:19:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:21:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:23:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:25:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 14:27:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:29:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:31:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:33:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:35:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:37:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 205451 tokens (205451 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:38:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 435470 tokens (435470 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:39:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:41:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 14:43:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:45:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:47:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:49:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:51:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:53:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 91538 tokens (91538 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:54:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:56:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 180741 tokens (180741 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 14:57:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 14:59:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 234650 tokens (234650 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 15:00:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:02:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:04:50,1121980.0822734833,1121981.9645881653,0.0,0,31479,,deepseek/deepseek-reasoner
2025-02-04 15:24:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:26:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:28:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:30:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86578 tokens (86578 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 15:31:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:33:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 96755 tokens (96755 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 15:34:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:36:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:38:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 107606 tokens (107606 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 15:39:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:41:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:43:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:45:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:47:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:49:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:51:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:53:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:55:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:57:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 15:59:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:01:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:03:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:05:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:07:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:09:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 250412 tokens (250412 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 16:10:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:12:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:14:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:16:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:18:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74363 tokens (74363 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 16:19:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:21:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:23:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 551075 tokens (551075 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 16:24:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 16:26:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:28:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:31:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:33:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:35:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:37:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:39:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:41:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:43:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:45:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:47:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:49:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:51:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:53:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:55:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:57:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 16:59:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:01:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:03:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:05:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:07:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 17:09:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:11:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:13:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:15:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:17:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:19:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:21:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:23:22,894449.57447052,894450.4301548004,0.0,0,2552,,deepseek/deepseek-reasoner
2025-02-04 17:39:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 186651 tokens (186651 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-04 17:40:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 159831 tokens (159831 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 17:41:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:43:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 121009 tokens (121009 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 17:44:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:46:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70703 tokens (70703 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 17:47:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:49:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:51:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:53:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:55:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:57:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 17:59:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 255786 tokens (255786 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 18:00:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:02:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:04:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:06:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:08:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:10:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:12:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:14:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 18:16:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:18:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:20:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 129630 tokens (129630 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 18:21:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:23:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 18:25:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 101024 tokens (101024 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 18:26:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:28:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:30:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:32:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:34:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:36:41,229675.85444450378,229676.46479606628,0.0,0,12,,deepseek/deepseek-reasoner
2025-02-04 18:41:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 18:43:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:45:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 18:47:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:49:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:51:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:53:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:55:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:57:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 18:59:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:01:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:03:39,1141954.5063972473,1141954.899072647,0.0,0,1676,,deepseek/deepseek-reasoner
2025-02-04 19:23:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:25:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:27:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:29:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:31:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:33:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:35:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:37:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 19:39:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:41:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:43:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:45:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:48:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 317733 tokens (317733 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 19:49:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:51:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:53:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:55:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 19:57:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 601030 tokens (601030 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 19:58:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:00:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:02:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:04:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 112601 tokens (112601 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:05:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:07:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 20:09:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:11:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 20:13:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:15:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 20:17:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 249571 tokens (249571 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-04 20:18:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 362924 tokens (362924 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:19:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:21:35,192177.78038978577,201373.62146377563,38.604407921327144,355,33,,deepseek/deepseek-reasoner
2025-02-04 20:25:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:27:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:29:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:31:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:33:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:36:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 721165 tokens (721165 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:37:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:39:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:41:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:43:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 89278 tokens (89278 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:44:05,156306.52594566345,158607.0716381073,43.033268291590964,99,2220,,deepseek/deepseek-reasoner
2025-02-04 20:47:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 20:49:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71215 tokens (71215 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:50:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:52:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:54:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:56:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 397667 tokens (397667 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 20:57:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 20:59:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 292354 tokens (292354 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 21:00:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:02:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:04:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:06:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:08:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128422 tokens (128422 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 21:09:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:11:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:13:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 141723 tokens (141723 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 21:14:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:16:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:18:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:21:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:23:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:25:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:27:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:29:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:31:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:33:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:35:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:37:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:39:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:41:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:43:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:45:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:47:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:49:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:51:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:53:09,17787.51540184021,18667.61612892151,48.858043945267525,43,7,,deepseek/deepseek-reasoner
2025-02-04 21:54:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:56:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 21:58:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:00:29,63560.09602546692,74292.8957939148,45.095409440401184,484,216,,deepseek/deepseek-reasoner
2025-02-04 22:02:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:04:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:06:45,517433.6769580841,517434.69500541687,0.0,0,777,,deepseek/deepseek-reasoner
2025-02-04 22:16:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:18:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 22:20:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:22:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:24:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128002 tokens (128002 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 22:25:26,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 179562 tokens (179562 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 22:26:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:28:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:30:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:32:29,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 124544 tokens (124544 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 22:33:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:35:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 409215 tokens (409215 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 22:36:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:38:34,41797.67370223999,49160.75563430786,44.13912584410506,325,8,,deepseek/deepseek-reasoner
2025-02-04 22:40:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:42:25,568890.2897834778,573070.0993537903,49.28454192342513,206,27842,,deepseek/deepseek-reasoner
2025-02-04 22:52:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 173608 tokens (173608 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 22:53:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:55:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 22:58:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:00:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:02:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:04:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:06:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 354687 tokens (354687 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:07:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 23:09:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:11:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 454716 tokens (454716 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:12:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:14:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 542904 tokens (542904 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:15:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 331364 tokens (331364 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-04 23:16:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-04 23:18:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:20:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:22:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 755746 tokens (755746 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:23:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:25:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:27:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 471228 tokens (471228 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:28:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:30:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:32:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 142913 tokens (142913 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:33:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:35:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:37:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:39:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:41:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:43:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 573002 tokens (573002 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:44:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 326089 tokens (326089 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:45:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:47:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:49:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:51:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:53:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:55:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 140006 tokens (140006 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-04 23:56:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-04 23:58:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:00:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:02:39,114033.12873840332,114036.0746383667,0.0,0,14,,deepseek/deepseek-chat
2025-02-05 00:05:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 71603 tokens (71603 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 00:06:34,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 113337 tokens (113337 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 00:07:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:09:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 188755 tokens (188755 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 00:10:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 323563 tokens (323563 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 00:12:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:14:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:16:01,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136922 tokens (136922 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 00:17:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:19:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:21:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:23:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:25:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:27:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:29:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:31:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:33:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:35:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:37:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:39:09,45827.011585235596,56640.22397994995,38.00905642072677,411,7,,deepseek/deepseek-reasoner
2025-02-05 00:41:06,111835.68120002747,111836.16995811462,0.0,0,50400,,deepseek/deepseek-reasoner
2025-02-05 00:43:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:45:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 00:47:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 00:49:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:52:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:54:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:56:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 00:58:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:00:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:02:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:04:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 402428 tokens (402428 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:05:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:07:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:09:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:11:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:13:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:15:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:17:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 262258 tokens (262258 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:18:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:20:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 305929 tokens (305929 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:21:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 84723 tokens (84723 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:22:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:24:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:26:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:28:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:30:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:32:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:34:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:36:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:38:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 01:40:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 86157 tokens (86157 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:41:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:43:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:45:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:47:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:49:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:51:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 734443 tokens (734443 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 01:52:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:54:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:56:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 01:58:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:00:28,917408.8816642761,917409.9006652832,0.0,0,9957,,deepseek/deepseek-reasoner
2025-02-05 02:16:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:18:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:20:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:22:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:24:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:26:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:28:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:30:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:32:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:34:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:36:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:38:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:40:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:42:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 346428 tokens (346428 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 02:43:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:45:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:47:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:49:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:51:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:53:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 518558 tokens (518558 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 02:55:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:57:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 02:59:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:01:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:03:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:05:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:07:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:09:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:11:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:13:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:15:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:17:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:19:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:21:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:23:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:25:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:27:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:29:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:31:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:33:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:35:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:37:17,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 102130 tokens (102130 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 03:38:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:40:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 242214 tokens (242214 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 03:41:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:43:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:45:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:47:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:49:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:51:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 03:53:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:55:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:57:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 03:59:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:01:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:03:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74647 tokens (74647 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:04:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:06:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 128231 tokens (128231 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:07:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:09:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:11:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:13:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:15:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:17:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:19:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:21:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 230730 tokens (230730 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:22:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:24:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:26:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:28:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 232920 tokens (232920 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:29:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 313127 tokens (313127 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:30:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:32:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:34:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:36:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:38:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118410 tokens (118410 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:39:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:41:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:43:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:45:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:47:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 237550 tokens (237550 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:48:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:50:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 332996 tokens (332996 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:51:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:53:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 501903 tokens (501903 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 04:54:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:56:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 04:58:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:00:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:02:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:04:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:06:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 111842 tokens (111842 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 05:07:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:09:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:11:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:14:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:16:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:18:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:20:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:22:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:24:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:26:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:28:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:30:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 460437 tokens (460437 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 05:31:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:33:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:35:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:37:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:39:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:41:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:43:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:45:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:47:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:49:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:51:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:53:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:55:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:57:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 05:59:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:01:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:03:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 257085 tokens (257085 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 06:04:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:06:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 277569 tokens (277569 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:07:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:09:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:11:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 516301 tokens (516301 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:12:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:14:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:16:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:18:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:20:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 220484 tokens (220484 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:21:30,123146.53658866882,123147.4301815033,0.0,0,89,,deepseek/deepseek-chat
2025-02-05 06:24:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:26:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:28:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:30:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:32:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 659570 tokens (659570 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:33:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:35:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:37:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:39:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:41:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:43:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:45:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 118503 tokens (118503 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:46:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:48:42,287612.9310131073,287614.22753334045,0.0,0,95,,deepseek/deepseek-reasoner
2025-02-05 06:54:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 595702 tokens (595702 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:55:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78868 tokens (78868 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 06:56:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 06:58:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:00:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 07:02:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 347260 tokens (347260 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:03:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:05:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 343816 tokens (343816 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:06:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:08:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:10:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:12:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 74798 tokens (74798 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:13:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 102448 tokens (102448 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:14:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 591308 tokens (591308 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:15:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:17:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:19:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:21:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 191608 tokens (191608 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:22:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82564 tokens (82564 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 07:23:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 506771 tokens (506771 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:24:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:26:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:28:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:30:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:32:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:34:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:36:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:38:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 520527 tokens (520527 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 07:39:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:41:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:44:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:46:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:48:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:50:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 07:52:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:54:02,100435.07361412048,100436.25688552856,0.0,0,11808,,deepseek/deepseek-chat
2025-02-05 07:56:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 07:58:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:00:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:02:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:04:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:06:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:08:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:10:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:12:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:14:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 175705 tokens (175705 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 08:15:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:17:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:19:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 241529 tokens (241529 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 08:20:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:22:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:24:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:26:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:28:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 08:30:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 117427 tokens (117427 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 08:32:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 643817 tokens (643817 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 08:33:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 08:35:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:37:04,341991.5668964386,341992.8228855133,0.0,0,1927,,deepseek/deepseek-reasoner
2025-02-05 08:43:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:45:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 08:48:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:50:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:52:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:54:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:56:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 08:58:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:00:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:02:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:04:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:06:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 260805 tokens (260805 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 09:07:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:09:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:11:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:13:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:15:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:17:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:19:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:21:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:23:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:25:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:27:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:29:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 444084 tokens (444084 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 09:30:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:32:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:34:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 09:36:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:38:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:40:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:42:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:44:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:46:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:48:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:50:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:52:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:54:28,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 421480 tokens (421480 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 09:55:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:57:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 09:59:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:01:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:03:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 10:05:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:07:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 10:09:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:11:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:13:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:15:35,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 741556 tokens (741556 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 10:16:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:18:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:20:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:22:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:24:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:26:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:28:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:30:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:32:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:34:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:36:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:38:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:40:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:42:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:44:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:46:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:48:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:50:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:52:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 141533 tokens (141533 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 10:53:51,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 192129 tokens (192129 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 10:54:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 10:56:53,282645.9918022156,282646.59214019775,0.0,0,447,,deepseek/deepseek-reasoner
2025-02-05 11:02:36,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 389160 tokens (389160 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:03:38,6156.365394592285,6995.346307754517,38.141511324003446,32,4,,deepseek/deepseek-chat
2025-02-05 11:04:45,177007.36165046692,183195.76382637024,16.80563044285646,104,349,,deepseek/deepseek-reasoner
2025-02-05 11:08:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:10:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:12:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 85188 tokens (85188 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:13:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 121340 tokens (121340 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:14:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:16:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 75515 tokens (75515 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:17:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:19:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 157859 tokens (157859 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:20:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:22:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 11:24:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:26:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:28:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 357948 tokens (357948 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 11:29:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 72484 tokens (72484 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:31:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:33:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:35:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:37:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:39:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:41:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 251611 tokens (251611 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:42:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:44:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 462493 tokens (462493 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:45:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:47:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:49:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:51:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 11:53:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 246900 tokens (246900 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 11:54:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:56:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 11:58:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:00:38,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 120057 tokens (120057 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:01:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:03:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 156543 tokens (156543 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:04:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 199865 tokens (199865 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:05:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 745917 tokens (745917 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:06:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:08:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:10:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:12:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:14:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:16:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:18:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:20:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:22:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:24:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 358270 tokens (358270 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:25:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 111127 tokens (111127 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:26:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:29:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:31:13,214916.02778434753,214916.50319099426,0.0,0,200,,deepseek/deepseek-reasoner
2025-02-05 12:35:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:37:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:39:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:41:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 359162 tokens (359162 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:42:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 83520 tokens (83520 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 12:43:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:45:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:47:54,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 326163 tokens (326163 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 12:48:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:50:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:52:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:54:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 12:57:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 12:59:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69501 tokens (69501 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:00:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:02:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:04:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:06:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:08:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:10:16,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 636131 tokens (636131 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:11:20,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136023 tokens (136023 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:12:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 13:14:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:16:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:18:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:20:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 104308 tokens (104308 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:21:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:23:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:25:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:27:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:30:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:32:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:34:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:36:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:38:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:40:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 264963 tokens (264963 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:41:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93406 tokens (93406 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:42:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:44:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:46:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:48:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:50:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:52:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 13:54:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 649016 tokens (649016 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 13:55:15,139567.3279762268,139567.84510612488,0.0,0,18,,deepseek/deepseek-reasoner
2025-02-05 13:58:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:00:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:02:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:04:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 723153 tokens (723153 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:05:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:07:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:09:43,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125403 tokens (125403 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 14:10:45,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298279 tokens (298279 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:11:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:13:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 426966 tokens (426966 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:14:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93341 tokens (93341 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:15:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:17:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:19:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:21:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:23:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:25:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:27:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:29:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:31:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 182235 tokens (182235 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:32:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 14:34:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:36:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:38:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 374772 tokens (374772 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 14:40:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:42:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:44:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 14:46:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:48:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:50:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:52:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:54:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:56:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 14:58:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:00:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 15:02:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:04:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:06:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 218184 tokens (218184 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 15:07:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:09:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:11:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:13:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:15:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:17:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:19:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:21:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:23:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:25:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:27:19,222028.68103981018,222029.14595603943,0.0,0,14694,,deepseek/deepseek-reasoner
2025-02-05 15:32:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:34:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:36:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:38:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:40:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:42:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 15:44:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:46:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:48:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 15:50:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 15:52:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 351817 tokens (351817 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 15:53:12,24785.77709197998,26899.298667907715,16.560039130254292,35,5,,deepseek/deepseek-chat
2025-02-05 15:54:39,100045.20344734192,100046.27776145935,0.0,0,5,,deepseek/deepseek-chat
2025-02-05 15:57:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 681852 tokens (681852 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 15:58:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:00:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 16:02:23,67966.08281135559,67966.49169921875,0.0,0,115,,deepseek/deepseek-chat
2025-02-05 16:04:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 367107 tokens (367107 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 16:05:33,56141.37268066406,59925.43601989746,16.120237567788124,61,183,,deepseek/deepseek-chat
2025-02-05 16:07:32,56695.5201625824,59262.293338775635,12.467015121086378,32,6,,deepseek/deepseek-chat
2025-02-05 16:09:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 16:11:32,83767.95506477356,83768.99528503418,0.0,0,9663,,deepseek/deepseek-chat
2025-02-05 16:13:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:15:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:17:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 108587 tokens (108587 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 16:18:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:20:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:23:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:25:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 16:27:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:29:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:31:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:33:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:35:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:37:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:39:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:41:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:43:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:45:06,73075.79064369202,73076.84755325317,0.0,0,2673,,deepseek/deepseek-chat
2025-02-05 16:47:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:49:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:51:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:53:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 298722 tokens (298722 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 16:54:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 16:56:25,46339.15209770203,49527.6198387146,15.36788325305085,49,69,,deepseek/deepseek-chat
2025-02-05 16:58:15,86933.82906913757,86935.17565727234,0.0,0,24591,,deepseek/deepseek-chat
2025-02-05 17:00:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:02:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:04:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:06:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:08:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:10:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:12:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:14:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:16:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:18:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:20:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:22:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:24:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 115322 tokens (115322 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:25:49,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146219 tokens (146219 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:26:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 330465 tokens (330465 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:27:53,132011.2817287445,132012.34316825867,0.0,0,8,,deepseek/deepseek-reasoner
2025-02-05 17:31:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:33:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:35:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:37:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:39:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:41:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:43:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 163129 tokens (163129 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:44:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:46:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 17:48:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 701506 tokens (701506 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:49:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103105 tokens (103105 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:50:15,19763.445377349854,24314.82481956482,13.622243714715905,62,343,,deepseek/deepseek-chat
2025-02-05 17:51:39,99574.49507713318,99575.66905021667,0.0,0,432,,deepseek/deepseek-chat
2025-02-05 17:54:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 103082 tokens (103082 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 17:55:21,70182.18564987183,70183.16793441772,0.0,0,12,,deepseek/deepseek-chat
2025-02-05 17:57:31,29937.79969215393,34511.86656951904,14.42917249999179,66,268,,deepseek/deepseek-chat
2025-02-05 17:59:05,72323.86636734009,72324.98359680176,0.0,0,1831,,deepseek/deepseek-chat
2025-02-05 18:01:18,551268.7032222748,551269.2153453827,0.0,0,199,,deepseek/deepseek-reasoner
2025-02-05 18:11:29,71592.56482124329,71593.11556816101,0.0,0,10370,,deepseek/deepseek-chat
2025-02-05 18:13:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:15:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:17:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:19:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 216348 tokens (216348 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 18:20:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125597 tokens (125597 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 18:21:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:23:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:25:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:27:46,35849.857807159424,37888.198375701904,15.20844969600277,31,4,,deepseek/deepseek-chat
2025-02-05 18:29:24,39464.73503112793,52872.32065200806,16.781544892735887,225,1188,,deepseek/deepseek-chat
2025-02-05 18:31:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:39:18,14563.560724258423,18905.184030532837,11.286101198412661,49,2559,,deepseek/deepseek-chat
2025-02-05 18:40:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 18:42:39,70080.66391944885,70081.1140537262,0.0,0,8764,,deepseek/deepseek-chat
2025-02-05 18:44:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:46:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:48:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:50:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 18:52:51,81971.40073776245,84943.2225227356,12.450275513521198,37,4,,deepseek/deepseek-chat
2025-02-05 18:55:16,14095.446825027466,18548.58350753784,14.821013749524903,66,166,,deepseek/deepseek-chat
2025-02-05 18:56:35,1050793.2074069977,1050794.225692749,0.0,0,8696,,deepseek/deepseek-reasoner
2025-02-05 19:15:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 370765 tokens (370765 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 19:16:07,10499.813556671143,24899.720191955566,18.472341990621942,266,751,,deepseek/deepseek-chat
2025-02-05 19:17:32,9560.953855514526,12586.05980873108,14.544945096291723,44,12,,deepseek/deepseek-chat
2025-02-05 19:18:45,10613.434076309204,13515.394687652588,16.540541526433643,48,36,,deepseek/deepseek-chat
2025-02-05 19:19:58,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 251485 tokens (251485 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 19:21:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 589911 tokens (589911 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 19:22:03,9976.99236869812,13903.538703918457,16.299311032174195,64,227,,deepseek/deepseek-chat
2025-02-05 19:23:16,14592.41533279419,17421.638250350952,16.965789334638732,48,158,,deepseek/deepseek-chat
2025-02-05 19:24:34,67966.62497520447,67967.85283088684,0.0,0,8663,,deepseek/deepseek-chat
2025-02-05 19:26:42,16446.00796699524,20048.682689666748,14.433720500151118,52,7,,deepseek/deepseek-chat
2025-02-05 19:28:02,17731.814861297607,20342.71764755249,16.469399100714824,43,7,,deepseek/deepseek-chat
2025-02-05 19:29:22,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 292696 tokens (292696 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 19:30:24,12164.43419456482,15446.084976196289,17.674031717404542,58,447,,deepseek/deepseek-chat
2025-02-05 19:31:39,73464.92767333984,73466.25208854675,0.0,0,18696,,deepseek/deepseek-chat
2025-02-05 19:33:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:35:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:37:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:39:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:41:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:43:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:45:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:47:57,87229.26783561707,87229.68626022339,0.0,0,6,,deepseek/deepseek-chat
2025-02-05 19:50:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:52:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:54:26,88855.50284385681,88856.0574054718,0.0,0,76,,deepseek/deepseek-chat
2025-02-05 19:56:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 77407 tokens (77407 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 19:57:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 19:59:56,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 68157 tokens (68157 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 20:00:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 208126 tokens (208126 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 20:01:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:03:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:06:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:08:00,1169241.8611049652,1169242.3582077026,0.0,0,3202,,deepseek/deepseek-reasoner
2025-02-05 20:28:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:30:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:32:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 20:34:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:36:31,97252.93588638306,97253.26871871948,0.0,0,14373,,deepseek/deepseek-chat
2025-02-05 20:39:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 435809 tokens (435809 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 20:40:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:42:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:44:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:46:12,107445.80721855164,107446.2890625,0.0,0,31,,deepseek/deepseek-chat
2025-02-05 20:49:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:51:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 271132 tokens (271132 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 20:52:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 20:54:04,77697.77822494507,79467.34976768494,22.60434180472132,40,17,,deepseek/deepseek-chat
2025-02-05 20:56:24,95801.47099494934,95802.23679542542,0.0,0,240,,deepseek/deepseek-chat
2025-02-05 20:58:59,90359.46798324585,92171.74649238586,17.65733017227305,32,4,,deepseek/deepseek-chat
2025-02-05 21:01:32,97381.45923614502,97382.62391090393,0.0,0,99,,deepseek/deepseek-chat
2025-02-05 21:04:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:06:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:08:10,65519.4776058197,68094.99526023865,22.90801614144276,59,18,,deepseek/deepseek-chat
2025-02-05 21:10:18,51546.78130149841,53581.55417442322,17.20093700172571,35,4,,deepseek/deepseek-chat
2025-02-05 21:12:12,986923.60496521,986924.7574806213,0.0,0,7081,,deepseek/deepseek-reasoner
2025-02-05 21:29:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:31:39,109840.4290676117,109841.56346321106,0.0,0,1293,,deepseek/deepseek-chat
2025-02-05 21:34:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:36:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:38:30,100910.03894805908,100910.57658195496,0.0,0,10136,,deepseek/deepseek-chat
2025-02-05 21:41:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:43:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 107895 tokens (107895 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 21:44:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:46:13,80223.37746620178,81687.75057792664,21.169468185253532,31,5,,deepseek/deepseek-chat
2025-02-05 21:48:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 21:50:35,77635.7741355896,78509.85527038574,21.73711254439929,19,4,,deepseek/deepseek-chat
2025-02-05 21:52:53,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 139168 tokens (139168 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 21:53:55,68164.96205329895,69907.20438957214,19.51508081977214,34,4,,deepseek/deepseek-chat
2025-02-05 21:56:05,97266.80970191956,97267.40527153015,0.0,0,57662,,deepseek/deepseek-chat
2025-02-05 21:58:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-05 22:00:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:02:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:04:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:06:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:08:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:10:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:12:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:14:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:16:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:18:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:20:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:22:50,24440.818786621094,28168.335676193237,38.89989081086191,145,7,,deepseek/deepseek-reasoner
2025-02-05 22:24:18,35769.161224365234,39009.824991226196,15.120359137863717,49,11,,deepseek/deepseek-chat
2025-02-05 22:25:57,100885.84876060486,100886.29174232483,0.0,0,30873,,deepseek/deepseek-chat
2025-02-05 22:28:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:30:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:32:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:34:40,108096.33231163025,108096.7845916748,0.0,0,127,,deepseek/deepseek-chat
2025-02-05 22:37:28,30250.107526779175,31997.56360054016,20.029115767511552,35,5,,deepseek/deepseek-chat
2025-02-05 22:39:00,114199.56088066101,114200.06918907166,0.0,0,25881,,deepseek/deepseek-chat
2025-02-05 22:41:54,384212.30459213257,384212.7208709717,0.0,0,6471,,deepseek/deepseek-reasoner
2025-02-05 22:49:18,84005.09810447693,84006.26182556152,0.0,0,34,,deepseek/deepseek-chat
2025-02-05 22:51:42,29675.652980804443,41575.0675201416,26.55592835726205,316,117,,deepseek/deepseek-chat
2025-02-05 22:53:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:55:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 22:57:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 583283 tokens (583283 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 22:58:27,272201.60269737244,272202.0194530487,0.0,0,1076,,deepseek/deepseek-reasoner
2025-02-05 23:03:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:06:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:08:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:10:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:12:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:14:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:16:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 451558 tokens (451558 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 23:17:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:19:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:21:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:23:06,222053.7235736847,232345.62039375305,37.69956177984914,388,45,,deepseek/deepseek-reasoner
2025-02-05 23:27:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:29:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:32:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:34:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:36:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:38:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:40:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:42:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:44:03,359278.1181335449,359279.64663505554,0.0,0,54937,,deepseek/deepseek-reasoner
2025-02-05 23:51:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:53:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:55:04,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 111803 tokens (111803 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-05 23:56:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 84271 tokens (84271 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-05 23:57:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-05 23:59:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 116883 tokens (116883 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 00:00:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 00:02:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:04:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:06:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:08:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:10:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:12:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 00:14:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:16:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:18:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:20:12,85742.5856590271,87772.6366519928,23.152127785390146,47,63,,deepseek/deepseek-chat
2025-02-06 00:22:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 292104 tokens (292104 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 00:23:42,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 380343 tokens (380343 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-06 00:24:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:26:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:28:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:30:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:32:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:34:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:36:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 532356 tokens (532356 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 00:37:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:39:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:41:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:43:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:45:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:47:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:49:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:51:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 00:53:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 00:55:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:57:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 00:59:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:01:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:03:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:05:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:07:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:09:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:11:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 01:14:00,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:16:00,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 93310 tokens (93310 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 01:17:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:19:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 01:21:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:23:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:25:05,1557922.3110675812,1557923.713684082,0.0,0,27848,,deepseek/deepseek-reasoner
2025-02-06 01:52:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:54:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:56:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 01:58:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:00:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:02:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:04:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:06:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:08:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:10:08,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 522084 tokens (522084 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 02:11:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 257683 tokens (257683 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 02:12:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:14:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 105600 tokens (105600 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 02:15:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 02:17:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:19:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:21:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 02:23:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:25:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:27:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:29:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:31:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:33:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:35:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:37:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:39:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:41:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:43:24,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:45:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:47:25,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 65817 tokens (65817 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 02:48:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:50:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:52:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:54:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:56:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 02:58:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:00:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:02:30,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 586931 tokens (586931 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:03:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:05:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:07:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:09:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:11:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:13:40,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82139 tokens (82139 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:14:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 92002 tokens (92002 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:15:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:17:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:19:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:21:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 66547 tokens (66547 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:22:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:24:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:26:46,75414.92915153503,75416.0840511322,0.0,0,239,,deepseek/deepseek-chat
2025-02-06 03:29:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:31:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:33:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:35:03,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 125160 tokens (125160 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:36:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:38:05,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:40:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 354792 tokens (354792 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:41:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:43:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:45:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:47:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:49:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:51:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 03:53:10,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:55:11,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 03:57:12,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 70498 tokens (70498 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-06 03:58:13,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 126320 tokens (126320 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 03:59:14,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 659941 tokens (659941 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:00:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:02:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:04:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:06:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:08:19,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 361241 tokens (361241 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:09:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:11:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:13:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:15:23,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 561784 tokens (561784 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:16:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:18:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:20:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:22:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:24:28,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:26:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:28:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:30:31,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 69204 tokens (69204 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:31:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:33:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 623184 tokens (623184 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:34:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:36:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:38:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:40:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:42:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:44:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:46:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 04:48:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:50:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:52:41,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 625168 tokens (625168 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 04:53:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:55:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:57:47,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 04:59:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 702539 tokens (702539 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-06 05:00:50,120342.79656410217,120343.31917762756,0.0,0,4,,deepseek/deepseek-chat
2025-02-06 05:03:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:05:52,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 261375 tokens (261375 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 05:06:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:08:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:10:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 82916 tokens (82916 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 05:11:56,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:13:57,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 251307 tokens (251307 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 05:14:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:16:59,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:19:00,168850.98385810852,168851.56726837158,0.0,0,7844,,deepseek/deepseek-reasoner
2025-02-06 05:22:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 05:24:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:26:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:28:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:30:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:32:51,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:34:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:36:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:38:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:40:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:42:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:44:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:46:55,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:48:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 281959 tokens (281959 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 05:49:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 05:51:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:53:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:55:59,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 87473 tokens (87473 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 05:57:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 05:59:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:01:01,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:03:02,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:05:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:07:03,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:09:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:11:04,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:13:05,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 305433 tokens (305433 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:14:06,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 611362 tokens (611362 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:15:09,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 411449 tokens (411449 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:16:11,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 507812 tokens (507812 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-06 06:17:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:19:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 06:21:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:23:15,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:25:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:27:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:29:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:31:18,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 341691 tokens (341691 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:32:21,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 604637 tokens (604637 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:33:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:35:25,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:37:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:39:26,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:41:27,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:43:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 297571 tokens (297571 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:44:29,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:46:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:48:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:50:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:52:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:54:32,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 136382 tokens (136382 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:55:33,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 720480 tokens (720480 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:56:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 06:58:37,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 219102 tokens (219102 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 06:59:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:01:39,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 676360 tokens (676360 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 07:02:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:04:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:06:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:08:44,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 76077 tokens (76077 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 07:09:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:11:46,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:13:47,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 373550 tokens (373550 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 07:14:48,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:16:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:18:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:20:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 165606 tokens (165606 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-reasoner
2025-02-06 07:21:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:23:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:25:53,253629.6727657318,253630.75351715088,0.0,0,2536,,deepseek/deepseek-reasoner
2025-02-06 07:31:06,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:33:07,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:35:08,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:37:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:39:09,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:41:10,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 574260 tokens (574260 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 07:42:12,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:44:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:46:13,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:48:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:50:14,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:52:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:54:16,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:56:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 07:58:17,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:00:18,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:02:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:04:19,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:06:20,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:08:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:10:21,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:12:22,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:14:23,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:16:24,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 581212 tokens (581212 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 08:17:27,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 618006 tokens (618006 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 08:18:30,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:20:31,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:22:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:24:32,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:26:33,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:28:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 08:30:34,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:32:35,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:34:36,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:36:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:38:37,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:40:38,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:42:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:44:39,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:46:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 08:48:40,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:50:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:52:41,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:54:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:56:42,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 08:58:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:00:43,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:02:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:04:44,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:06:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:08:45,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 09:10:46,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 252403 tokens (252403 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 09:11:48,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 146253 tokens (146253 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 09:12:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-reasoner
2025-02-06 09:14:49,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:16:50,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:18:50,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 78267 tokens (78267 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 09:19:52,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:21:53,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:23:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:25:54,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:27:55,,,,,,"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': ""This model's maximum context length is 65536 tokens. However, you requested 407776 tokens (407776 in the messages, 0 in the completion). Please reduce the length of the messages or completion."", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",deepseek/deepseek-chat
2025-02-06 09:28:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:30:57,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
2025-02-06 09:32:58,,,,,,"litellm.APIError: APIError: DeepseekException - error - Expecting value: line 1 column 1 (char 0), Received response - <APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>, Type of response - <class 'openai._legacy_response.LegacyAPIResponse'>",deepseek/deepseek-chat
